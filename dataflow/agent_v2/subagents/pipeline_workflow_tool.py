#!/usr/bin/env python3
"""
Pipeline Workflow Tool - ç›´æ¥è°ƒç”¨ dataflowagent çš„æ•°æ®å¤„ç†æµæ°´çº¿æ¨èåŠŸèƒ½
"""

import logging
import os
import asyncio
from typing import Any, Dict, Optional, List
from pydantic import BaseModel, Field

from dataflow.agent_v2.base.core import BaseTool

logger = logging.getLogger(__name__)


class PipelineWorkflowToolParams(BaseModel):
    """Pipeline Workflow Tool å‚æ•°"""
    json_file: str = Field(description="æ•°æ®æ–‡ä»¶è·¯å¾„")
    target: str = Field(description="ç”¨æˆ·éœ€æ±‚ç›®æ ‡")
    python_file_path: str = Field(description="è¾“å‡ºè„šæœ¬è·¯å¾„")
    language: Optional[str] = Field(default="zh", description="è¯­è¨€è®¾ç½®")
    chat_api_url: Optional[str] = Field(default=None, description="APIåœ°å€")
    api_key: Optional[str] = Field(default=None, description="APIå¯†é’¥")
    model: Optional[str] = Field(default="gpt-4o", description="æ¨¡å‹åç§°")
    need_debug: Optional[bool] = Field(default=True, description="æ˜¯å¦éœ€è¦è°ƒè¯•")
    max_debug_rounds: Optional[int] = Field(default=3, description="æœ€å¤§è°ƒè¯•è½®æ•°")


class PipelineWorkflowTool(BaseTool):
    """Pipeline Workflow Tool - æ•°æ®å¤„ç†æµæ°´çº¿æ¨èå·¥å…·"""
    
    @classmethod
    def name(cls) -> str:
        return "pipeline_workflow_agent"
    
    @classmethod
    def description(cls) -> str:
        return """æ•°æ®å¤„ç†æµæ°´çº¿æ¨èå·¥å…·ï¼Œè‡ªåŠ¨åŒ–å®Œæˆæ•°æ®åˆ†æã€ç®—å­æ¨èã€ä»£ç ç”Ÿæˆå…¨æµç¨‹ã€‚

ã€å‰ç½®å·¥å…·ã€‘ï¼šformer_agent - å»ºè®®å…ˆç”¨Formerå·¥å…·åˆ†ææ•°æ®éœ€æ±‚
ã€åç½®å·¥å…·ã€‘ï¼šæ—  - æ­¤å·¥å…·é€šå¸¸ä½œä¸ºå·¥ä½œæµç»ˆç‚¹

åŠŸèƒ½ç‰¹ç‚¹ï¼š
- è‡ªåŠ¨åˆ†ææ•°æ®ç±»å‹å’Œç‰¹å¾
- æ™ºèƒ½æ¨èæœ€ä½³å¤„ç†ç®—å­ç»„åˆ
- ç”Ÿæˆå®Œæ•´å¯æ‰§è¡Œçš„æ•°æ®å¤„ç†æµæ°´çº¿ä»£ç 
- æ”¯æŒå¤šè½®è°ƒè¯•ç¡®ä¿ä»£ç è´¨é‡
- æä¾›è¯¦ç»†çš„æ‰§è¡ŒæŠ¥å‘Šå’Œä½¿ç”¨æŒ‡å—

é€‚ç”¨åœºæ™¯ï¼š
- å·²é€šè¿‡Formerå·¥å…·æ˜ç¡®çš„æ•°æ®å¤„ç†éœ€æ±‚
- éœ€è¦å®Œæ•´æ•°æ®å¤„ç†æµæ°´çº¿çš„åœºæ™¯
- è‡ªåŠ¨åŒ–æ•°æ®å¤„ç†å’Œåˆ†æéœ€æ±‚
- ä»æ•°æ®æ–‡ä»¶åˆ°å¤„ç†è„šæœ¬çš„ç«¯åˆ°ç«¯ç”Ÿæˆ"""
    
    @classmethod
    def params(cls) -> type:
        return PipelineWorkflowToolParams
    
    @classmethod
    def prerequisite_tools(cls) -> List[str]:
        """å‰ç½®å·¥å…·åˆ—è¡¨"""
        return ["former_agent"]  # å»ºè®®å…ˆç”¨Formerå·¥å…·åˆ†æéœ€æ±‚
    
    @classmethod
    def suggested_followup_tools(cls) -> List[str]:
        """å»ºè®®çš„åç½®å·¥å…·åˆ—è¡¨"""
        return []  # é€šå¸¸ä½œä¸ºå·¥ä½œæµç»ˆç‚¹
    
    async def execute(self, **kwargs) -> Dict[str, Any]:
        """æ‰§è¡Œ Pipeline Workflow å·¥å…· - ç›´æ¥è°ƒç”¨ dataflowagent"""
        try:
            # ä»kwargsåˆ›å»ºå‚æ•°å¯¹è±¡
            params = PipelineWorkflowToolParams(**kwargs)
            
            json_file = params.json_file
            target = params.target
            python_file_path = params.python_file_path
            language = params.language or "zh"
            chat_api_url = params.chat_api_url or os.getenv("DATAFLOW_API_URL", "http://localhost:3000/v1/")
            api_key = params.api_key or os.getenv("DF_API_KEY", "sk-dummy")
            model = params.model or "gpt-4o"
            need_debug = params.need_debug if params.need_debug is not None else True
            max_debug_rounds = params.max_debug_rounds or 3
            
            logger.info(f"ğŸš€ Pipeline Workflow Tool å¼€å§‹å¤„ç†: {target}")
            logger.info(f"ğŸ“„ æ•°æ®æ–‡ä»¶: {json_file}")
            logger.info(f"ğŸ“ è¾“å‡ºè„šæœ¬: {python_file_path}")
            
            # ç›´æ¥è°ƒç”¨ dataflowagent çš„æ ¸å¿ƒåŠŸèƒ½
            try:
                # å¯¼å…¥å¿…è¦çš„ç»„ä»¶
                from dataflow.dataflowagent.state import DFRequest, DFState
                from dataflow.dataflowagent.script.pipeline_nodes import create_pipeline_graph
                
                # åˆ›å»º DFRequest å¯¹è±¡
                df_request = DFRequest(
                    language=language,
                    chat_api_url=chat_api_url,
                    api_key=api_key,
                    model=model,
                    json_file=json_file,
                    target=target,
                    python_file_path=python_file_path,
                    need_debug=need_debug,
                    max_debug_rounds=max_debug_rounds
                )
                
                # åˆ›å»ºåˆå§‹çŠ¶æ€
                df_state = DFState(request=df_request, messages=[])
                df_state.temp_data = {"round": 0}
                df_state.debug_mode = need_debug
                
                # åˆ›å»ºå¹¶è¿è¡Œ pipeline å›¾
                logger.info("ğŸ”„ æ‰§è¡Œ dataflowagent æµæ°´çº¿å·¥ä½œæµ...")
                graph_builder = create_pipeline_graph()
                pipeline_graph = graph_builder.build()
                
                # æ‰§è¡Œå®Œæ•´å·¥ä½œæµ
                final_state = await pipeline_graph.ainvoke(df_state)
                
                # ===== DEBUG: æ‰“å° final_state çš„å…³é”®ä¿¡æ¯ =====
                logger.info("ğŸ” DEBUG: æŸ¥çœ‹ final_state çš„å†…å®¹...")
                logger.info(f"ğŸ“‹ final_state ç±»å‹: {type(final_state)}")
                logger.info(f"ğŸ“‹ final_state å±æ€§: {dir(final_state) if hasattr(final_state, '__dict__') else 'N/A'}")
                
                # å°è¯•ä¸åŒæ–¹å¼è·å– pipeline_code
                pipeline_code_from_attr = getattr(final_state, 'pipeline_code', None)
                pipeline_code_from_get = final_state.get('pipeline_code', None) if hasattr(final_state, 'get') else None
                pipeline_code_from_temp = final_state.get('temp_data', {}).get('pipeline_code', None) if hasattr(final_state, 'get') else None
                
                logger.info(f"ğŸ” pipeline_code (ç›´æ¥å±æ€§): {type(pipeline_code_from_attr)} = {pipeline_code_from_attr}")
                logger.info(f"ğŸ” pipeline_code (getæ–¹æ³•): {type(pipeline_code_from_get)} = {pipeline_code_from_get}")
                logger.info(f"ğŸ” pipeline_code (temp_data): {type(pipeline_code_from_temp)} = {pipeline_code_from_temp}")
                
                # å¦‚æœæœ‰ temp_dataï¼Œæ‰“å°å®Œæ•´å†…å®¹
                if hasattr(final_state, 'temp_data'):
                    temp_data = getattr(final_state, 'temp_data', {})
                    logger.info(f"ğŸ” temp_data å†…å®¹: {temp_data}")
                
                # æå–ç»“æœ
                execution_result = final_state.get("execution_result", {})
                execution_successful = execution_result.get("success", False)
                execution_output = execution_result.get("stdout", "")
                execution_error = execution_result.get("stderr", "")
                
                # æå–å„é˜¶æ®µç»“æœ
                classification_result = final_state.get("category", {})
                recommendation_result = final_state.get("recommendation", [])
                
                # æå–ç”Ÿæˆçš„ä»£ç  - ä¼˜å…ˆä»ä¸åŒåœ°æ–¹å°è¯•è·å–
                generated_code = ""
                
                # æ–¹æ³•1ï¼šç›´æ¥ä» pipeline_code å±æ€§
                if pipeline_code_from_attr:
                    if isinstance(pipeline_code_from_attr, dict):
                        generated_code = pipeline_code_from_attr.get("code", str(pipeline_code_from_attr))
                    else:
                        generated_code = str(pipeline_code_from_attr)
                    logger.info(f"âœ… æ–¹æ³•1æˆåŠŸè·å–ä»£ç ï¼Œé•¿åº¦: {len(generated_code)}")
                
                # æ–¹æ³•2ï¼šä» get æ–¹æ³•
                elif pipeline_code_from_get:
                    if isinstance(pipeline_code_from_get, dict):
                        generated_code = pipeline_code_from_get.get("code", str(pipeline_code_from_get))
                    else:
                        generated_code = str(pipeline_code_from_get)
                    logger.info(f"âœ… æ–¹æ³•2æˆåŠŸè·å–ä»£ç ï¼Œé•¿åº¦: {len(generated_code)}")
                
                # æ–¹æ³•3ï¼šä» temp_data
                elif pipeline_code_from_temp:
                    generated_code = str(pipeline_code_from_temp)
                    logger.info(f"âœ… æ–¹æ³•3æˆåŠŸè·å–ä»£ç ï¼Œé•¿åº¦: {len(generated_code)}")
                
                else:
                    logger.warning("âŒ æ‰€æœ‰æ–¹æ³•éƒ½æœªèƒ½è·å–åˆ° pipeline_code")
                    # å°è¯•ä» pipeline_file_path è¯»å–æ–‡ä»¶
                    if hasattr(final_state, 'pipeline_file_path'):
                        pipeline_file = getattr(final_state, 'pipeline_file_path', '')
                        if pipeline_file and os.path.exists(pipeline_file):
                            try:
                                with open(pipeline_file, 'r', encoding='utf-8') as f:
                                    generated_code = f.read()
                                logger.info(f"âœ… ä»æ–‡ä»¶ {pipeline_file} è¯»å–ä»£ç ï¼Œé•¿åº¦: {len(generated_code)}")
                            except Exception as e:
                                logger.warning(f"âŒ ä»æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
                
                logger.info(f"ğŸ¯ æœ€ç»ˆè·å–çš„ä»£ç é¢„è§ˆ (å‰100å­—ç¬¦): {generated_code[:100]}...")
                
                # æ„å»ºè¯¦ç»†ç»“æœ
                if execution_successful:
                    logger.info("âœ… æ•°æ®å¤„ç†æµæ°´çº¿å·¥ä½œæµæ‰§è¡ŒæˆåŠŸ!")
                    
                    output_lines = []
                    output_lines.append("ğŸ‰ **æ•°æ®å¤„ç†æµæ°´çº¿æ¨èå·¥ä½œæµæ‰§è¡ŒæˆåŠŸï¼**")
                    output_lines.append(f"\nğŸ“Š **å·¥ä½œæµç»“æœ:**")
                    output_lines.append(f"- æ•°æ®æ–‡ä»¶: {json_file}")
                    output_lines.append(f"- ç”¨æˆ·ç›®æ ‡: {target}")
                    output_lines.append(f"- è¾“å‡ºè„šæœ¬: {python_file_path}")
                    
                    output_lines.append(f"\nğŸ” **æ•°æ®åˆ†æç»“æœ:**")
                    output_lines.append(f"- æ•°æ®ç±»å‹: {classification_result.get('category', 'æœªçŸ¥')}")
                    output_lines.append(f"- ç½®ä¿¡åº¦: {classification_result.get('confidence', 'N/A')}")
                    
                    output_lines.append(f"\nğŸ’¡ **æ¨èç®—å­:**")
                    if isinstance(recommendation_result, list):
                        for i, op in enumerate(recommendation_result, 1):
                            output_lines.append(f"   {i}. {op}")
                    else:
                        output_lines.append(f"   {recommendation_result}")
                    
                    output_lines.append(f"\nâœ… **ç”Ÿæˆçš„æµæ°´çº¿ä»£ç :**")
                    output_lines.append(f"```python\n{generated_code}\n```")
                    
                    output_lines.append(f"\nğŸ“‹ **æ‰§è¡Œç»“æœ:**")
                    output_lines.append(execution_output)
                    
                    output_lines.append(f"\nğŸš€ **ä½¿ç”¨æ–¹æ³•:**")
                    output_lines.append(f"å¯é€šè¿‡ä»¥ä¸‹å‘½ä»¤å¤„ç†å®Œæ•´æ•°æ®:")
                    output_lines.append(f"python {python_file_path}")
                    
                    output_lines.append(f"\nâœ¨ **å·¥ä½œæµç¨‹:**")
                    output_lines.append("1. æ•°æ®å†…å®¹åˆ†ç±» - è‡ªåŠ¨è¯†åˆ«æ•°æ®ç±»å‹å’Œç‰¹å¾")
                    output_lines.append("2. ç®—å­æ¨è - åŸºäºæ•°æ®ç‰¹å¾æ¨èæœ€ä½³å¤„ç†æµç¨‹")
                    output_lines.append("3. ä»£ç ç”Ÿæˆ - ç”Ÿæˆå®Œæ•´å¯æ‰§è¡Œçš„æ•°æ®å¤„ç†æµæ°´çº¿")
                    output_lines.append("4. è°ƒè¯•éªŒè¯ - å¤šè½®è°ƒè¯•ç¡®ä¿ä»£ç è´¨é‡")
                    
                    output_lines.append("\nğŸ’ **æµæ°´çº¿å·²é€šè¿‡è‡ªåŠ¨åŒ–éªŒè¯ï¼Œå¯ä»¥ç›´æ¥æŠ•å…¥ä½¿ç”¨ï¼**")
                    
                    output = "\n".join(output_lines)
                else:
                    logger.warning("âŒ æ•°æ®å¤„ç†æµæ°´çº¿å·¥ä½œæµæ‰§è¡Œå¤±è´¥")
                    
                    output_lines = []
                    output_lines.append("âš ï¸ **æ•°æ®å¤„ç†æµæ°´çº¿æ¨èå·¥ä½œæµæ‰§è¡Œå¤±è´¥**")
                    output_lines.append(f"\nğŸ“Š **å·¥ä½œæµé…ç½®:**")
                    output_lines.append(f"- æ•°æ®æ–‡ä»¶: {json_file}")
                    output_lines.append(f"- ç”¨æˆ·ç›®æ ‡: {target}")
                    output_lines.append(f"- è¾“å‡ºè„šæœ¬: {python_file_path}")
                    
                    output_lines.append(f"\nğŸ” **å·²å®Œæˆçš„æ­¥éª¤:**")
                    output_lines.append(f"- æ•°æ®åˆ†æ: {classification_result.get('category', 'æœªå®Œæˆ')}")
                    output_lines.append(f"- ç®—å­æ¨è: {len(recommendation_result) if isinstance(recommendation_result, list) else 0} ä¸ªç®—å­")
                    output_lines.append(f"- ä»£ç ç”Ÿæˆ: {'å·²ç”Ÿæˆ' if generated_code else 'æœªå®Œæˆ'}")
                    
                    output_lines.append(f"\nâŒ **é”™è¯¯ä¿¡æ¯:**")
                    output_lines.append(execution_error or "æœªçŸ¥é”™è¯¯")
                    
                    if generated_code:
                        output_lines.append(f"\nğŸ”§ **ç”Ÿæˆçš„ä»£ç :**")
                        output_lines.append(f"```python\n{generated_code}\n```")
                    
                    output_lines.append(f"\nğŸ’­ **é—®é¢˜æ’æŸ¥å»ºè®®:**")
                    output_lines.append("1. æ£€æŸ¥æ•°æ®æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®")
                    output_lines.append("2. ç¡®è®¤ç”¨æˆ·ç›®æ ‡æè¿°æ˜¯å¦æ¸…æ™°") 
                    output_lines.append("3. éªŒè¯è¾“å‡ºè·¯å¾„æ˜¯å¦æœ‰å†™å…¥æƒé™")
                    output_lines.append("4. æ£€æŸ¥APIé…ç½®æ˜¯å¦æ­£ç¡®")
                    
                    output = "\n".join(output_lines)
                
                return {
                    "success": execution_successful,
                    "output": output,
                    "execution_successful": execution_successful,
                    "execution_output": execution_output,
                    "execution_error": execution_error,
                    "generated_pipeline_code": generated_code,
                    "classification_result": classification_result,
                    "recommendation_result": recommendation_result,
                    "workflow_result": output,
                    # ğŸ¯ æ–°å¢ï¼šç”¨äºå‰ç«¯æ ‡ç­¾é¡µçš„æ•°æ®
                    "frontend_code_data": {
                        "code_content": generated_code,
                        "file_name": os.path.basename(python_file_path),
                        "file_path": python_file_path,
                        "language": "python",
                        "tool_source": "pipeline_workflow_agent",
                        "timestamp": asyncio.get_event_loop().time() if asyncio.get_event_loop().is_running() else None
                    }
                }
                
            except Exception as dataflow_error:
                logger.warning(f"DataflowAgent æ‰§è¡Œå¤±è´¥: {dataflow_error}")
                logger.info("åˆ‡æ¢åˆ°ç®€åŒ–çš„åå¤‡å®ç°")
                
                # ä½¿ç”¨ç®€åŒ–çš„åå¤‡å®ç°
                return await self._run_simple_pipeline_workflow(
                    json_file, target, python_file_path, 
                    language, chat_api_url, api_key, model
                )
                
        except Exception as e:
            logger.error(f"Pipeline Workflow Tool æ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return {
                "success": False,
                "error": str(e),
                "output": f"æ•°æ®å¤„ç†æµæ°´çº¿æ¨èå·¥ä½œæµæ‰§è¡Œå¤±è´¥: {str(e)}"
            }
    
    async def _run_simple_pipeline_workflow(
        self, 
        json_file: str, 
        target: str, 
        python_file_path: str,
        language: str = "zh",
        chat_api_url: Optional[str] = None,
        api_key: Optional[str] = None,
        model: str = "gpt-4o"
    ) -> Dict[str, Any]:
        """ç®€åŒ–çš„æ•°æ®å¤„ç†æµæ°´çº¿å®ç°"""
        try:
            logger.info("ğŸ”„ ä½¿ç”¨ç®€åŒ–çš„æ•°æ®å¤„ç†æµæ°´çº¿å®ç°")
            
            from dataflow.agent_v2.llm_client import get_llm_client
            
            llm = get_llm_client()
            
            # ç¬¬ä¸€æ­¥ï¼šåˆ†ææ•°æ®æ–‡ä»¶
            logger.info("ğŸ” ç¬¬ä¸€æ­¥ï¼šåˆ†ææ•°æ®æ–‡ä»¶...")
            data_analysis_prompt = f"""è¯·åˆ†ææ•°æ®æ–‡ä»¶å¹¶è¿›è¡Œåˆ†ç±»ï¼š

æ•°æ®æ–‡ä»¶è·¯å¾„: {json_file}
ç”¨æˆ·ç›®æ ‡: {target}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¿”å›æ•°æ®åˆ†æç»“æœï¼š

{{
    "category": "æ•°æ®ç±»å‹ï¼ˆå¦‚textã€imageã€tabularç­‰ï¼‰",
    "confidence": "ç½®ä¿¡åº¦ï¼ˆ0.0-1.0ï¼‰",
    "characteristics": "æ•°æ®ç‰¹å¾æè¿°",
    "size_estimate": "æ•°æ®è§„æ¨¡ä¼°è®¡"
}}

åªè¿”å›JSONæ ¼å¼çš„åˆ†æç»“æœï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚"""
            
            analysis_response = llm.call_llm("", data_analysis_prompt)
            analysis_content = analysis_response.get('content', '{}')
            
            # è§£æåˆ†æç»“æœ
            try:
                import json
                analysis_result = json.loads(analysis_content)
            except:
                analysis_result = {
                    "category": "text", 
                    "confidence": 0.8,
                    "characteristics": "é€šç”¨æ•°æ®",
                    "size_estimate": "æœªçŸ¥"
                }
            
            logger.info(f"âœ… æ•°æ®åˆ†æå®Œæˆ: {analysis_result}")
            
            # ç¬¬äºŒæ­¥ï¼šç®—å­æ¨è
            logger.info("ğŸ’¡ ç¬¬äºŒæ­¥ï¼šæ¨èå¤„ç†ç®—å­...")
            recommendation_prompt = f"""åŸºäºæ•°æ®åˆ†æç»“æœæ¨èå¤„ç†ç®—å­ï¼š

æ•°æ®åˆ†æç»“æœ: {analysis_result}
ç”¨æˆ·ç›®æ ‡: {target}

è¯·æ¨èåˆé€‚çš„æ•°æ®å¤„ç†ç®—å­åºåˆ—ï¼Œè¿”å›æ ¼å¼ï¼š

{{
    "recommended_operators": ["ç®—å­1", "ç®—å­2", "ç®—å­3"],
    "explanation": "æ¨èç†ç”±è¯´æ˜"
}}

åªè¿”å›JSONæ ¼å¼çš„æ¨èç»“æœã€‚"""
            
            recommendation_response = llm.call_llm("", recommendation_prompt)
            recommendation_content = recommendation_response.get('content', '{}')
            
            # è§£ææ¨èç»“æœ
            try:
                recommendation_result = json.loads(recommendation_content)
                recommended_ops = recommendation_result.get("recommended_operators", ["æ•°æ®åŠ è½½", "æ•°æ®æ¸…æ´—", "ç‰¹å¾æå–"])
            except:
                recommended_ops = ["æ•°æ®åŠ è½½", "æ•°æ®æ¸…æ´—", "ç‰¹å¾æå–"]
            
            logger.info(f"âœ… ç®—å­æ¨èå®Œæˆ: {recommended_ops}")
            
            # ç¬¬ä¸‰æ­¥ï¼šä»£ç ç”Ÿæˆ
            logger.info("ğŸ’» ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆæµæ°´çº¿ä»£ç ...")
            code_generation_prompt = f"""ç”Ÿæˆæ•°æ®å¤„ç†æµæ°´çº¿ä»£ç ï¼š

æ•°æ®æ–‡ä»¶: {json_file}
ç”¨æˆ·ç›®æ ‡: {target}
æ•°æ®åˆ†æ: {analysis_result}
æ¨èç®—å­: {recommended_ops}
è¾“å‡ºè·¯å¾„: {python_file_path}

è¯·ç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„Pythonæ•°æ®å¤„ç†æµæ°´çº¿ä»£ç ï¼Œè¦æ±‚ï¼š
1. èƒ½å¤Ÿå¤„ç†æŒ‡å®šçš„æ•°æ®æ–‡ä»¶
2. å®ç°æ¨èçš„ç®—å­åŠŸèƒ½
3. è¾¾æˆç”¨æˆ·ç›®æ ‡
4. åŒ…å«å¿…è¦çš„é”™è¯¯å¤„ç†
5. æœ‰æ¸…æ™°çš„è¾“å‡ºå’Œæ—¥å¿—

ç›´æ¥è¿”å›Pythonä»£ç ï¼Œç”¨```pythonå’Œ```åŒ…å›´ï¼š

```python
# ä½ çš„ä»£ç 
```"""
            
            code_response = llm.call_llm("", code_generation_prompt)
            code_content = code_response.get('content', '')
            
            # æå–ä»£ç 
            import re
            code_pattern = r"```python\s*\n(.*?)\n```"
            code_match = re.search(code_pattern, code_content, re.DOTALL)
            
            if code_match:
                generated_code = code_match.group(1).strip()
            else:
                generated_code = code_content.strip()
            
            logger.info("âœ… ä»£ç ç”Ÿæˆå®Œæˆ")
            
            # ç¬¬å››æ­¥ï¼šä¿å­˜ä»£ç åˆ°æŒ‡å®šè·¯å¾„
            logger.info(f"ğŸ’¾ ç¬¬å››æ­¥ï¼šä¿å­˜ä»£ç åˆ° {python_file_path}...")
            try:
                os.makedirs(os.path.dirname(python_file_path), exist_ok=True)
                with open(python_file_path, 'w', encoding='utf-8') as f:
                    f.write(generated_code)
                logger.info(f"âœ… ä»£ç å·²ä¿å­˜åˆ°: {python_file_path}")
                save_success = True
            except Exception as save_error:
                logger.warning(f"âŒ ä»£ç ä¿å­˜å¤±è´¥: {save_error}")
                save_success = False
            
            # æ„å»ºæˆåŠŸç»“æœ
            output_lines = []
            output_lines.append("ğŸ‰ **æ•°æ®å¤„ç†æµæ°´çº¿æ¨èå·¥ä½œæµæ‰§è¡ŒæˆåŠŸ** (ç®€åŒ–å®ç°)")
            output_lines.append(f"\nğŸ“‹ **ç”¨æˆ·éœ€æ±‚:** {target}")
            output_lines.append(f"\nğŸ“„ **æ•°æ®æ–‡ä»¶:** {json_file}")
            output_lines.append(f"\nğŸ” **æ•°æ®åˆ†æç»“æœ:**")
            output_lines.append(f"- æ•°æ®ç±»å‹: {analysis_result.get('category', 'æœªçŸ¥')}")
            output_lines.append(f"- ç½®ä¿¡åº¦: {analysis_result.get('confidence', 'N/A')}")
            output_lines.append(f"- ç‰¹å¾: {analysis_result.get('characteristics', 'æœªçŸ¥')}")
            
            output_lines.append(f"\nğŸ’¡ **æ¨èç®—å­:**")
            for i, op in enumerate(recommended_ops, 1):
                output_lines.append(f"   {i}. {op}")
            
            output_lines.append(f"\nğŸ’» **ç”Ÿæˆçš„æµæ°´çº¿ä»£ç :**")
            output_lines.append(f"```python\n{generated_code}\n```")
            
            if save_success:
                output_lines.append(f"\nâœ… **ä»£ç å·²ä¿å­˜åˆ°:** {python_file_path}")
                output_lines.append(f"\nğŸš€ **ä½¿ç”¨æ–¹æ³•:** python {python_file_path}")
            else:
                output_lines.append(f"\nâš ï¸ **ä»£ç ä¿å­˜å¤±è´¥ï¼Œä½†å·²ç”Ÿæˆå®Œæ•´ä»£ç **")
            
            output_lines.append(f"\nğŸ¯ **ç®€åŒ–æµç¨‹è¯´æ˜:**")
            output_lines.append("1. æ™ºèƒ½æ•°æ®åˆ†æ - LLMè‡ªåŠ¨è¯†åˆ«æ•°æ®ç‰¹å¾")
            output_lines.append("2. ç®—å­æ™ºèƒ½æ¨è - åŸºäºåˆ†æç»“æœæ¨èæœ€ä½³ç®—å­")
            output_lines.append("3. æµæ°´çº¿ä»£ç ç”Ÿæˆ - ç”Ÿæˆå®Œæ•´å¯æ‰§è¡Œä»£ç ")
            output_lines.append("4. è‡ªåŠ¨ä¿å­˜éƒ¨ç½² - ä»£ç ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„")
            
            return {
                "success": True,
                "output": "\n".join(output_lines),
                "execution_successful": save_success,
                "execution_output": f"æµæ°´çº¿ä»£ç å·²ç”Ÿæˆ{'å¹¶ä¿å­˜' if save_success else 'ä½†ä¿å­˜å¤±è´¥'}",
                "execution_error": "" if save_success else "ä»£ç ä¿å­˜å¤±è´¥",
                "generated_pipeline_code": generated_code,
                "classification_result": analysis_result,
                "recommendation_result": recommended_ops,
                "workflow_result": "æ•°æ®å¤„ç†æµæ°´çº¿æ¨èå·¥ä½œæµå®Œæˆï¼ˆç®€åŒ–å®ç°ï¼‰"
            }
            
        except Exception as e:
            logger.error(f"ç®€åŒ–æ•°æ®å¤„ç†æµæ°´çº¿æ‰§è¡Œå¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "output": f"æ•°æ®å¤„ç†æµæ°´çº¿æ¨èå·¥ä½œæµæ‰§è¡Œå¤±è´¥: {str(e)}"
            }
