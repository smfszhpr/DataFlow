"""
DataFlow Master Agent
åŸºäº MyScaleKB-Agent æ¶æ„çš„ä¸»æ§æ™ºèƒ½ä½“ - ä½¿ç”¨çœŸæ­£çš„LangGraphå·¥ä½œæµ
"""
import logging
import asyncio
import time
import uuid
from typing import Dict, List, Any, Union, Optional, Tuple, Protocol
from pydantic import BaseModel
from dataclasses import dataclass
from enum import Enum

# LangGraphæ ¸å¿ƒç»„ä»¶
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolExecutor
from langchain_core.tools import StructuredTool
from langchain_core.agents import AgentFinish as LCAgentFinish, AgentAction as LCAgentAction
from dataflow.agent_v2.base.core import SubAgent, GraphBuilder, BaseTool


from dataflow.agent_v2.llm_client import get_llm_client
from dataflow.agent_v2.subagents.apikey_agent import APIKeyTool
from dataflow.agent_v2.subagents.mock_tools import SleepTool, MockSearchTool, MockFormerTool, MockCodeGenTool
from dataflow.agent_v2.subagents.csvtools import CSVProfileTool, CSVDetectTimeColumnsTool, CSVVegaSpecTool, ASTStaticCheckTool, UnitTestStubTool, LocalIndexBuildTool, LocalIndexQueryTool

from langchain_core.tools import StructuredTool
from langgraph.prebuilt import ToolExecutor

def to_langchain_tool(tool: BaseTool) -> StructuredTool:
    ArgsSchema = tool.params()  # ä½ çš„å·¥å…·å·²ç»æä¾›äº† Pydantic å‚æ•°ç±»

    async def _arun(**kwargs):
        # äº¤ç»™åŸå·¥å…·æ‰§è¡Œï¼ˆç¡®ä¿æ˜¯ asyncï¼‰
        return await tool.execute(**kwargs)

    return StructuredTool.from_function(
        coroutine=_arun,                      # å¼‚æ­¥å‡½æ•°
        name=tool.name(),
        description=tool.description(),
        args_schema=ArgsSchema,               # å‚æ•°æ ¡éªŒ
        return_direct=False,                  # å¸¸è§„æƒ…å†µ Falseï¼›éœ€è¦æ—¶å¯ True
    )


# äº‹ä»¶åè®®å®šä¹‰
class EventType(str, Enum):
    RUN_STARTED = "run_started"
    PLAN_DECISION = "plan_decision"
    TOOL_STARTED = "tool_started"
    TOOL_RESULT = "tool_result"
    TOOL_ERROR = "tool_error"
    NODE_ENTER = "node_enter"
    NODE_EXIT = "node_exit"
    RUN_FINISHED = "run_finished"


class Event(BaseModel):
    session_id: str
    step_id: str         # ä¾‹å¦‚ "step-001" æ–¹ä¾¿å‰ç«¯åšå»é‡/æ’åº
    event: EventType
    data: Dict[str, Any] # è½½è·ï¼ˆå·¥å…·åã€å‚æ•°æ‘˜è¦ã€ç»“æœæ‘˜è¦ã€å†³ç­–ç­‰ï¼‰
    ts: float            # time.time()


class EventSink(Protocol):
    async def emit(self, event: Event) -> None: ...

def new_step_id() -> str:
    return f"step-{uuid.uuid4().hex[:8]}"


class PlannerOutput(BaseModel):
    decision: str              # "continue" | "finish"
    next_actions: list = []    # [ {"tool": "...", "tool_input": {...}} ... ]
    user_message: Optional[str] = None
    reasons: Optional[str] = None

logger = logging.getLogger(__name__)


class AgentState(BaseModel):
    """Master Agent çŠ¶æ€å®šä¹‰ - æ”¯æŒå¤šè½®ç¼–æ’"""
    input: str = ""
    agent_outcome: Optional[Any] = None
    intermediate_steps: List[Tuple[Any, Any]] = []  # ä¿®æ”¹ä¸ºæ”¯æŒç»“æ„åŒ–ç»“æœ
    session_id: Optional[str] = None
    current_step: str = "bootstrap"
    form_data: Optional[Dict[str, Any]] = None
    xml_content: Optional[str] = None
    execution_result: Optional[str] = None
    conversation_history: List[Dict[str, str]] = []  # å¯¹è¯å†å²
    last_tool_results: Optional[Dict[str, Any]] = None  # æœ€è¿‘çš„å·¥å…·ç»“æœ
    
    # å¤šè½®ç¼–æ’æ”¯æŒ
    pending_actions: List[LCAgentAction] = []  # å¾…æ‰§è¡Œçš„åŠ¨ä½œ
    tool_results: List[Dict[str, Any]] = []    # ç»“æ„åŒ–å·¥å…·ç»“æœ
    loop_guard: int = 0                        # å¾ªç¯è®¡æ•°å™¨
    max_steps: int = 8                         # æœ€å¤§æ­¥æ•°
    context_vars: Dict[str, Any] = {}          # è·¨æ­¥å…±äº«æ•°æ®
    next_action: Optional[str] = None          # ä¸‹ä¸€ä¸ªåŠ¨ä½œå†³ç­–
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]):
        return cls(**data)


class ActionType(Enum):
    """åŠ¨ä½œç±»å‹"""
    TOOL_EXECUTION = "tool_execution"
    SUB_AGENT_FORWARD = "sub_agent_forward"
    GENERAL_CONVERSATION = "general_conversation"
    END = "end"


class MasterAgent:
    """DataFlow Master Agent - çœŸæ­£çš„LangGraphæ¶æ„"""
    
    def __init__(self):
        self.llm = get_llm_client()  # åˆå§‹åŒ–çœŸæ­£çš„LLMå®¢æˆ·ç«¯
        self.forward_paths = {}
        self.sub_agents = {}
        self.conversation_sessions = {}  # ä¼šè¯ç®¡ç†
        self.tools = []
        self.compiled_graph = None
        
        # æ³¨å†Œå·¥å…·
        self._register_tools()
        
        # æ„å»ºLangGraph
        self._build_langgraph()
    
    def _register_tools(self):
        """æ³¨å†Œå·¥å…·"""
        try:
            self.tools = [
                APIKeyTool(),
                # æ·»åŠ Mockå·¥å…·ç”¨äºæµ‹è¯•å¤šè½®ç¼–æ’
                SleepTool(),
                MockSearchTool(),
                MockFormerTool(),
                MockCodeGenTool(),
                CSVProfileTool(), 
                CSVDetectTimeColumnsTool(), 
                CSVVegaSpecTool(), 
                ASTStaticCheckTool(), 
                UnitTestStubTool(), 
                LocalIndexBuildTool(), 
                LocalIndexQueryTool()

            ]
            logger.info(f"å·²æ³¨å†Œ {len(self.tools)} ä¸ªå·¥å…·")
        except Exception as e:
            logger.error(f"å·¥å…·æ³¨å†Œå¤±è´¥: {e}")
            self.tools = []
        
        self.lc_tools = [to_langchain_tool(t) for t in self.tools]
        self.tool_executor = ToolExecutor(self.lc_tools)
    
    def _build_langgraph(self):
        """æ„å»ºçœŸæ­£çš„LangGraphå·¥ä½œæµ - æ”¯æŒå¤šè½®ç¼–æ’"""
        try:
            # åˆ›å»ºStateGraph
            workflow = StateGraph(AgentState)
            
            # æ·»åŠ èŠ‚ç‚¹ - å‚ç…§MyScaleKB-Agentçš„èŠ‚ç‚¹ç»“æ„ï¼Œå¢åŠ plannerèŠ‚ç‚¹
            workflow.add_node("bootstrap", self.bootstrap_node)
            workflow.add_node("execute_tools", self.execute_tools_node)
            workflow.add_node("general_conversation", self.general_conversation_node)
            workflow.add_node("planner", self.planner_node)  # æ–°å¢è§„åˆ’å™¨èŠ‚ç‚¹
            workflow.add_node("summarize", self.summarize_node)
            
            # è®¾ç½®å…¥å£ç‚¹
            workflow.set_entry_point("bootstrap")
            
            # æ·»åŠ æ¡ä»¶è¾¹ - å‚ç…§MyScaleKB-Agentçš„action_forwardé€»è¾‘
            workflow.add_conditional_edges(
                "bootstrap",
                self.action_forward,
                {
                    "execute_tools": "execute_tools",
                    "general_conversation": "general_conversation", 
                    "end": "summarize"
                }
            )
            
            # æ‰§è¡Œå·¥å…·åè¿›å…¥è§„åˆ’å™¨è¿›è¡Œä¸‹ä¸€è½®å†³ç­–
            workflow.add_edge("execute_tools", "planner")
            workflow.add_edge("general_conversation", "planner")
            
            # è§„åˆ’å™¨å†³å®šç»§ç»­æ‰§è¡Œè¿˜æ˜¯ç»“æŸ
            workflow.add_conditional_edges(
                "planner",
                self.planner_router,
                {
                    "continue": "execute_tools",  # ç»§ç»­æ‰§è¡Œæ›´å¤šå·¥å…·
                    "finish": "summarize"        # å®Œæˆä»»åŠ¡
                }
            )
            
            workflow.add_edge("summarize", END)
            
            # ç¼–è¯‘å›¾
            self.compiled_graph = workflow.compile()
            logger.info("âœ… LangGraphå·¥ä½œæµæ„å»ºæˆåŠŸ - æ”¯æŒå¤šè½®ç¼–æ’")
            
        except Exception as e:
            logger.error(f"LangGraphæ„å»ºå¤±è´¥: {e}")
            self.compiled_graph = None
    
    async def bootstrap_node(self, state: AgentState) -> AgentState:
        """å¼•å¯¼èŠ‚ç‚¹ - æ¯æ¬¡åªè§„åˆ’ç¬¬ä¸€ä¸ªåŠ¨ä½œï¼Œåç»­é€šè¿‡plannerèŠ‚ç‚¹é€æ­¥è§„åˆ’"""
        user_input = state.input
        logger.info(f"ğŸ”„ BootstrapèŠ‚ç‚¹: {user_input}")
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šBootstrapåªè´Ÿè´£å¯åŠ¨ç¬¬ä¸€ä¸ªåŠ¨ä½œï¼Œä¸è¿›è¡Œå¤æ‚çš„å¤šæ­¥éª¤è§„åˆ’
        # è·å–å¯ç”¨å·¥å…·åˆ—è¡¨
        available_tools = [tool.name() for tool in self.tools]
        logger.info(f"ğŸ”§ å¯ç”¨å·¥å…·åˆ—è¡¨: {available_tools}")
        
        # ğŸ”§ é‡è¦ï¼šBootstrapé˜¶æ®µåªé€‰æ‹©å¯åŠ¨åŠ¨ä½œï¼Œä¸è€ƒè™‘æ¬¡æ•°å’Œé—´éš”
        # ç®€åŒ–ç”¨æˆ·è¾“å…¥ï¼Œåªæå–ä¸»è¦ä»»åŠ¡ç±»å‹
        simplified_input = self._extract_main_task(user_input)
        logger.info(f"ğŸ“ ç®€åŒ–ä»»åŠ¡: {simplified_input}")
        
        # ä½¿ç”¨LLMåˆ†æç”¨æˆ·æ„å›¾ï¼Œä½†åªå…³æ³¨ç¬¬ä¸€ä¸ªåŠ¨ä½œ
        try:
            intent_analysis = self.llm.analyze_user_intent(simplified_input, available_tools)
            
            selected_tool = intent_analysis.get("selected_tool")
            confidence = intent_analysis.get("confidence", 0.0)
            parameters = intent_analysis.get("parameters", {})
            
            logger.info(f"ğŸ¯ æ„å›¾åˆ†æç»“æœ: å·¥å…·={selected_tool}, ç½®ä¿¡åº¦={confidence}")
            
            if selected_tool and confidence > 0.3:
                # ğŸ”§ é‡è¦ï¼šåªåˆ›å»ºä¸€ä¸ªåŠ¨ä½œï¼Œè®©plannerè´Ÿè´£åç»­è§„åˆ’
                logger.info(f"âœ… Bootstrapé€‰æ‹©å·¥å…·: {selected_tool} (å•æ¬¡æ‰§è¡Œ)")
                
                action = LCAgentAction(
                    tool=selected_tool,
                    tool_input=parameters,
                    log=f"Bootstrapå¯åŠ¨: {selected_tool}"
                )
                state.agent_outcome = [action]  # æ³¨æ„ï¼šåªæœ‰ä¸€ä¸ªåŠ¨ä½œ
                logger.info(f"ğŸš€ Bootstrapåˆ›å»ºå•ä¸ªAction: {action.tool}")
            else:
                # æ²¡æœ‰åˆé€‚çš„å·¥å…·ï¼Œæ ‡è®°ä¸ºéœ€è¦é€šç”¨å¯¹è¯
                logger.info(f"âŒ æ²¡æœ‰åˆé€‚å·¥å…·ï¼Œä½¿ç”¨é€šç”¨å¯¹è¯ (ç½®ä¿¡åº¦: {confidence})")

                action = LCAgentAction(
                    tool="general_conversation",
                    tool_input={"user_input": user_input},
                    log="é€šç”¨å¯¹è¯"
                )

                state.agent_outcome = [action]
                logger.info(f"ğŸ’¬ Bootstrapåˆ›å»ºå¯¹è¯Action")
                
        except Exception as e:
            logger.error(f"LLMæ„å›¾åˆ†æå¤±è´¥: {e}")
            
            raise Exception("LLMæœåŠ¡ä¸å¯ç”¨ï¼Œæ— æ³•æ‰§è¡Œä»»ä½•å·¥å…·æˆ–SubAgent")
        return state
    
    def _extract_main_task(self, user_input: str) -> str:
        """ç›´æ¥è¿”å›ç”¨æˆ·è¾“å…¥ï¼Œä¸åšä»»ä½•å…³é”®è¯åŒ¹é…å¤„ç†"""
        return user_input.strip()
    
    async def execute_tools_node(self, state: AgentState) -> AgentState:
        """æ‰§è¡Œå·¥å…·èŠ‚ç‚¹ - ç¡®ä¿æ¯æ¬¡åªæ‰§è¡Œä¸€ä¸ªåŠ¨ä½œ"""
        agent_outcome = state.agent_outcome
        
        logger.info(f"ğŸ› ï¸ è¿›å…¥execute_tools_nodeï¼Œagent_outcomeç±»å‹: {type(agent_outcome)}")
        logger.info(f"ğŸ› ï¸ agent_outcomeå†…å®¹: {agent_outcome}")

        # ğŸ”§ å…³é”®ä¿®å¤ï¼šç¡®ä¿æ¯æ¬¡åªå¤„ç†ä¸€ä¸ªåŠ¨ä½œ
        actions: List[LCAgentAction] = []
        if isinstance(agent_outcome, list):
            # ğŸ”§ é‡è¦ï¼šå³ä½¿agent_outcomeæ˜¯åˆ—è¡¨ï¼Œä¹Ÿåªå–ç¬¬ä¸€ä¸ªåŠ¨ä½œ
            if agent_outcome:
                actions = [agent_outcome[0]]  # åªå–ç¬¬ä¸€ä¸ª
                logger.info(f"ğŸ“‹ ä»åˆ—è¡¨ä¸­å–ç¬¬ä¸€ä¸ªåŠ¨ä½œ: {actions[0].tool}")
            else:
                logger.warning("âš ï¸ agent_outcomeæ˜¯ç©ºåˆ—è¡¨")
                return state
        elif hasattr(agent_outcome, "tool"):
            actions = [agent_outcome]
            logger.info(f"ğŸ“‹ å•ä¸ªåŠ¨ä½œ: {agent_outcome.tool}")
        else:
            logger.warning(f"âš ï¸ æ²¡æœ‰å¯æ‰§è¡Œçš„åŠ¨ä½œï¼Œagent_outcomeç±»å‹: {type(agent_outcome)}")
            return state

        if state.tool_results is None:
            state.tool_results = []

        # ğŸ”§ å…³é”®ï¼šåªæ‰§è¡Œè¿™ä¸€ä¸ªåŠ¨ä½œ
        if actions:
            action = actions[0]
            logger.info(f"ğŸ› ï¸ å¼€å§‹æ‰§è¡Œå•ä¸ªå·¥å…·: {action.tool}")
            
            try:
                result = await self.tool_executor.ainvoke(action)
                
                # è®°å½•åˆ°intermediate_steps
                state.intermediate_steps.append((action, result))
                
                # è®°å½•åˆ°tool_results
                state.tool_results.append({
                    "tool": action.tool,
                    "ok": bool(result.get("success", True)) if isinstance(result, dict) else True,
                    "payload": result
                })
                
                logger.info(f"âœ… å·¥å…·æ‰§è¡Œå®Œæˆ: {action.tool}")
                
            except Exception as e:
                logger.error(f"âŒ å·¥å…·æ‰§è¡Œå¤±è´¥: {action.tool}, é”™è¯¯: {e}")
                err = {"success": False, "error": str(e)}
                state.intermediate_steps.append((action, err))
                state.tool_results.append({"tool": action.tool, "ok": False, "payload": err})

        # ğŸ”§ å…³é”®ï¼šæ¸…ç©ºagent_outcomeï¼Œé¿å…é‡å¤æ‰§è¡Œ
        state.agent_outcome = []
        logger.info(f"ğŸ”„ å·¥å…·æ‰§è¡Œå®Œæˆï¼Œæ¸…ç©ºagent_outcomeï¼Œè¿›å…¥plannerèŠ‚ç‚¹")
        
        return state
    
    async def general_conversation_node(self, state: AgentState) -> AgentState:
        """é€šç”¨å¯¹è¯èŠ‚ç‚¹ - å¤„ç†ä¸éœ€è¦å·¥å…·çš„å¯¹è¯"""
        user_input = state.input
        conversation_history = state.conversation_history
        
        logger.info(f"ğŸ’¬ é€šç”¨å¯¹è¯èŠ‚ç‚¹: {user_input}")
        
        # å¦‚æœLLMå¯ç”¨ï¼Œä½¿ç”¨æ™ºèƒ½å¯¹è¯
        if self.llm.api_available:
            try:
                # æ„å»ºé€šç”¨å¯¹è¯æç¤ºè¯
                system_prompt = """ä½ æ˜¯DataFlowæ™ºèƒ½åŠ©æ‰‹ï¼Œä¸€ä¸ªä¸“ä¸šã€å‹å¥½ã€æ™ºèƒ½çš„AIåŠ©æ‰‹ã€‚ä½ å¯ä»¥ï¼š

1. å›ç­”å„ç§é€šç”¨é—®é¢˜ï¼ˆç§‘å­¦ã€æŠ€æœ¯ã€ç”Ÿæ´»ã€å­¦ä¹ ç­‰ï¼‰
2. æä¾›ä¸“ä¸šçš„ç¼–ç¨‹ã€æ•°æ®åˆ†æå»ºè®®
3. ååŠ©è§£å†³é—®é¢˜å’Œæä¾›åˆ›æ„æƒ³æ³•
4. è®°ä½å¯¹è¯å†å²ï¼Œä¿æŒè¿è´¯å¯¹è¯
5. å½“ç”¨æˆ·éœ€è¦æ—¶ï¼Œå»ºè®®ä½¿ç”¨ä¸“ä¸šå·¥å…·ï¼ˆAPIå¯†é’¥ã€è¡¨å•ç”Ÿæˆã€æ•°æ®å¤„ç†ç­‰ï¼‰

ä½ çš„ç‰¹ç‚¹ï¼š
- çŸ¥è¯†ä¸°å¯Œï¼Œå–„äºåˆ†æå’Œè§£é‡Š
- å›ç­”å‡†ç¡®ã€æœ‰æ¡ç†
- è¯­è¨€è‡ªç„¶ã€å‹å¥½
- èƒ½å¤Ÿæ ¹æ®ä¸Šä¸‹æ–‡ç†è§£ç”¨æˆ·çœŸæ­£çš„éœ€æ±‚
- å¦‚æœç”¨æˆ·é—®é¢˜å¯èƒ½éœ€è¦ä¸“ä¸šå·¥å…·ï¼Œä¼šä¸»åŠ¨å»ºè®®

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œä¿æŒä¸“ä¸šä½†å‹å¥½çš„è¯­æ°”ã€‚"""

                # æ„å»ºå¯¹è¯å†å²
                history_text = self._build_history_text(conversation_history, k=8, clip=200)
                
                user_prompt = f"""ç”¨æˆ·é—®é¢˜: {user_input}

å¯¹è¯å†å²:
{history_text}

è¯·åŸºäºå¯¹è¯å†å²å’Œå½“å‰é—®é¢˜ï¼Œç»™å‡ºæœ€åˆé€‚çš„å›ç­”ã€‚å¦‚æœç”¨æˆ·è¯¢é—®ä¹‹å‰çš„å¯¹è¯å†…å®¹ï¼Œè¯·å‡†ç¡®å›å¿†ã€‚å¦‚æœé—®é¢˜å¯èƒ½éœ€è¦ä¸“ä¸šå·¥å…·ååŠ©ï¼ˆå¦‚APIå¯†é’¥è·å–ã€è¡¨å•ç”Ÿæˆã€æ•°æ®åˆ†æã€ä»£ç ç”Ÿæˆç­‰ï¼‰ï¼Œè¯·ä¸»åŠ¨å»ºè®®ã€‚"""

                # è°ƒç”¨LLM
                llm_service = self.llm._create_llm_service()
                responses = llm_service.generate_from_input(
                    user_inputs=[user_prompt],
                    system_prompt=system_prompt
                )
                
                if responses and responses[0]:
                    response = responses[0].strip()
                    
                    # å¦‚æœå›å¤å¤ªçŸ­ï¼Œå°è¯•æ‰©å±•
                    if len(response) < 30:
                        followup_prompt = f"""ç”¨æˆ·é—®é¢˜: {user_input}

è¯·æä¾›ä¸€ä¸ªæ›´è¯¦ç»†ã€æ›´æœ‰å¸®åŠ©çš„å›ç­”ã€‚å³ä½¿é—®é¢˜ç®€å•ï¼Œä¹Ÿè¦ç»™å‡ºå‹å¥½çš„å›å¤å’Œå¯èƒ½çš„æ‰©å±•å»ºè®®ã€‚å¦‚æœé—®é¢˜æ¶‰åŠæŠ€æœ¯ï¼Œå¯ä»¥æä¾›ä¸€äº›ç›¸å…³èƒŒæ™¯çŸ¥è¯†ã€‚"""
                        
                        followup_responses = llm_service.generate_from_input(
                            user_inputs=[followup_prompt],
                            system_prompt=system_prompt
                        )
                        
                        if followup_responses and followup_responses[0]:
                            response = followup_responses[0].strip()
                    

                    finish = LCAgentFinish(
                        return_values={"output": response},
                        log="é€šç”¨å¯¹è¯å®Œæˆ"
                    )

                    state.agent_outcome = finish
                    return state
                    
            except Exception as e:
                logger.error(f"é€šç”¨å¯¹è¯LLMè°ƒç”¨å¤±è´¥: {e}")
        
        # LLMä¸å¯ç”¨æ—¶çš„fallbacké€»è¾‘
        response = self._get_fallback_response(user_input, conversation_history)
        

        finish = LCAgentFinish(
            return_values={"output": response},
            log="Fallbackå“åº”"
        )

        state.agent_outcome = finish
        return state
    
    async def planner_node(self, state: AgentState) -> AgentState:
        """è§„åˆ’å™¨èŠ‚ç‚¹ - æ¯æ¬¡åªè§„åˆ’ä¸‹ä¸€ä¸ªå•ç‹¬åŠ¨ä½œ"""
        # âœ… æ­¥æ•°æŠ¤æ 
        if not hasattr(state, 'loop_guard'):
            state.loop_guard = 0
        state.loop_guard += 1
        
        max_steps = getattr(state, 'max_steps', 8)
        if state.loop_guard >= max_steps:
            # è§¦å‘æŠ¤æ ç›´æ¥ç»“æŸ
            logger.info(f"ğŸ›‘ è¾¾åˆ°æœ€å¤§æ­¥éª¤æ•° {max_steps}ï¼Œè‡ªåŠ¨ç»“æŸ")
            state.agent_outcome = []  # æ¸…ç©ºï¼Œè®©summarizeå¤„ç†
            state.next_action = "finish"
            return state
            
        logger.info(f"ğŸ¯ è¿›å…¥è§„åˆ’å™¨èŠ‚ç‚¹ - æ­¥éª¤ {state.loop_guard}/{max_steps}")
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ¯æ¬¡åªè§„åˆ’ä¸‹ä¸€ä¸ªå•ç‹¬åŠ¨ä½œ
        try:
            analysis = self._analyze_user_needs(state.input, state.tool_results or [])
            logger.info(f"ğŸ“‹ éœ€æ±‚åˆ†æ: {analysis}")
            
            if analysis["should_continue"] and analysis["next_action"]:
                # ğŸ”§ é‡è¦ï¼šåªåˆ›å»ºä¸€ä¸ªåŠ¨ä½œ
                next_action = analysis["next_action"]
                
                single_action = LCAgentAction(
                    tool=next_action.get("tool", ""),
                    tool_input=next_action.get("tool_input", {}),
                    log=f"Plannerè§„åˆ’: {next_action.get('tool','')}"
                )
                
                state.agent_outcome = [single_action]  # æ³¨æ„ï¼šåªæœ‰ä¸€ä¸ªåŠ¨ä½œ
                state.next_action = "continue"
                
                logger.info(f"ğŸ“‹ Plannerè§„åˆ’ä¸‹ä¸€ä¸ªåŠ¨ä½œ: {next_action.get('tool', '')} (å•æ¬¡)")
                logger.info(f"ğŸ“‹ åŸå› : {'; '.join(analysis['reasons']) if isinstance(analysis['reasons'], list) else analysis['reasons']}")
            else:
                # ğŸ”§ ä¿®å¤ï¼šæµè½¬åˆ°summarizeèŠ‚ç‚¹è¿›è¡Œæ™ºèƒ½æ€»ç»“
                state.agent_outcome = []  # æ¸…ç©ºï¼Œè®©summarize_nodeå¤„ç†
                state.next_action = "finish"
                logger.info(f"ğŸ Plannerå†³å®šç»“æŸï¼Œæµè½¬åˆ°summarizeèŠ‚ç‚¹")
                logger.info(f"ğŸ“‹ ç»“æŸåŸå› : {'; '.join(analysis['reasons']) if isinstance(analysis['reasons'], list) else analysis['reasons']}")
                
            return state
            
        except Exception as e:
            logger.error(f"è§„åˆ’å™¨é”™è¯¯: {e}")
            # ğŸ”§ ä¿®å¤ï¼šå¼‚å¸¸æ—¶ä¹Ÿæµè½¬åˆ°summarizeèŠ‚ç‚¹
            state.agent_outcome = []
            state.next_action = "finish"
            state.error_message = f"è§„åˆ’é”™è¯¯: {str(e)}"
            return state

    def planner_router(self, state: AgentState) -> str:
        """è§„åˆ’å™¨è·¯ç”±å™¨ - ä¿®å¤ç‰ˆæœ¬ï¼Œå…œåº•è¿”å›finish"""
        next_action = getattr(state, "next_action", None)
        result = "continue" if next_action == "continue" else "finish"
        logger.info(f"ï¿½ è·¯ç”±å†³ç­–: {next_action} -> {result}")
        return result
    
    async def summarize_node(self, state: AgentState) -> AgentState:
        """æ€»ç»“èŠ‚ç‚¹ - æ™ºèƒ½æ€»ç»“æ‰€æœ‰å·¥å…·æ‰§è¡Œç»“æœï¼Œè€Œä¸æ˜¯ç®€å•çš„æ­¥éª¤è®¡æ•°"""
        logger.info(f"ğŸ“ æ€»ç»“èŠ‚ç‚¹å¼€å§‹ï¼Œintermediate_stepsæ•°é‡: {len(state.intermediate_steps or [])}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯æ¶ˆæ¯
        if hasattr(state, 'error_message'):

            finish = LCAgentFinish(
                return_values={"output": state.error_message},
                log="é”™è¯¯æ€»ç»“"
            )

            state.agent_outcome = finish
            return state
        
        # å¦‚æœå·²ç»æ˜¯æœ€ç»ˆç»“æœï¼Œç›´æ¥è¿”å›
        if hasattr(state.agent_outcome, 'return_values'):
            # å·²ç»æ˜¯æœ€ç»ˆç»“æœï¼Œç›´æ¥è¿”å›
            logger.info("ğŸ“ æ£€æµ‹åˆ°å·²æœ‰return_valuesï¼Œç›´æ¥è¿”å›")
            return state
        
        # ğŸ”§ æ ¸å¿ƒä¿®å¤ï¼šå¯¹æ‰€æœ‰å·¥å…·æ‰§è¡Œç»“æœè¿›è¡ŒLLMæ™ºèƒ½æ€»ç»“
        if state.intermediate_steps:
            logger.info(f"ğŸ¤– å¼€å§‹LLMæ™ºèƒ½æ€»ç»“ï¼Œå…±{len(state.intermediate_steps)}ä¸ªæ‰§è¡Œæ­¥éª¤")
            final_output = await self._generate_conversation_response(state)
            logger.info(f"âœ… LLMæ™ºèƒ½æ€»ç»“å®Œæˆ: {final_output[:100]}...")
        else:
            # å¦‚æœæ²¡æœ‰å·¥å…·æ‰§è¡Œï¼Œç›´æ¥ä½¿ç”¨é€šç”¨å¯¹è¯å›å¤
            logger.info("ğŸ’¬ æ²¡æœ‰å·¥å…·æ‰§è¡Œï¼Œä½¿ç”¨é€šç”¨å¯¹è¯å›å¤")
            final_output = await self._get_direct_conversation_response(state)
        

        finish = LCAgentFinish(
            return_values={"output": final_output},
            log="æ™ºèƒ½æ€»ç»“å®Œæˆ"
        )

        state.agent_outcome = finish
        
        return state
    
    def action_forward(self, state: AgentState) -> str:
        """å†³å®šä¸‹ä¸€æ­¥åŠ¨ä½œ - å‚ç…§MyScaleKB-Agentçš„action_forward"""
        logger.info(f"ğŸ”€ Action Forwardå¼€å§‹ï¼Œagent_outcomeç±»å‹: {type(state.agent_outcome)}")
        logger.info(f"ğŸ”€ Agent outcomeå†…å®¹: {state.agent_outcome}")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç»“æŸçŠ¶æ€
        if hasattr(state.agent_outcome, 'return_values'):
            logger.info("ğŸ“ æ£€æµ‹åˆ°return_valuesï¼Œç»“æŸæµç¨‹")
            return "end"

        
        # è·å–agent_action - ç›´æ¥ä½¿ç”¨agent_outcomeæˆ–ä»åˆ—è¡¨ä¸­å–ç¬¬ä¸€ä¸ª
        if isinstance(state.agent_outcome, list):
            agent_action = state.agent_outcome[0] if state.agent_outcome else None
            logger.info(f"ğŸ¬ ä»åˆ—è¡¨è·å–agent_action: {agent_action}")
        else:
            agent_action = state.agent_outcome
            logger.info(f"ğŸ¬ ç›´æ¥è·å–agent_action: {agent_action}")
        
        if agent_action:
            # æ£€æŸ¥LangGraphæ¨¡å¼ä¸‹çš„å·¥å…·
            if hasattr(agent_action, 'tool'):
                tool_name = agent_action.tool
                logger.info(f"ğŸ”§ LangGraphæ¨¡å¼ - å·¥å…·å: {tool_name}")
                # é™¤äº†general_conversationå¤–ï¼Œæ‰€æœ‰å·¥å…·éƒ½è·¯ç”±åˆ°execute_tools
                if tool_name == "general_conversation":
                    logger.info("ğŸ’¬ è·¯ç”±åˆ°: general_conversation")
                    return "general_conversation"
                else:
                    logger.info(f"ğŸ› ï¸ è·¯ç”±åˆ°: execute_tools (å·¥å…·: {tool_name})")
                    return "execute_tools"
            
        logger.info("âš ï¸ æ— åŒ¹é…æ¡ä»¶ï¼Œé»˜è®¤è·¯ç”±åˆ°general_conversation")
        return "general_conversation"
    
    def _simple_keyword_fallback(self, user_input: str) -> Optional[Dict[str, Any]]:
        """å½“LLMä¸å¯ç”¨æ—¶ï¼Œæ‰€æœ‰SubAgentéƒ½æ²¡æœ‰æ„ä¹‰ï¼Œç›´æ¥æŠ›å‡ºé”™è¯¯"""
        raise Exception("LLMæœåŠ¡ä¸å¯ç”¨ï¼Œæ— æ³•æ‰§è¡Œä»»ä½•å·¥å…·æˆ–SubAgent")
    
    def _build_history_text(self, conversation_history: List[Dict[str, str]], k: int = 8, clip: int = 200) -> str:
        """æŠŠæœ€è¿‘ k æ¡å†å²æ‹¼æˆç»Ÿä¸€æ–‡æœ¬ï¼›é•¿æ¶ˆæ¯è£å‰ªåˆ° clip å­—ç¬¦ã€‚"""
        if not conversation_history:
            return ""

        recent = conversation_history[-k:]
        lines = []
        for msg in recent:
            role = "ç”¨æˆ·" if msg.get("role") == "user" else "åŠ©æ‰‹"
            content = msg.get("content", "")
            if len(content) > clip:
                content = content[:clip] + "..."
            lines.append(f"{role}: {content}")
        return "\n".join(lines)
    
    def _get_fallback_response(self, user_input: str, conversation_history: List[Dict[str, str]]) -> str:
        """Fallbackå“åº” - ä¸åšå‡è£…å›å¤ï¼Œç›´æ¥æŠ›å‡ºå¼‚å¸¸"""
        raise Exception("LLMæœåŠ¡ä¸å¯ç”¨ï¼Œæ— æ³•å¤„ç†å¯¹è¯")
    
    async def _generate_conversation_response(self, state: AgentState) -> str:
        """åŸºäºå·¥å…·æ‰§è¡Œç»“æœå’Œå¯¹è¯å†å²ç”Ÿæˆæ™ºèƒ½å“åº” - è®©å¤§æ¨¡å‹å¯¹æ‰€æœ‰å·¥å…·ç»“æœè¿›è¡Œæ™ºèƒ½æ€»ç»“"""
        user_input = state.input
        conversation_history = state.conversation_history
        
        # æ„å»ºè¯¦ç»†çš„å·¥å…·æ‰§è¡Œç»“æœ - åŒ…å«æ‰€æœ‰æ‰§è¡Œæ­¥éª¤å’Œå…·ä½“ç»“æœ
        detailed_tool_results = []
        api_keys_collected = []  # ä¸“é—¨æ”¶é›†APIå¯†é’¥
        sleep_records = []       # ä¸“é—¨æ”¶é›†ç¡çœ è®°å½•
        
        for i, (action, result) in enumerate(state.intermediate_steps):
            tool_name = action.tool
            step_num = i + 1
            
            # è§£æå·¥å…·ç»“æœå¹¶æ”¶é›†è¯¦ç»†ä¿¡æ¯
            if isinstance(result, dict):
                detailed_tool_results.append({
                    "step": step_num,
                    "tool": tool_name,
                    "result": result
                })
                
                # ä¸“é—¨æ”¶é›†APIå¯†é’¥
                if tool_name == "APIKeyè·å–å·¥å…·" and result.get("access_granted"):
                    api_key = result.get("apikey", "")
                    api_keys_collected.append(f"ç¬¬{step_num}æ¬¡: {api_key}")
                
                # ä¸“é—¨æ”¶é›†ç¡çœ è®°å½•
                elif tool_name == "sleep_tool" and result.get("success"):
                    duration = result.get("duration", 0)
                    sleep_records.append(f"ç¬¬{step_num}æ¬¡ç¡çœ : {duration}ç§’")
                    
            else:
                # å¦‚æœä¸æ˜¯å­—å…¸ï¼Œä¹Ÿè¦è®°å½•
                detailed_tool_results.append({
                    "step": step_num,
                    "tool": tool_name,
                    "result": {"raw_output": str(result)}
                })
        
        # å¦‚æœLLMä¸å¯ç”¨ï¼Œä½¿ç”¨å¢å¼ºçš„æ ¼å¼åŒ–è¾“å‡º
        if not self.llm.api_available:
            return self._enhanced_format_results(detailed_tool_results, api_keys_collected, sleep_records, user_input)
        
        try:
            # æ„å»ºæ›´æ™ºèƒ½çš„æç¤ºè¯ï¼Œè¦æ±‚å¤§æ¨¡å‹è¿›è¡Œæ·±åº¦æ€»ç»“
            system_prompt = """ä½ æ˜¯DataFlowæ™ºèƒ½åŠ©æ‰‹ã€‚ç”¨æˆ·åˆšåˆšå®Œæˆäº†ä¸€ç³»åˆ—å·¥å…·è°ƒç”¨ï¼Œä½ éœ€è¦å¯¹æ‰§è¡Œç»“æœè¿›è¡Œæ™ºèƒ½æ€»ç»“å’Œåˆ†æã€‚

æ€»ç»“è¦æ±‚ï¼š
1. **å¿…é¡»åŒ…å«æ‰€æœ‰å…·ä½“ç»“æœ** - å¦‚æœæœ‰å¤šä¸ªAPIå¯†é’¥ï¼Œè¦å…¨éƒ¨åˆ—å‡ºï¼›å¦‚æœæœ‰ç¡çœ é—´éš”ï¼Œè¦è¯´æ˜å…·ä½“æ—¶é•¿
2. **åˆ†ææ‰§è¡Œè¿‡ç¨‹** - è¯´æ˜è°ƒç”¨é¡ºåºã€é—´éš”æ§åˆ¶ç­‰
3. **å›ç­”ç”¨æˆ·å…³å¿ƒçš„é—®é¢˜** - æ¯”å¦‚"æœ‰ä»€ä¹ˆä¸åŒ"è¦å…·ä½“å¯¹æ¯”åˆ†æ
4. **è¯­è¨€è‡ªç„¶å‹å¥½** - ä¸è¦ç”Ÿç¡¬åœ°åˆ—ä¸¾ï¼Œè¦åƒçœŸæ­£çš„åŠ©æ‰‹ä¸€æ ·äº¤æµ
5. **çªå‡ºé‡ç‚¹ä¿¡æ¯** - æŠŠç”¨æˆ·æœ€å…³å¿ƒçš„ç»“æœæ”¾åœ¨å‰é¢

ç‰¹åˆ«æ³¨æ„ï¼š
- å¦‚æœç”¨æˆ·é—®"æœ‰ä»€ä¹ˆä¸åŒ"ï¼Œè¦ä»”ç»†å¯¹æ¯”æ¯æ¬¡ç»“æœçš„å·®å¼‚
- å¦‚æœæœ‰å¤šä¸ªAPIå¯†é’¥ï¼Œå¿…é¡»å…¨éƒ¨æ˜¾ç¤ºï¼Œä¸èƒ½é—æ¼
- å¦‚æœæœ‰æ—¶é—´é—´éš”ï¼Œè¦è¯´æ˜å…·ä½“çš„ç­‰å¾…æ—¶é—´å’Œæ§åˆ¶æ•ˆæœ"""

            # æ„å»ºå¯¹è¯å†å²æ–‡æœ¬
            history_text = self._build_history_text(conversation_history, k=10, clip=300)
            
            # æ„å»ºè¯¦ç»†çš„å·¥å…·æ‰§è¡ŒæŠ¥å‘Š
            try:
                execution_report = self._build_detailed_execution_report(detailed_tool_results, api_keys_collected, sleep_records)
                logger.info(f"ğŸ“Š æ‰§è¡ŒæŠ¥å‘Šæ„å»ºæˆåŠŸï¼Œé•¿åº¦: {len(execution_report)}")
            except Exception as report_error:
                logger.error(f"æ„å»ºæ‰§è¡ŒæŠ¥å‘Šå¤±è´¥: {report_error}")
                execution_report = f"æ‰§è¡ŒæŠ¥å‘Šæ„å»ºå¤±è´¥: {str(report_error)}"
            
            user_prompt = f"""ç”¨æˆ·è¯·æ±‚: {user_input}

æ‰§è¡Œè¿‡ç¨‹è¯¦ç»†æŠ¥å‘Š:
{execution_report}

å¯¹è¯å†å²:
{history_text}

è¯·ä½ ä½œä¸ºæ™ºèƒ½åŠ©æ‰‹ï¼Œå¯¹è¿™æ¬¡æ‰§è¡Œç»“æœè¿›è¡Œå…¨é¢ã€è¯¦ç»†çš„æ€»ç»“ã€‚ç‰¹åˆ«è¦æ³¨æ„ï¼š
1. å¦‚æœç”¨æˆ·é—®"æœ‰ä»€ä¹ˆä¸åŒ"ï¼Œè¦ä»”ç»†åˆ†ææ¯æ¬¡ç»“æœçš„å…·ä½“å·®å¼‚
2. æ‰€æœ‰è·å–çš„APIå¯†é’¥éƒ½è¦å®Œæ•´å±•ç¤ºï¼Œä¸èƒ½é—æ¼
3. å¦‚æœæœ‰é—´éš”æ§åˆ¶ï¼Œè¦è¯´æ˜å…·ä½“çš„æ—¶é—´æ§åˆ¶æ•ˆæœ
4. ç”¨è‡ªç„¶çš„è¯­è¨€å›ç­”ï¼ŒåƒçœŸæ­£çš„åŠ©æ‰‹ä¸€æ ·"""

            logger.info(f"ğŸš€ å‡†å¤‡è°ƒç”¨LLMï¼Œuser_prompté•¿åº¦: {len(user_prompt)}")
            
            # è°ƒç”¨LLMç”Ÿæˆæ™ºèƒ½æ€»ç»“ - å¢åŠ è¶…æ—¶æ§åˆ¶
            try:
                import asyncio
                from concurrent.futures import ThreadPoolExecutor
                
                def sync_llm_call():
                    try:
                        llm_service = self.llm._create_llm_service()
                        # å‡å°‘é‡è¯•æ¬¡æ•°é¿å…é•¿æ—¶é—´é˜»å¡
                        llm_service.max_retries = 1
                        return llm_service.generate_from_input(
                            user_inputs=[user_prompt],
                            system_prompt=system_prompt
                        )
                    except Exception as e:
                        logger.error(f"LLMæœåŠ¡å†…éƒ¨é”™è¯¯: {e}")
                        return None
                
                # å¼‚æ­¥æ‰§è¡Œï¼Œè®¾ç½®5ç§’è¶…æ—¶
                with ThreadPoolExecutor() as executor:
                    future = executor.submit(sync_llm_call)
                    try:
                        responses = await asyncio.wait_for(
                            asyncio.wrap_future(future), 
                            timeout=50.0
                        )
                        
                        if responses and responses[0]:
                            llm_response = responses[0].strip()
                            logger.info(f"ğŸ¤– LLMæ™ºèƒ½æ€»ç»“ç”ŸæˆæˆåŠŸ: {llm_response[:100]}...")
                            return llm_response
                        else:
                            logger.warning("âš ï¸ LLMè¿”å›ç©ºå“åº”ï¼Œä½¿ç”¨fallback")
                            
                    except asyncio.TimeoutError:
                        logger.warning("âš ï¸ LLMè°ƒç”¨è¶…æ—¶ï¼ˆ5ç§’ï¼‰ï¼Œä½¿ç”¨fallback")
                    except Exception as e:
                        logger.error(f"LLMå¼‚æ­¥è°ƒç”¨é”™è¯¯: {e}")
                        
            except Exception as e:
                logger.error(f"LLMæ™ºèƒ½æ€»ç»“è°ƒç”¨å¤±è´¥: {e}")
                import traceback
                logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            
        except Exception as e:
            logger.error(f"LLMæ™ºèƒ½æ€»ç»“ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        
        # fallbackåˆ°å¢å¼ºæ ¼å¼åŒ–
        return self._enhanced_format_results(detailed_tool_results, api_keys_collected, sleep_records, user_input)
    
    def _simple_format_results(self, tool_results_summary: List[Dict[str, Any]]) -> str:
        """ç®€å•æ ¼å¼åŒ–å·¥å…·ç»“æœï¼ˆå½“LLMä¸å¯ç”¨æ—¶ï¼‰"""
        for tool_summary in tool_results_summary:
            tool_name = tool_summary["tool"]
            result = tool_summary["result"]
            
            if tool_name == "APIKeyè·å–å·¥å…·":
                if result.get("access_granted"):
                    api_key = result.get("apikey", "")
                    return f"ğŸ”‘ ä»Šå¤©çš„ç§˜å¯†APIå¯†é’¥æ˜¯: `{api_key}`"
                else:
                    return "âŒ æ— æ³•è·å–APIå¯†é’¥"
        
        return "âœ… æ“ä½œå®Œæˆ"
    
    def _enhanced_format_results(self, detailed_tool_results: List[Dict], api_keys_collected: List[str], sleep_records: List[str], user_input: str) -> str:
        """å¢å¼ºæ ¼å¼åŒ–å·¥å…·ç»“æœï¼ˆLLMä¸å¯ç”¨æ—¶çš„fallbackï¼‰"""
        if not detailed_tool_results:
            return "âœ… ä»»åŠ¡å®Œæˆï¼Œä½†æ²¡æœ‰å·¥å…·æ‰§è¡Œè®°å½•"
        
        # æ„å»ºè¯¦ç»†çš„æ‰§è¡ŒæŠ¥å‘Š
        report_lines = ["ğŸ“‹ æ‰§è¡ŒæŠ¥å‘Šï¼š"]
        
        # å¦‚æœæœ‰APIå¯†é’¥ï¼Œä¼˜å…ˆå±•ç¤º
        if api_keys_collected:
            report_lines.append(f"\nğŸ”‘ è·å–åˆ°çš„APIå¯†é’¥ï¼ˆå…±{len(api_keys_collected)}ä¸ªï¼‰ï¼š")
            for api_key_info in api_keys_collected:
                report_lines.append(f"  â€¢ {api_key_info}")
        
        # å¦‚æœæœ‰ç¡çœ è®°å½•ï¼Œå±•ç¤ºé—´éš”æ§åˆ¶
        if sleep_records:
            report_lines.append(f"\nâ° é—´éš”æ§åˆ¶è®°å½•ï¼š")
            for sleep_info in sleep_records:
                report_lines.append(f"  â€¢ {sleep_info}")
        
        # å±•ç¤ºå®Œæ•´æ‰§è¡Œæµç¨‹
        report_lines.append(f"\nğŸ“ å®Œæ•´æ‰§è¡Œæµç¨‹ï¼ˆå…±{len(detailed_tool_results)}æ­¥ï¼‰ï¼š")
        for tool_result in detailed_tool_results:
            step = tool_result["step"]
            tool = tool_result["tool"]
            result = tool_result["result"]
            
            if tool == "APIKeyè·å–å·¥å…·" and result.get("access_granted"):
                api_key = result.get("apikey", "N/A")
                report_lines.append(f"  æ­¥éª¤{step}: è·å–APIå¯†é’¥ â†’ {api_key}")
            elif tool == "sleep_tool" and result.get("success"):
                duration = result.get("duration", 0)
                report_lines.append(f"  æ­¥éª¤{step}: ç¡çœ ç­‰å¾… â†’ {duration}ç§’")
            else:
                report_lines.append(f"  æ­¥éª¤{step}: {tool} â†’ æ‰§è¡Œå®Œæˆ")
        
        # å¦‚æœç”¨æˆ·é—®"æœ‰ä»€ä¹ˆä¸åŒ"ï¼Œå°è¯•ç®€å•åˆ†æ
        if "ä¸åŒ" in user_input and api_keys_collected:
            report_lines.append(f"\nğŸ” å·®å¼‚åˆ†æï¼š")
            if len(api_keys_collected) > 1:
                report_lines.append("  â€¢ æ¯æ¬¡è·å–çš„APIå¯†é’¥éƒ½æ˜¯ä¸åŒçš„ï¼ˆåŒ…å«æ—¶é—´æˆ³ï¼‰")
                report_lines.append("  â€¢ æ—¶é—´æˆ³ä½“ç°äº†æ‰§è¡Œçš„å…ˆåé¡ºåº")
            else:
                report_lines.append("  â€¢ åªæœ‰ä¸€æ¬¡æ‰§è¡Œï¼Œæ— æ³•è¿›è¡Œå·®å¼‚å¯¹æ¯”")
        
        return "\n".join(report_lines)
    
    def _build_detailed_execution_report(self, detailed_tool_results: List[Dict], api_keys_collected: List[str], sleep_records: List[str]) -> str:
        """æ„å»ºè¯¦ç»†çš„æ‰§è¡ŒæŠ¥å‘Šä¾›LLMåˆ†æ"""
        report_sections = []
        
        # æ‰§è¡Œæ¦‚å†µ
        total_steps = len(detailed_tool_results)
        api_count = len(api_keys_collected)
        sleep_count = len(sleep_records)
        
        report_sections.append(f"æ‰§è¡Œæ¦‚å†µ: æ€»å…±{total_steps}ä¸ªæ­¥éª¤, è·å–{api_count}ä¸ªAPIå¯†é’¥, {sleep_count}æ¬¡ç¡çœ é—´éš”")
        
        # APIå¯†é’¥è¯¦æƒ…
        if api_keys_collected:
            report_sections.append("\nAPIå¯†é’¥è·å–è¯¦æƒ…:")
            for i, api_key_info in enumerate(api_keys_collected, 1):
                report_sections.append(f"  {i}. {api_key_info}")
        
        # ç¡çœ é—´éš”è¯¦æƒ…
        if sleep_records:
            report_sections.append("\nç¡çœ é—´éš”è¯¦æƒ…:")
            for i, sleep_info in enumerate(sleep_records, 1):
                report_sections.append(f"  {i}. {sleep_info}")
        
        # å®Œæ•´æ‰§è¡Œæ—¶åº
        report_sections.append(f"\nå®Œæ•´æ‰§è¡Œæ—¶åº:")
        for tool_result in detailed_tool_results:
            step = tool_result["step"]
            tool = tool_result["tool"]
            result = tool_result["result"]
            
            if tool == "APIKeyè·å–å·¥å…·":
                if result.get("access_granted"):
                    api_key = result.get("apikey", "N/A")
                    timestamp = api_key.split('_')[-1] if '_' in api_key else "æ— æ—¶é—´æˆ³"
                    report_sections.append(f"  æ­¥éª¤{step}: [APIå¯†é’¥è·å–] æˆåŠŸ â†’ å¯†é’¥: {api_key} (æ—¶é—´æˆ³: {timestamp})")
                else:
                    report_sections.append(f"  æ­¥éª¤{step}: [APIå¯†é’¥è·å–] å¤±è´¥")
            elif tool == "sleep_tool":
                if result.get("success"):
                    duration = result.get("duration", 0)
                    label = result.get("label", "æœªçŸ¥")
                    report_sections.append(f"  æ­¥éª¤{step}: [ç¡çœ é—´éš”] æˆåŠŸ â†’ ç­‰å¾…{duration}ç§’ (æ ‡ç­¾: {label})")
                else:
                    report_sections.append(f"  æ­¥éª¤{step}: [ç¡çœ é—´éš”] å¤±è´¥")
            else:
                status = "æˆåŠŸ" if result.get("success", True) else "å¤±è´¥"
                report_sections.append(f"  æ­¥éª¤{step}: [å…¶ä»–å·¥å…·: {tool}] {status}")
        
        return "\n".join(report_sections)
    
    async def _get_direct_conversation_response(self, state: AgentState) -> str:
        """å½“æ²¡æœ‰å·¥å…·æ‰§è¡Œæ—¶ï¼Œè·å–ç›´æ¥å¯¹è¯å›å¤"""
        user_input = state.input
        conversation_history = state.conversation_history
        
        # ç›´æ¥ä½¿ç”¨LLMï¼Œä¸åšfallback
        if self.llm.api_available:
            try:
                system_prompt = "ä½ æ˜¯DataFlowæ™ºèƒ½åŠ©æ‰‹ï¼Œè¯·ç›´æ¥ã€è‡ªç„¶åœ°å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"
                
                # æ„å»ºå¯¹è¯å†å²
                history_text = self._build_history_text(conversation_history, k=8, clip=200)
                
                user_prompt = f"""ç”¨æˆ·é—®é¢˜: {user_input}

å¯¹è¯å†å²:
{history_text}

è¯·åŸºäºå¯¹è¯å†å²è‡ªç„¶åœ°å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"""
                
                llm_service = self.llm._create_llm_service()
                responses = llm_service.generate_from_input(
                    user_inputs=[user_prompt],
                    system_prompt=system_prompt
                )
                
                if responses and responses[0]:
                    return responses[0].strip()
                    
            except Exception as e:
                logger.error(f"LLMè°ƒç”¨å¤±è´¥: {e}")
                raise e
        
        # LLMä¸å¯ç”¨æ—¶æŠ›å‡ºå¼‚å¸¸
        raise Exception("LLMæœåŠ¡ä¸å¯ç”¨ï¼Œæ— æ³•å¤„ç†å¯¹è¯")

    async def _planner(self, state: AgentState) -> PlannerOutput:
        """æ™ºèƒ½è§„åˆ’å™¨ï¼šåŸºäºç”¨æˆ·éœ€æ±‚å’Œå·²æ‰§è¡Œçš„å·¥å…·ç»“æœï¼Œå†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨"""
        user_input = state.input
        tool_results = state.tool_results
        
        # åˆ†æç”¨æˆ·åŸå§‹éœ€æ±‚ä¸­çš„å…³é”®ä¿¡æ¯
        needs_analysis = self._analyze_user_needs(user_input, tool_results)
        
        logger.info(f"ğŸ¯ è§„åˆ’å™¨åˆ†æ: {needs_analysis}")
        
        # åŸºäºåˆ†æç»“æœå†³å®šä¸‹ä¸€æ­¥
        if needs_analysis["should_continue"]:
            next_action = needs_analysis["next_action"]
            logger.info(f"âœ… è§„åˆ’å™¨å†³å®šç»§ç»­: {next_action}")
            return PlannerOutput(
                decision="continue",
                next_actions=[next_action],
                reasons="; ".join(needs_analysis["reasons"]) if isinstance(needs_analysis["reasons"], list) else needs_analysis["reasons"]
            )
        else:
            logger.info(f"ğŸ è§„åˆ’å™¨å†³å®šç»“æŸ: {needs_analysis['reasons']}")
            # ç”Ÿæˆç®€å•çš„æ€»ç»“æ¶ˆæ¯ï¼Œé¿å…å¤æ‚çš„LLMè°ƒç”¨
            summary_msg = "ä»»åŠ¡å·²å®Œæˆ"
            if state.tool_results:
                latest_result = state.tool_results[-1]
                if latest_result.get("tool") == "APIKeyè·å–å·¥å…·":
                    summary_msg = f"å·²æˆåŠŸè·å–APIå¯†é’¥: {latest_result.get('result', {}).get('api_key', 'N/A')}"
                elif latest_result.get("tool") == "sleep_tool":
                    summary_msg = "å·²å®Œæˆç­‰å¾…ä»»åŠ¡"
                else:
                    summary_msg = f"å·²å®Œæˆ{latest_result.get('tool', 'å·¥å…·')}æ‰§è¡Œ"
            
            return PlannerOutput(
                decision="finish",
                user_message=summary_msg,
                reasons="; ".join(needs_analysis["reasons"]) if isinstance(needs_analysis["reasons"], list) else needs_analysis["reasons"]
            )

    def _analyze_user_needs(self, user_input: str, tool_results: List[Dict]) -> Dict[str, Any]:
        """è®©LLMæ™ºèƒ½åˆ†æç”¨æˆ·éœ€æ±‚å’Œå½“å‰æ‰§è¡ŒçŠ¶æ€ï¼Œå†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨ - å®Œå…¨åŸºäºLLMå†³ç­–"""
        
        # ğŸ”§ æ ¸å¿ƒä¿®å¤ï¼šå®Œå…¨å»æ‰å…³é”®è¯åŒ¹é…ï¼Œè®©LLMæ¥ç†è§£å’Œå†³ç­–
        if not self.llm.api_available:
            # LLMä¸å¯ç”¨æ—¶çš„ç®€å•fallback
            return {
                "should_continue": False,
                "next_action": None,
                "reasons": ["LLMä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œæ™ºèƒ½å†³ç­–"],
                "analysis": {}
            }
        
        try:
            # æ„å»ºå¯ç”¨å·¥å…·çš„è¯¦ç»†æè¿°
            available_tools = []
            for tool in self.tools:
                tool_info = {
                    "name": tool.name(),
                    "description": tool.description()
                }
                available_tools.append(tool_info)
            
            # æ„å»ºæ‰§è¡Œå†å²
            execution_history = []
            for i, result in enumerate(tool_results, 1):
                tool_name = result.get("tool", "unknown")
                success = result.get("ok", False)
                payload = result.get("payload", {})
                
                step_info = f"æ­¥éª¤{i}: æ‰§è¡Œäº†{tool_name}"
                if success:
                    if isinstance(payload, dict) and payload.get("success"):
                        step_info += " - æˆåŠŸ"
                        if "apikey" in payload:
                            step_info += f" (è·å¾—APIå¯†é’¥: {payload['apikey']})"
                        elif "duration" in payload:
                            step_info += f" (ç­‰å¾…äº†{payload['duration']}ç§’)"
                    else:
                        step_info += " - å®Œæˆ"
                else:
                    step_info += " - å¤±è´¥"
                
                execution_history.append(step_info)
            
            # æ„å»ºLLMå†³ç­–æç¤ºè¯
            system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½å†³ç­–åŠ©æ‰‹ï¼Œéœ€è¦æ ¹æ®ç”¨æˆ·éœ€æ±‚å’Œå½“å‰æ‰§è¡Œæƒ…å†µï¼Œå†³å®šä¸‹ä¸€æ­¥åº”è¯¥æ‰§è¡Œä»€ä¹ˆå·¥å…·ã€‚

å†³ç­–åŸåˆ™ï¼š
1. ä»”ç»†ç†è§£ç”¨æˆ·çš„å®Œæ•´éœ€æ±‚ï¼ŒåŒ…æ‹¬è¦æ‰§è¡Œå¤šå°‘æ¬¡ã€æ˜¯å¦éœ€è¦é—´éš”ç­‰
2. åˆ†æå½“å‰çš„æ‰§è¡Œå†å²ï¼Œäº†è§£å·²ç»å®Œæˆäº†ä»€ä¹ˆ
3. åŸºäºå·¥å…·çš„åŠŸèƒ½æè¿°ï¼Œé€‰æ‹©æœ€åˆé€‚çš„ä¸‹ä¸€æ­¥åŠ¨ä½œ
4. æ¯æ¬¡åªå†³ç­–ä¸€ä¸ªåŠ¨ä½œï¼Œä¸è¦ä¸€æ¬¡æ€§è§„åˆ’å¤šä¸ªæ­¥éª¤
5. å¦‚æœä»»åŠ¡å·²å®Œæˆï¼Œåº”è¯¥é€‰æ‹©ç»“æŸ

è¿”å›æ ¼å¼ï¼ˆå¿…é¡»æ˜¯æœ‰æ•ˆçš„JSONï¼‰ï¼š
{
    "should_continue": true/false,
    "next_tool": "å·¥å…·åç§°" æˆ– null,
    "reasoning": "è¯¦ç»†çš„å†³ç­–åŸå› ",
    "task_progress": "å½“å‰ä»»åŠ¡è¿›åº¦åˆ†æ"
}"""

            user_prompt = f"""ç”¨æˆ·åŸå§‹éœ€æ±‚: {user_input}

å¯ç”¨å·¥å…·åˆ—è¡¨:
{chr(10).join([f"- {tool['name']}: {tool['description']}" for tool in available_tools])}

å½“å‰æ‰§è¡Œå†å²:
{chr(10).join(execution_history) if execution_history else "è¿˜æ²¡æœ‰æ‰§è¡Œä»»ä½•å·¥å…·"}

è¯·åˆ†æå½“å‰æƒ…å†µï¼Œå†³å®šä¸‹ä¸€æ­¥åº”è¯¥ï¼š
1. ç»§ç»­æ‰§è¡ŒæŸä¸ªå·¥å…·ï¼ˆå¦‚æœä»»åŠ¡æœªå®Œæˆï¼‰
2. è¿˜æ˜¯ç»“æŸä»»åŠ¡ï¼ˆå¦‚æœå·²ç»æ»¡è¶³ç”¨æˆ·éœ€æ±‚ï¼‰

æ³¨æ„ï¼šæ¯æ¬¡åªèƒ½é€‰æ‹©ä¸€ä¸ªä¸‹ä¸€æ­¥åŠ¨ä½œï¼Œä¸è¦åŒæ—¶è§„åˆ’å¤šä¸ªæ­¥éª¤ã€‚"""

            logger.info(f"ğŸ¤– è°ƒç”¨LLMè¿›è¡Œæ™ºèƒ½å†³ç­–...")
            
            # è°ƒç”¨LLMè¿›è¡Œå†³ç­–
            llm_service = self.llm._create_llm_service()
            responses = llm_service.generate_from_input(
                user_inputs=[user_prompt],
                system_prompt=system_prompt
            )
            
            if responses and responses[0]:
                content = responses[0].strip()
                logger.info(f"ğŸ¤– LLMå†³ç­–å“åº”: {content}")
                
                # è§£æLLMå“åº”
                try:
                    import json
                    decision = json.loads(content)
                    
                    should_continue = decision.get("should_continue", False)
                    next_tool = decision.get("next_tool")
                    reasoning = decision.get("reasoning", "")
                    task_progress = decision.get("task_progress", "")
                    
                    # æ„å»ºnext_action
                    next_action = None
                    if should_continue and next_tool:
                        next_action = {
                            "tool": next_tool,
                            "tool_input": {"user_message": user_input}
                        }
                    
                    result = {
                        "should_continue": should_continue,
                        "next_action": next_action,
                        "reasons": [reasoning],
                        "analysis": {
                            "task_progress": task_progress,
                            "llm_decision": decision,
                            "execution_count": len(tool_results)
                        }
                    }
                    
                    logger.info(f"ğŸ¯ LLMå†³ç­–ç»“æœ: continue={should_continue}, next_tool={next_tool}")
                    logger.info(f"ğŸ¯ å†³ç­–åŸå› : {reasoning}")
                    
                    return result
                    
                except json.JSONDecodeError as e:
                    logger.error(f"LLMå“åº”JSONè§£æå¤±è´¥: {e}")
                    logger.error(f"åŸå§‹å“åº”: {content}")
                    
                    # å°è¯•ç®€å•è§£æ
                    return self._simple_parse_llm_response(content, tool_results)
            
        except Exception as e:
            logger.error(f"LLMæ™ºèƒ½å†³ç­–å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
        
        # å‡ºé”™æ—¶çš„fallback
        return {
            "should_continue": False,
            "next_action": None,
            "reasons": ["æ™ºèƒ½å†³ç­–å¤±è´¥ï¼Œç»“æŸä»»åŠ¡"],
            "analysis": {}
        }
    
    def _simple_parse_llm_response(self, content: str, tool_results: List[Dict]) -> Dict[str, Any]:
        """ç®€å•è§£æLLMå“åº”çš„fallbackæ–¹æ³•"""
        content_lower = content.lower()
        
        # ç®€å•åˆ¤æ–­æ˜¯å¦åº”è¯¥ç»§ç»­
        should_continue = False
        next_tool = None
        reasoning = "åŸºäºæ–‡æœ¬å†…å®¹çš„ç®€å•è§£æ"
        
        if "ç»§ç»­" in content or "continue" in content_lower or "true" in content_lower:
            should_continue = True
            
            # å°è¯•æ‰¾åˆ°å·¥å…·å
            for tool in self.tools:
                tool_name = tool.name()
                if tool_name in content:
                    next_tool = tool_name
                    break
        
        # æ„å»ºnext_action
        next_action = None
        if should_continue and next_tool:
            next_action = {
                "tool": next_tool,
                "tool_input": {"user_message": "ç»§ç»­æ‰§è¡Œ"}
            }
        
        return {
            "should_continue": should_continue,
            "next_action": next_action,
            "reasons": [reasoning],
            "analysis": {"simple_parse": True, "execution_count": len(tool_results)}
        }


class MasterAgentExecutor:
    """Master Agent æ‰§è¡Œå™¨ - ä½¿ç”¨LangGraphæ‰§è¡Œ"""
    
    def __init__(self, agent: MasterAgent):
        self.agent = agent
    
    async def execute(self, user_input: str, session_id: str = None) -> Dict[str, Any]:
        """æ‰§è¡Œç”¨æˆ·è¯·æ±‚ - ä½¿ç”¨LangGraphæˆ–Fallbackæ‰§è¡Œå™¨"""
        try:
            # è·å–æˆ–åˆ›å»ºä¼šè¯å†å²
            if session_id not in self.agent.conversation_sessions:
                self.agent.conversation_sessions[session_id] = []
            
            conversation_history = self.agent.conversation_sessions[session_id]
            
            # åˆå§‹åŒ–çŠ¶æ€
            initial_state = {
                "input": user_input,
                "session_id": session_id,
                "conversation_history": conversation_history.copy(),
                "agent_outcome": None,
                "intermediate_steps": [],
                "current_step": "bootstrap"
            }
            
            if  self.agent.compiled_graph:
                # ä½¿ç”¨çœŸæ­£çš„LangGraphæ‰§è¡Œ
                logger.info("ğŸš€ ä½¿ç”¨LangGraphæ‰§è¡Œ")
                
                final_state = await self.agent.compiled_graph.ainvoke(initial_state)
                
                # è·å–æœ€ç»ˆè¾“å‡º
                agent_outcome = final_state.get("agent_outcome")
                if agent_outcome and hasattr(agent_outcome, 'return_values'):
                    output = agent_outcome.return_values.get("output", "æ‰§è¡Œå®Œæˆ")
                else:
                    output = "æ‰§è¡Œå®Œæˆï¼Œä½†æœªè·å–åˆ°è¾“å‡º"
                
                logger.info(f"âœ… LangGraphæ‰§è¡Œå®Œæˆ")
                
            
            # ä¿å­˜å¯¹è¯å†å²
            conversation_history.append({"role": "user", "content": user_input})
            conversation_history.append({"role": "assistant", "content": output})
            
            # ä¿æŒå†å²é•¿åº¦åœ¨åˆç†èŒƒå›´å†…ï¼ˆæœ€è¿‘20è½®å¯¹è¯ï¼‰
            if len(conversation_history) > 40:
                conversation_history = conversation_history[-40:]
            
            self.agent.conversation_sessions[session_id] = conversation_history
            
            return {
                "success": True,
                "output": output,
                "session_id": session_id
            }
                
        except Exception as e:
            logger.error(f"Master Agent æ‰§è¡Œå¤±è´¥: {e}")
            return {
                "success": False,
                "output": f"æ‰§è¡Œå¤±è´¥: {str(e)}",
                "session_id": session_id
            }
    
    async def run_with_events(
        self,
        user_input: str,
        session_id: str,
        sink: EventSink,
    ) -> Dict[str, Any]:
        """æ–°çš„äº‹ä»¶é©±åŠ¨å¤šè½®ç¼–æ’æ‰§è¡Œ"""
        state = AgentState(
            input=user_input,
            session_id=session_id,
            conversation_history=self.agent.conversation_sessions.get(session_id, []),
            agent_outcome=None,
            intermediate_steps=[],
            current_step="bootstrap",
        )

        await sink.emit(Event(
            session_id=session_id, step_id=new_step_id(),
            event=EventType.RUN_STARTED, ts=time.time(),
            data={"input": user_input}
        ))

        # 1) bootstrapï¼šåˆå§‹è·¯ç”±æˆ–ç›´æ¥å¾—åˆ°ç¬¬ä¸€æ‰¹ actions
        state = await self.agent.bootstrap_node(state)

        # å¦‚æœ bootstrap ç›´æ¥ç»™äº† LCAgentFinishï¼Œå°±ç»“æŸ
        if hasattr(state.agent_outcome, "return_values"):
            await sink.emit(Event(
                session_id=session_id, step_id=new_step_id(),
                event=EventType.RUN_FINISHED, ts=time.time(),
                data={"output": state.agent_outcome.return_values}
            ))
            return {"success": True, "output": state.agent_outcome.return_values.get("output", "")}
            
        # æ ‡å‡†åŒ–æˆ pending_actions
        state.pending_actions = list(state.agent_outcome) if isinstance(state.agent_outcome, list) else []
        
        # ä¸»å¾ªç¯
        state.tool_results = []
        state.loop_guard = 0
        MAX_STEPS = 8

        while state.loop_guard < MAX_STEPS:
            # è‹¥æ²¡æœ‰å¾…æ‰§è¡ŒåŠ¨ä½œï¼Œè¿›å…¥ planner å†³ç­–ï¼ˆå¯èƒ½ finish æˆ–è¿½åŠ  actionsï¼‰
            if not state.pending_actions:
                plan = await self._planner(state)
                await sink.emit(Event(
                    session_id=session_id, step_id=new_step_id(),
                    event=EventType.PLAN_DECISION, ts=time.time(),
                    data=plan.model_dump()
                ))
                if plan.decision == "finish":
                    final_text = plan.user_message or await self._summarize(state)
                    await sink.emit(Event(
                        session_id=session_id, step_id=new_step_id(),
                        event=EventType.RUN_FINISHED, ts=time.time(),
                        data={"output": final_text}
                    ))
                    self._append_history(session_id, user_input, final_text)
                    return {"success": True, "output": final_text}
                else:
                    for na in plan.next_actions:
                        state.pending_actions.append(LCAgentAction(
                            tool=na["tool"], 
                            tool_input=na.get("tool_input", {}), 
                            log=na.get("log", "")
                        ))

            # å–å‡ºä¸€ä¸ª action æ‰§è¡Œ
            action = state.pending_actions.pop(0)
            await sink.emit(Event(
                session_id=session_id, step_id=new_step_id(),
                event=EventType.TOOL_STARTED, ts=time.time(),
                data={"tool": action.tool, "tool_input": action.tool_input}
            ))

            try:
                # ç”¨ ToolExecutor ç»Ÿä¸€æ‰§è¡Œ
                result = await self.agent.tool_executor.ainvoke(action)
                norm = self._normalize_tool_result(action.tool, result)
                state.tool_results.append(norm)
                await sink.emit(Event(
                    session_id=session_id, step_id=new_step_id(),
                    event=EventType.TOOL_RESULT, ts=time.time(),
                    data={"tool": action.tool, "result": norm}
                ))
            except Exception as e:
                err = {"success": False, "error": str(e)}
                state.tool_results.append({"tool": action.tool, "ok": False, "payload": err})
                await sink.emit(Event(
                    session_id=session_id, step_id=new_step_id(),
                    event=EventType.TOOL_ERROR, ts=time.time(),
                    data={"tool": action.tool, "error": str(e)}
                ))

            state.loop_guard += 1

        # è¶…è¿‡æœ€å¤§æ­¥æ•°ï¼Œå…œåº•ç»“æŸ
        final_text = await self._summarize(state)
        await sink.emit(Event(
            session_id=session_id, step_id=new_step_id(),
            event=EventType.RUN_FINISHED, ts=time.time(),
            data={"output": final_text, "reason": "max_steps_reached"}
        ))
        self._append_history(session_id, user_input, final_text)
        return {"success": True, "output": final_text}

    async def _summarize(self, state: AgentState) -> str:
        """åŸºäºå·¥å…·ç»“æœç”Ÿæˆæ€»ç»“"""
        return await self.agent._generate_conversation_response(state)

    def _append_history(self, session_id: str, user_input: str, output: str):
        """æ·»åŠ å¯¹è¯å†å²"""
        conv = self.agent.conversation_sessions.get(session_id, [])
        conv.append({"role": "user", "content": user_input})
        conv.append({"role": "assistant", "content": output})
        self.agent.conversation_sessions[session_id] = conv

    def _normalize_tool_result(self, tool: str, raw: Any) -> Dict[str, Any]:
        """æ ‡å‡†åŒ–å·¥å…·ç»“æœ"""
        return {
            "tool": tool,
            "ok": bool(raw.get("success", True)) if isinstance(raw, dict) else True,
            "payload": raw,
            "summary": raw.get("analysis_result") if isinstance(raw, dict) and "analysis_result" in raw else str(raw)[:200]
        }


# å·¥å‚å‡½æ•°
def create_master_agent() -> Tuple[MasterAgent, MasterAgentExecutor]:
    """åˆ›å»º Master Agent å’Œæ‰§è¡Œå™¨"""
    agent = MasterAgent()
    executor = MasterAgentExecutor(agent)
    return agent, executor
