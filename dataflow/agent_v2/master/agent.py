"""
DataFlow Master Agent
åŸºäº MyScaleKB-Agent æ¶æ„çš„ä¸»æ§æ™ºèƒ½ä½“ - ä½¿ç”¨çœŸæ­£çš„LangGraphå·¥ä½œæµ
"""
import logging
import asyncio
from typing import Dict, List, Any, Union, Optional, Tuple
from pydantic import BaseModel
from dataclasses import dataclass
from enum import Enum

# LangGraphæ ¸å¿ƒç»„ä»¶
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolExecutor
from langchain_core.tools import StructuredTool
from langchain_core.agents import AgentFinish as LCAgentFinish, AgentAction as LCAgentAction
LANGGRAPH_AVAILABLE = True
from dataflow.agent_v2.base.core import SubAgent, GraphBuilder, BaseTool, node, entry, conditional_edge
from dataflow.agent.agentrole.former import FormerAgent
from dataflow.agent.xmlforms.models import FormRequest, FormResponse
from dataflow.agent_v2.llm_client import get_llm_client
from dataflow.agent_v2.subagents.apikey_agent import APIKeyTool

logger = logging.getLogger(__name__)


class AgentState(BaseModel):
    """Master Agent çŠ¶æ€å®šä¹‰ - å…¼å®¹LangGraphçš„TypedDict"""
    input: str = ""
    agent_outcome: Optional[Any] = None
    intermediate_steps: List[Tuple[Any, str]] = []
    session_id: Optional[str] = None
    current_step: str = "bootstrap"
    form_data: Optional[Dict[str, Any]] = None
    xml_content: Optional[str] = None
    execution_result: Optional[str] = None
    conversation_history: List[Dict[str, str]] = []  # å¯¹è¯å†å²
    last_tool_results: Optional[Dict[str, Any]] = None  # æœ€è¿‘çš„å·¥å…·ç»“æœ
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]):
        return cls(**data)


class ActionType(Enum):
    """åŠ¨ä½œç±»å‹"""
    TOOL_EXECUTION = "tool_execution"
    SUB_AGENT_FORWARD = "sub_agent_forward"
    GENERAL_CONVERSATION = "general_conversation"
    END = "end"


@dataclass
class AgentAction:
    """ä»£ç†åŠ¨ä½œ"""
    tool: str
    tool_input: Dict[str, Any]
    action_type: ActionType = ActionType.TOOL_EXECUTION


@dataclass 
class AgentFinish:
    """ä»£ç†ç»“æŸ"""
    return_values: Dict[str, Any]
    log: str = ""


class FormerAgentTool(BaseTool):
    """Former Agent å·¥å…·å°è£…"""
    
    def __init__(self):
        self.former_agent = FormerAgent()
    
    @classmethod
    def name(cls) -> str:
        return "former_agent"
    
    @classmethod 
    def description(cls) -> str:
        return "å¤„ç†ç”¨æˆ·å¯¹è¯ï¼Œç”ŸæˆXMLè¡¨å•ã€‚é€‚ç”¨äºéœ€è¦æ”¶é›†ç”¨æˆ·éœ€æ±‚å¹¶ç”Ÿæˆç»“æ„åŒ–é…ç½®çš„åœºæ™¯ã€‚"
    
    def params(self) -> type[BaseModel]:
        class FormerParams(BaseModel):
            user_query: str
            session_id: Optional[str] = None
            conversation_history: List[Dict[str, str]] = []
        return FormerParams
    
    async def execute(self, user_query: str, session_id: str = None, conversation_history: List[Dict[str, str]] = None) -> Dict[str, Any]:
        """æ‰§è¡Œ Former Agent"""
        try:
            form_request = FormRequest(
                user_query=user_query,
                session_id=session_id,
                conversation_history=conversation_history or []
            )
            
            response = await self.former_agent.process_conversation(form_request)
            
            return {
                "success": True,
                "need_more_info": response.need_more_info,
                "agent_response": response.agent_response,
                "xml_form": response.xml_form,
                "form_type": response.form_type,
                "session_id": session_id
            }
        except Exception as e:
            logger.error(f"Former Agent æ‰§è¡Œå¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "agent_response": "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯ã€‚"
            }


class DataAnalysisTool(BaseTool):
    """æ•°æ®åˆ†æå·¥å…·"""
    
    @classmethod
    def name(cls) -> str:
        return "data_analysis"
    
    @classmethod
    def description(cls) -> str:
        return "åˆ†ææ•°æ®é›†å¹¶æä¾›æ´å¯Ÿã€‚å¯ä»¥å¤„ç†CSVã€JSONç­‰æ ¼å¼çš„æ•°æ®ã€‚"
    
    def params(self) -> type[BaseModel]:
        class AnalysisParams(BaseModel):
            data_path: str
            analysis_type: str = "basic"
            output_format: str = "summary"
        return AnalysisParams
    
    async def execute(self, data_path: str, analysis_type: str = "basic", output_format: str = "summary") -> Dict[str, Any]:
        """æ‰§è¡Œæ•°æ®åˆ†æ"""
        await asyncio.sleep(1)  # æ¨¡æ‹Ÿå¼‚æ­¥å¤„ç†
        
        return {
            "success": True,
            "analysis_result": f"å¯¹ {data_path} è¿›è¡Œäº† {analysis_type} åˆ†æ",
            "insights": [
                "æ•°æ®è´¨é‡è‰¯å¥½",
                "å‘ç°3ä¸ªä¸»è¦æ¨¡å¼",
                "å»ºè®®è¿›è¡Œè¿›ä¸€æ­¥æ¸…æ´—"
            ],
            "output_format": output_format
        }


class CodeGeneratorTool(BaseTool):
    """ä»£ç ç”Ÿæˆå·¥å…·"""
    
    @classmethod
    def name(cls) -> str:
        return "code_generator"
    
    @classmethod
    def description(cls) -> str:
        return "æ ¹æ®éœ€æ±‚ç”Ÿæˆ DataFlow ç®—å­ä»£ç ã€‚æ”¯æŒå¤šç§ç¼–ç¨‹æ¨¡å¼å’Œæ•°æ®å¤„ç†ä»»åŠ¡ã€‚"
    
    def params(self) -> type[BaseModel]:
        class CodeGenParams(BaseModel):
            requirements: str
            operator_type: str = "processor"
            language: str = "python"
        return CodeGenParams
    
    async def execute(self, requirements: str, operator_type: str = "processor", language: str = "python") -> Dict[str, Any]:
        """ç”Ÿæˆä»£ç """
        await asyncio.sleep(1.5)  # æ¨¡æ‹Ÿä»£ç ç”Ÿæˆæ—¶é—´
        
        generated_code = f'''
def {operator_type}_operator(data):
    """
    æ ¹æ®éœ€æ±‚ç”Ÿæˆçš„ç®—å­: {requirements}
    """
    # TODO: å®ç°å…·ä½“é€»è¾‘
    result = process_data(data)
    return result

def process_data(data):
    # å¤„ç†æ•°æ®çš„æ ¸å¿ƒé€»è¾‘
    return data
'''
        
        return {
            "success": True,
            "generated_code": generated_code,
            "operator_type": operator_type,
            "language": language,
            "file_path": f"generated_{operator_type}_operator.py"
        }


class MasterAgent:
    """DataFlow Master Agent - çœŸæ­£çš„LangGraphæ¶æ„"""
    
    def __init__(self):
        self.llm = get_llm_client()  # åˆå§‹åŒ–çœŸæ­£çš„LLMå®¢æˆ·ç«¯
        self.forward_paths = {}
        self.sub_agents = {}
        self.conversation_sessions = {}  # ä¼šè¯ç®¡ç†
        self.tools = []
        self.compiled_graph = None
        
        # æ³¨å†Œå·¥å…·
        self._register_tools()
        
        # æ„å»ºLangGraph
        if LANGGRAPH_AVAILABLE:
            self._build_langgraph()
        else:
            logger.warning("LangGraphä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬")
    
    def _register_tools(self):
        """æ³¨å†Œå·¥å…·"""
        try:
            self.tools = [
                FormerAgentTool(),
                DataAnalysisTool(),
                CodeGeneratorTool(),
                APIKeyTool()  # æ–°å¢APIå¯†é’¥å·¥å…·
            ]
            logger.info(f"å·²æ³¨å†Œ {len(self.tools)} ä¸ªå·¥å…·")
        except Exception as e:
            logger.error(f"å·¥å…·æ³¨å†Œå¤±è´¥: {e}")
            self.tools = []
    
    def _build_langgraph(self):
        """æ„å»ºçœŸæ­£çš„LangGraphå·¥ä½œæµ - å‚ç…§MyScaleKB-Agent"""
        if not LANGGRAPH_AVAILABLE:
            return
        
        try:
            # åˆ›å»ºStateGraph
            workflow = StateGraph(AgentState)
            
            # æ·»åŠ èŠ‚ç‚¹ - å‚ç…§MyScaleKB-Agentçš„èŠ‚ç‚¹ç»“æ„
            workflow.add_node("bootstrap", self.bootstrap_node)
            workflow.add_node("execute_tools", self.execute_tools_node)
            workflow.add_node("general_conversation", self.general_conversation_node)
            workflow.add_node("summarize", self.summarize_node)
            
            # è®¾ç½®å…¥å£ç‚¹
            workflow.set_entry_point("bootstrap")
            
            # æ·»åŠ æ¡ä»¶è¾¹ - å‚ç…§MyScaleKB-Agentçš„action_forwardé€»è¾‘
            workflow.add_conditional_edges(
                "bootstrap",
                self.action_forward,
                {
                    "execute_tools": "execute_tools",
                    "general_conversation": "general_conversation", 
                    "end": "summarize"
                }
            )
            
            # æ·»åŠ æ™®é€šè¾¹
            workflow.add_edge("execute_tools", "summarize")
            workflow.add_edge("general_conversation", "summarize")
            workflow.add_edge("summarize", END)
            
            # ç¼–è¯‘å›¾
            self.compiled_graph = workflow.compile()
            logger.info("âœ… LangGraphå·¥ä½œæµæ„å»ºæˆåŠŸ")
            
        except Exception as e:
            logger.error(f"LangGraphæ„å»ºå¤±è´¥: {e}")
            self.compiled_graph = None
    
    async def bootstrap_node(self, state: AgentState) -> AgentState:
        """å¼•å¯¼èŠ‚ç‚¹ - å‚ç…§MyScaleKB-Agentçš„bootstrapæ¨¡å¼"""
        user_input = state.input
        logger.info(f"ğŸ”„ BootstrapèŠ‚ç‚¹: {user_input}")
        
        # è·å–å¯ç”¨å·¥å…·åˆ—è¡¨
        available_tools = [tool.name() for tool in self.tools]
        logger.info(f"ğŸ”§ å¯ç”¨å·¥å…·åˆ—è¡¨: {available_tools}")
        
        # ä½¿ç”¨LLMåˆ†æç”¨æˆ·æ„å›¾
        try:
            intent_analysis = self.llm.analyze_user_intent(user_input, available_tools)
            
            selected_tool = intent_analysis.get("selected_tool")
            confidence = intent_analysis.get("confidence", 0.0)
            parameters = intent_analysis.get("parameters", {})
            
            logger.info(f"ğŸ¯ æ„å›¾åˆ†æç»“æœ: å·¥å…·={selected_tool}, ç½®ä¿¡åº¦={confidence}")
            logger.info(f"ğŸ“‹ å®Œæ•´æ„å›¾åˆ†æ: {intent_analysis}")
            
            if selected_tool and confidence > 0.3:
                # æœ‰åˆé€‚çš„å·¥å…·ï¼Œæ‰§è¡Œå·¥å…·
                logger.info(f"âœ… é€‰æ‹©æ‰§è¡Œå·¥å…·: {selected_tool} (ç½®ä¿¡åº¦: {confidence})")
                if LANGGRAPH_AVAILABLE:
                    action = LCAgentAction(
                        tool=selected_tool,
                        tool_input=parameters,
                        log=f"é€‰æ‹©å·¥å…·: {selected_tool}"
                    )
                else:
                    action = AgentAction(
                        tool=selected_tool,
                        tool_input=parameters,
                        action_type=ActionType.TOOL_EXECUTION
                    )
                state.agent_outcome = [action]
                logger.info(f"ğŸš€ åˆ›å»ºå·¥å…·æ‰§è¡ŒAction: {action}")
            else:
                # æ²¡æœ‰åˆé€‚çš„å·¥å…·ï¼Œæ ‡è®°ä¸ºéœ€è¦é€šç”¨å¯¹è¯
                logger.info(f"âŒ æ²¡æœ‰åˆé€‚å·¥å…·ï¼Œä½¿ç”¨é€šç”¨å¯¹è¯ (ç½®ä¿¡åº¦: {confidence})")
                if LANGGRAPH_AVAILABLE:
                    action = LCAgentAction(
                        tool="general_conversation",
                        tool_input={"user_input": user_input},
                        log="é€šç”¨å¯¹è¯"
                    )
                else:
                    action = AgentAction(
                        tool="general_conversation",
                        tool_input={"user_input": user_input},
                        action_type=ActionType.GENERAL_CONVERSATION
                    )
                state.agent_outcome = [action]
                logger.info(f"ğŸ’¬ åˆ›å»ºé€šç”¨å¯¹è¯Action: {action}")
                
        except Exception as e:
            logger.error(f"LLMæ„å›¾åˆ†æå¤±è´¥: {e}")
            
            # fallbackåˆ°ç®€å•çš„å…³é”®è¯åŒ¹é…
            selected_tool = self._simple_keyword_fallback(user_input)
            if selected_tool:
                if LANGGRAPH_AVAILABLE:
                    action = LCAgentAction(
                        tool=selected_tool["name"],
                        tool_input=selected_tool["input"],
                        log=f"å…³é”®è¯åŒ¹é…: {selected_tool['name']}"
                    )
                else:
                    action = AgentAction(
                        tool=selected_tool["name"],
                        tool_input=selected_tool["input"],
                        action_type=ActionType.TOOL_EXECUTION
                    )
                state.agent_outcome = [action]
            else:
                # fallbackä¹Ÿæ²¡æ‰¾åˆ°å·¥å…·ï¼Œæ ‡è®°ä¸ºéœ€è¦é€šç”¨å¯¹è¯
                if LANGGRAPH_AVAILABLE:
                    action = LCAgentAction(
                        tool="general_conversation",
                        tool_input={"user_input": user_input},
                        log="fallbackåˆ°é€šç”¨å¯¹è¯"
                    )
                else:
                    action = AgentAction(
                        tool="general_conversation",
                        tool_input={"user_input": user_input},
                        action_type=ActionType.GENERAL_CONVERSATION
                    )
                state.agent_outcome = [action]
        
        return state
    
    async def execute_tools_node(self, state: AgentState) -> AgentState:
        """å·¥å…·æ‰§è¡ŒèŠ‚ç‚¹ - å‚ç…§MyScaleKB-Agentçš„execute_tools"""
        agent_outcome = state.agent_outcome
        if not isinstance(agent_outcome, list):
            return state
        
        intermediate_steps = []
        
        logger.info(f"ğŸ”§ æ‰§è¡Œå·¥å…·èŠ‚ç‚¹ï¼Œå·¥å…·æ•°é‡: {len(agent_outcome)}")
        
        for action in agent_outcome:
            if LANGGRAPH_AVAILABLE and hasattr(action, 'tool'):
                tool_name = action.tool
                tool_input = action.tool_input
            elif hasattr(action, 'tool'):
                tool_name = action.tool
                tool_input = action.tool_input
            else:
                continue
                
            logger.info(f"æ‰§è¡Œå·¥å…·: {tool_name}, å‚æ•°: {tool_input}")
            
            # æŸ¥æ‰¾å¹¶æ‰§è¡Œå·¥å…·
            tool = self._find_tool(tool_name)
            if tool:
                try:
                    result = await tool.execute(**tool_input)
                    intermediate_steps.append((action, str(result)))
                    logger.info(f"å·¥å…· {tool_name} æ‰§è¡ŒæˆåŠŸ")
                except Exception as e:
                    logger.error(f"å·¥å…·æ‰§è¡Œå¤±è´¥: {e}")
                    intermediate_steps.append((action, f"æ‰§è¡Œå¤±è´¥: {str(e)}"))
            else:
                logger.warning(f"æœªæ‰¾åˆ°å·¥å…·: {tool_name}")
                intermediate_steps.append((action, f"æœªæ‰¾åˆ°å·¥å…·: {tool_name}"))
        
        state.intermediate_steps = intermediate_steps
        return state
    
    async def general_conversation_node(self, state: AgentState) -> AgentState:
        """é€šç”¨å¯¹è¯èŠ‚ç‚¹ - å¤„ç†ä¸éœ€è¦å·¥å…·çš„å¯¹è¯"""
        user_input = state.input
        conversation_history = state.conversation_history
        
        logger.info(f"ğŸ’¬ é€šç”¨å¯¹è¯èŠ‚ç‚¹: {user_input}")
        
        # å¦‚æœLLMå¯ç”¨ï¼Œä½¿ç”¨æ™ºèƒ½å¯¹è¯
        if self.llm.api_available:
            try:
                # æ„å»ºé€šç”¨å¯¹è¯æç¤ºè¯
                system_prompt = """ä½ æ˜¯DataFlowæ™ºèƒ½åŠ©æ‰‹ï¼Œä¸€ä¸ªä¸“ä¸šã€å‹å¥½ã€æ™ºèƒ½çš„AIåŠ©æ‰‹ã€‚ä½ å¯ä»¥ï¼š

1. å›ç­”å„ç§é€šç”¨é—®é¢˜ï¼ˆç§‘å­¦ã€æŠ€æœ¯ã€ç”Ÿæ´»ã€å­¦ä¹ ç­‰ï¼‰
2. æä¾›ä¸“ä¸šçš„ç¼–ç¨‹ã€æ•°æ®åˆ†æå»ºè®®
3. ååŠ©è§£å†³é—®é¢˜å’Œæä¾›åˆ›æ„æƒ³æ³•
4. è®°ä½å¯¹è¯å†å²ï¼Œä¿æŒè¿è´¯å¯¹è¯
5. å½“ç”¨æˆ·éœ€è¦æ—¶ï¼Œå»ºè®®ä½¿ç”¨ä¸“ä¸šå·¥å…·ï¼ˆAPIå¯†é’¥ã€è¡¨å•ç”Ÿæˆã€æ•°æ®å¤„ç†ç­‰ï¼‰

ä½ çš„ç‰¹ç‚¹ï¼š
- çŸ¥è¯†ä¸°å¯Œï¼Œå–„äºåˆ†æå’Œè§£é‡Š
- å›ç­”å‡†ç¡®ã€æœ‰æ¡ç†
- è¯­è¨€è‡ªç„¶ã€å‹å¥½
- èƒ½å¤Ÿæ ¹æ®ä¸Šä¸‹æ–‡ç†è§£ç”¨æˆ·çœŸæ­£çš„éœ€æ±‚
- å¦‚æœç”¨æˆ·é—®é¢˜å¯èƒ½éœ€è¦ä¸“ä¸šå·¥å…·ï¼Œä¼šä¸»åŠ¨å»ºè®®

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œä¿æŒä¸“ä¸šä½†å‹å¥½çš„è¯­æ°”ã€‚"""

                # æ„å»ºå¯¹è¯å†å²
                history_text = self._build_history_text(conversation_history, k=8, clip=200)
                
                user_prompt = f"""ç”¨æˆ·é—®é¢˜: {user_input}

å¯¹è¯å†å²:
{history_text}

è¯·åŸºäºå¯¹è¯å†å²å’Œå½“å‰é—®é¢˜ï¼Œç»™å‡ºæœ€åˆé€‚çš„å›ç­”ã€‚å¦‚æœç”¨æˆ·è¯¢é—®ä¹‹å‰çš„å¯¹è¯å†…å®¹ï¼Œè¯·å‡†ç¡®å›å¿†ã€‚å¦‚æœé—®é¢˜å¯èƒ½éœ€è¦ä¸“ä¸šå·¥å…·ååŠ©ï¼ˆå¦‚APIå¯†é’¥è·å–ã€è¡¨å•ç”Ÿæˆã€æ•°æ®åˆ†æã€ä»£ç ç”Ÿæˆç­‰ï¼‰ï¼Œè¯·ä¸»åŠ¨å»ºè®®ã€‚"""

                # è°ƒç”¨LLM
                llm_service = self.llm._create_llm_service()
                responses = llm_service.generate_from_input(
                    user_inputs=[user_prompt],
                    system_prompt=system_prompt
                )
                
                if responses and responses[0]:
                    response = responses[0].strip()
                    
                    # å¦‚æœå›å¤å¤ªçŸ­ï¼Œå°è¯•æ‰©å±•
                    if len(response) < 30:
                        followup_prompt = f"""ç”¨æˆ·é—®é¢˜: {user_input}

è¯·æä¾›ä¸€ä¸ªæ›´è¯¦ç»†ã€æ›´æœ‰å¸®åŠ©çš„å›ç­”ã€‚å³ä½¿é—®é¢˜ç®€å•ï¼Œä¹Ÿè¦ç»™å‡ºå‹å¥½çš„å›å¤å’Œå¯èƒ½çš„æ‰©å±•å»ºè®®ã€‚å¦‚æœé—®é¢˜æ¶‰åŠæŠ€æœ¯ï¼Œå¯ä»¥æä¾›ä¸€äº›ç›¸å…³èƒŒæ™¯çŸ¥è¯†ã€‚"""
                        
                        followup_responses = llm_service.generate_from_input(
                            user_inputs=[followup_prompt],
                            system_prompt=system_prompt
                        )
                        
                        if followup_responses and followup_responses[0]:
                            response = followup_responses[0].strip()
                    
                    if LANGGRAPH_AVAILABLE:
                        finish = LCAgentFinish(
                            return_values={"output": response},
                            log="é€šç”¨å¯¹è¯å®Œæˆ"
                        )
                    else:
                        finish = AgentFinish(
                            return_values={"output": response}
                        )
                    state.agent_outcome = finish
                    return state
                    
            except Exception as e:
                logger.error(f"é€šç”¨å¯¹è¯LLMè°ƒç”¨å¤±è´¥: {e}")
        
        # LLMä¸å¯ç”¨æ—¶çš„fallbacké€»è¾‘
        response = self._get_fallback_response(user_input, conversation_history)
        
        if LANGGRAPH_AVAILABLE:
            finish = LCAgentFinish(
                return_values={"output": response},
                log="Fallbackå“åº”"
            )
        else:
            finish = AgentFinish(
                return_values={"output": response}
            )
        state.agent_outcome = finish
        return state
    
    async def summarize_node(self, state: AgentState) -> AgentState:
        """æ€»ç»“èŠ‚ç‚¹ - å‚ç…§MyScaleKB-Agentçš„summarizeæ¨¡å¼"""
        logger.info(f"ğŸ“ æ€»ç»“èŠ‚ç‚¹")
        
        if LANGGRAPH_AVAILABLE and hasattr(state.agent_outcome, 'return_values'):
            # å¦‚æœå·²ç»æ˜¯æœ€ç»ˆç»“æœï¼Œç›´æ¥è¿”å›
            return state
        elif hasattr(state, 'agent_outcome') and isinstance(state.agent_outcome, AgentFinish):
            # å¦‚æœå·²ç»æ˜¯æœ€ç»ˆç»“æœï¼Œç›´æ¥è¿”å›
            return state
        
        # ä½¿ç”¨LLMåŸºäºå·¥å…·æ‰§è¡Œç»“æœè¿›è¡Œæ™ºèƒ½å¯¹è¯
        if state.intermediate_steps:
            final_output = await self._generate_conversation_response(state)
        else:
            # å¦‚æœæ²¡æœ‰å·¥å…·æ‰§è¡Œï¼Œç›´æ¥ä½¿ç”¨é€šç”¨å¯¹è¯å›å¤
            final_output = await self._get_direct_conversation_response(state)
        
        if LANGGRAPH_AVAILABLE:
            finish = LCAgentFinish(
                return_values={"output": final_output},
                log="æ€»ç»“å®Œæˆ"
            )
        else:
            finish = AgentFinish(
                return_values={"output": final_output}
            )
        state.agent_outcome = finish
        
        return state
    
    def action_forward(self, state: AgentState) -> str:
        """å†³å®šä¸‹ä¸€æ­¥åŠ¨ä½œ - å‚ç…§MyScaleKB-Agentçš„action_forward"""
        logger.info(f"ğŸ”€ Action Forwardå¼€å§‹ï¼Œagent_outcomeç±»å‹: {type(state.agent_outcome)}")
        logger.info(f"ğŸ”€ Agent outcomeå†…å®¹: {state.agent_outcome}")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç»“æŸçŠ¶æ€
        if LANGGRAPH_AVAILABLE and hasattr(state.agent_outcome, 'return_values'):
            logger.info("ğŸ“ æ£€æµ‹åˆ°return_valuesï¼Œç»“æŸæµç¨‹")
            return "end"
        elif hasattr(state, 'agent_outcome') and isinstance(state.agent_outcome, AgentFinish):
            logger.info("ğŸ æ£€æµ‹åˆ°AgentFinishï¼Œç»“æŸæµç¨‹")
            return "end"
        
        # è·å–agent_action - ç›´æ¥ä½¿ç”¨agent_outcomeæˆ–ä»åˆ—è¡¨ä¸­å–ç¬¬ä¸€ä¸ª
        if isinstance(state.agent_outcome, list):
            agent_action = state.agent_outcome[0] if state.agent_outcome else None
            logger.info(f"ğŸ¬ ä»åˆ—è¡¨è·å–agent_action: {agent_action}")
        else:
            agent_action = state.agent_outcome
            logger.info(f"ğŸ¬ ç›´æ¥è·å–agent_action: {agent_action}")
        
        if agent_action:
            # æ£€æŸ¥LangGraphæ¨¡å¼ä¸‹çš„å·¥å…·
            if LANGGRAPH_AVAILABLE and hasattr(agent_action, 'tool'):
                tool_name = agent_action.tool
                logger.info(f"ğŸ”§ LangGraphæ¨¡å¼ - å·¥å…·å: {tool_name}")
                # é™¤äº†general_conversationå¤–ï¼Œæ‰€æœ‰å·¥å…·éƒ½è·¯ç”±åˆ°execute_tools
                if tool_name == "general_conversation":
                    logger.info("ğŸ’¬ è·¯ç”±åˆ°: general_conversation")
                    return "general_conversation"
                else:
                    logger.info(f"ğŸ› ï¸ è·¯ç”±åˆ°: execute_tools (å·¥å…·: {tool_name})")
                    return "execute_tools"
            
            # æ£€æŸ¥ä¼ ç»Ÿæ¨¡å¼ä¸‹çš„å·¥å…·
            elif hasattr(agent_action, 'tool'):
                tool_name = agent_action.tool
                logger.info(f"ğŸ”§ ä¼ ç»Ÿæ¨¡å¼ - å·¥å…·å: {tool_name}")
                # æœ‰å·¥å…·å°±æ‰§è¡Œå·¥å…·
                logger.info(f"ğŸ› ï¸ è·¯ç”±åˆ°: execute_tools (å·¥å…·: {tool_name})")
                return "execute_tools"
        
        logger.info("âš ï¸ æ— åŒ¹é…æ¡ä»¶ï¼Œé»˜è®¤è·¯ç”±åˆ°general_conversation")
        return "general_conversation"
    def _simple_keyword_fallback(self, user_input: str) -> Optional[Dict[str, Any]]:
        """å½“LLMä¸å¯ç”¨æ—¶ï¼Œæ‰€æœ‰SubAgentéƒ½æ²¡æœ‰æ„ä¹‰ï¼Œç›´æ¥æŠ›å‡ºé”™è¯¯"""
        raise Exception("LLMæœåŠ¡ä¸å¯ç”¨ï¼Œæ— æ³•æ‰§è¡Œä»»ä½•å·¥å…·æˆ–SubAgent")
    
    def _find_tool(self, tool_name: str) -> Optional[BaseTool]:
        """æŸ¥æ‰¾å·¥å…·"""
        for tool in self.tools:
            if tool.name() == tool_name:
                return tool
        return None
    
    def _build_history_text(self, conversation_history: List[Dict[str, str]], k: int = 8, clip: int = 200) -> str:
        """æŠŠæœ€è¿‘ k æ¡å†å²æ‹¼æˆç»Ÿä¸€æ–‡æœ¬ï¼›é•¿æ¶ˆæ¯è£å‰ªåˆ° clip å­—ç¬¦ã€‚"""
        if not conversation_history:
            return ""

        recent = conversation_history[-k:]
        lines = []
        for msg in recent:
            role = "ç”¨æˆ·" if msg.get("role") == "user" else "åŠ©æ‰‹"
            content = msg.get("content", "")
            if len(content) > clip:
                content = content[:clip] + "..."
            lines.append(f"{role}: {content}")
        return "\n".join(lines)
    
    def _get_fallback_response(self, user_input: str, conversation_history: List[Dict[str, str]]) -> str:
        """Fallbackå“åº” - ä¸åšå‡è£…å›å¤ï¼Œç›´æ¥æŠ›å‡ºå¼‚å¸¸"""
        raise Exception("LLMæœåŠ¡ä¸å¯ç”¨ï¼Œæ— æ³•å¤„ç†å¯¹è¯")
    
    async def _generate_conversation_response(self, state: AgentState) -> str:
        """åŸºäºå·¥å…·æ‰§è¡Œç»“æœå’Œå¯¹è¯å†å²ç”Ÿæˆæ™ºèƒ½å“åº”"""
        user_input = state.input
        conversation_history = state.conversation_history
        
        # æ„å»ºå·¥å…·æ‰§è¡Œæ‘˜è¦
        tool_results_summary = []
        for action, result_str in state.intermediate_steps:
            tool_name = action.tool
            
            # è§£æå·¥å…·ç»“æœ
            import ast
            try:
                if result_str.startswith("{'") or result_str.startswith('{"'):
                    result_dict = ast.literal_eval(result_str)
                    tool_results_summary.append({
                        "tool": tool_name,
                        "result": result_dict
                    })
                else:
                    tool_results_summary.append({
                        "tool": tool_name,
                        "result": {"raw_output": result_str}
                    })
            except (ValueError, SyntaxError):
                tool_results_summary.append({
                    "tool": tool_name,
                    "result": {"raw_output": result_str}
                })
        
        # å¦‚æœLLMä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€å•æ ¼å¼åŒ–
        if not self.llm.api_available:
            return self._simple_format_results(tool_results_summary)
        
        try:
            # æ„å»ºæ™ºèƒ½å¯¹è¯æç¤ºè¯
            system_prompt = """ä½ æ˜¯DataFlowæ™ºèƒ½åŠ©æ‰‹ï¼Œä¸€ä¸ªä¸“ä¸šã€å‹å¥½çš„AIåŠ©æ‰‹ã€‚ä½ å¯ä»¥ï¼š

1. è°ƒç”¨ä¸“ä¸šå·¥å…·å¤„ç†ç‰¹å®šä»»åŠ¡ï¼ˆAPIå¯†é’¥è·å–ã€è¡¨å•ç”Ÿæˆã€æ•°æ®åˆ†æã€ä»£ç ç”Ÿæˆç­‰ï¼‰
2. è¿›è¡Œé€šç”¨æ™ºèƒ½å¯¹è¯ï¼Œå›ç­”å„ç§é—®é¢˜
3. è®°ä½å¯¹è¯å†å²ï¼Œæä¾›è¿è´¯çš„å¯¹è¯ä½“éªŒ

å½“å‰å¯¹è¯æƒ…å†µï¼š
- ç”¨æˆ·åˆšæ‰æå‡ºäº†ä¸€ä¸ªè¯·æ±‚
- æˆ‘å·²ç»è°ƒç”¨äº†ç›¸å…³å·¥å…·å¹¶è·å¾—ç»“æœï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
- ç°åœ¨éœ€è¦åŸºäºå·¥å…·ç»“æœå’Œå¯¹è¯å†å²ï¼Œè‡ªç„¶åœ°å›ç­”ç”¨æˆ·

å›ç­”è¦æ±‚ï¼š
1. ä¼˜å…ˆåŸºäºå·¥å…·ç»“æœæä¾›å‡†ç¡®ä¿¡æ¯
2. å¦‚æœç”¨æˆ·é—®åŠå¯¹è¯å†å²ï¼Œè¦å‡†ç¡®å›å¿†
3. ä¿æŒå¯¹è¯è‡ªç„¶æµç•…ï¼ŒåƒçœŸæ­£çš„åŠ©æ‰‹
4. å¦‚æœæ²¡æœ‰å·¥å…·ç»“æœï¼Œå°±è¿›è¡Œæ­£å¸¸çš„AIå¯¹è¯
5. ç”¨ä¸­æ–‡å›ç­”ï¼Œè¯­æ°”å‹å¥½ä¸“ä¸š

è¯·æ ¹æ®ç”¨æˆ·é—®é¢˜å’Œä¸Šä¸‹æ–‡ï¼Œç»™å‡ºæœ€åˆé€‚çš„å›ç­”ã€‚"""

            # æ„å»ºå¯¹è¯å†å²æ–‡æœ¬
            history_text = self._build_history_text(conversation_history, k=10, clip=300)
            
            # æ„å»ºå·¥å…·ç»“æœæè¿°
            tools_info = ""
            if tool_results_summary:
                tools_info_list = []
                for tool_summary in tool_results_summary:
                    tool_name = tool_summary["tool"]
                    result = tool_summary["result"]
                    
                    if tool_name == "APIKeyè·å–å·¥å…·":
                        if result.get("access_granted"):
                            api_key = result.get("apikey", "")
                            tools_info_list.append(f"æˆåŠŸè·å–APIå¯†é’¥: {api_key}")
                        else:
                            tools_info_list.append("APIå¯†é’¥è·å–å¤±è´¥")
                    elif tool_name == "former_agent":
                        if result.get("success"):
                            response = result.get("agent_response", "")
                            tools_info_list.append(f"è¡¨å•ç”Ÿæˆç»“æœ: {response}")
                        else:
                            tools_info_list.append("è¡¨å•ç”Ÿæˆå¤±è´¥")
                    else:
                        tools_info_list.append(f"å·¥å…· {tool_name} æ‰§è¡Œå®Œæˆ")
                
                tools_info = f"\n\nåˆšåˆšæ‰§è¡Œçš„å·¥å…·ç»“æœ:\n" + "\n".join(tools_info_list)
            
            user_prompt = f"""å½“å‰ç”¨æˆ·é—®é¢˜: {user_input}

å¯¹è¯å†å²:
{history_text}

å·¥å…·æ‰§è¡Œç»“æœ:
{tools_info}

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯è‡ªç„¶åœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœç”¨æˆ·è¯¢é—®å¯¹è¯å†å²ä¸­çš„å†…å®¹ï¼Œè¯·å‡†ç¡®å›å¿†ã€‚"""

            # è°ƒç”¨LLMç”Ÿæˆå›å¤
            llm_service = self.llm._create_llm_service()
            responses = llm_service.generate_from_input(
                user_inputs=[user_prompt],
                system_prompt=system_prompt
            )
            
            if responses and responses[0]:
                return responses[0].strip()
            
        except Exception as e:
            logger.error(f"LLMå¯¹è¯ç”Ÿæˆå¤±è´¥: {e}")
        
        # fallbackåˆ°ç®€å•æ ¼å¼åŒ–
        return self._simple_format_results(tool_results_summary)
    
    def _simple_format_results(self, tool_results_summary: List[Dict[str, Any]]) -> str:
        """ç®€å•æ ¼å¼åŒ–å·¥å…·ç»“æœï¼ˆå½“LLMä¸å¯ç”¨æ—¶ï¼‰"""
        for tool_summary in tool_results_summary:
            tool_name = tool_summary["tool"]
            result = tool_summary["result"]
            
            if tool_name == "APIKeyè·å–å·¥å…·":
                if result.get("access_granted"):
                    api_key = result.get("apikey", "")
                    return f"ğŸ”‘ ä»Šå¤©çš„ç§˜å¯†APIå¯†é’¥æ˜¯: `{api_key}`"
                else:
                    return "âŒ æ— æ³•è·å–APIå¯†é’¥"
        
        return "âœ… æ“ä½œå®Œæˆ"
    
    async def _get_direct_conversation_response(self, state: AgentState) -> str:
        """å½“æ²¡æœ‰å·¥å…·æ‰§è¡Œæ—¶ï¼Œè·å–ç›´æ¥å¯¹è¯å›å¤"""
        user_input = state.input
        conversation_history = state.conversation_history
        
        # ç›´æ¥ä½¿ç”¨LLMï¼Œä¸åšfallback
        if self.llm.api_available:
            try:
                system_prompt = "ä½ æ˜¯DataFlowæ™ºèƒ½åŠ©æ‰‹ï¼Œè¯·ç›´æ¥ã€è‡ªç„¶åœ°å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"
                
                # æ„å»ºå¯¹è¯å†å²
                history_text = self._build_history_text(conversation_history, k=8, clip=200)
                
                user_prompt = f"""ç”¨æˆ·é—®é¢˜: {user_input}

å¯¹è¯å†å²:
{history_text}

è¯·åŸºäºå¯¹è¯å†å²è‡ªç„¶åœ°å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"""
                
                llm_service = self.llm._create_llm_service()
                responses = llm_service.generate_from_input(
                    user_inputs=[user_prompt],
                    system_prompt=system_prompt
                )
                
                if responses and responses[0]:
                    return responses[0].strip()
                    
            except Exception as e:
                logger.error(f"LLMè°ƒç”¨å¤±è´¥: {e}")
                raise e
        
        # LLMä¸å¯ç”¨æ—¶æŠ›å‡ºå¼‚å¸¸
        raise Exception("LLMæœåŠ¡ä¸å¯ç”¨ï¼Œæ— æ³•å¤„ç†å¯¹è¯")


class MasterAgentExecutor:
    """Master Agent æ‰§è¡Œå™¨ - ä½¿ç”¨LangGraphæ‰§è¡Œ"""
    
    def __init__(self, agent: MasterAgent):
        self.agent = agent
    
    async def execute(self, user_input: str, session_id: str = None) -> Dict[str, Any]:
        """æ‰§è¡Œç”¨æˆ·è¯·æ±‚ - ä½¿ç”¨LangGraphæˆ–Fallbackæ‰§è¡Œå™¨"""
        try:
            # è·å–æˆ–åˆ›å»ºä¼šè¯å†å²
            if session_id not in self.agent.conversation_sessions:
                self.agent.conversation_sessions[session_id] = []
            
            conversation_history = self.agent.conversation_sessions[session_id]
            
            # åˆå§‹åŒ–çŠ¶æ€
            initial_state = {
                "input": user_input,
                "session_id": session_id,
                "conversation_history": conversation_history.copy(),
                "agent_outcome": None,
                "intermediate_steps": [],
                "current_step": "bootstrap"
            }
            
            if LANGGRAPH_AVAILABLE and self.agent.compiled_graph:
                # ä½¿ç”¨çœŸæ­£çš„LangGraphæ‰§è¡Œ
                logger.info("ğŸš€ ä½¿ç”¨LangGraphæ‰§è¡Œ")
                
                final_state = await self.agent.compiled_graph.ainvoke(initial_state)
                
                # è·å–æœ€ç»ˆè¾“å‡º
                agent_outcome = final_state.get("agent_outcome")
                if agent_outcome and hasattr(agent_outcome, 'return_values'):
                    output = agent_outcome.return_values.get("output", "æ‰§è¡Œå®Œæˆ")
                else:
                    output = "æ‰§è¡Œå®Œæˆï¼Œä½†æœªè·å–åˆ°è¾“å‡º"
                
                logger.info(f"âœ… LangGraphæ‰§è¡Œå®Œæˆ")
                
            else:
                # ä½¿ç”¨Fallbackæ‰§è¡Œå™¨
                logger.info("ğŸ”„ ä½¿ç”¨Fallbackæ‰§è¡Œå™¨")
                output = await self._fallback_execute(initial_state)
            
            # ä¿å­˜å¯¹è¯å†å²
            conversation_history.append({"role": "user", "content": user_input})
            conversation_history.append({"role": "assistant", "content": output})
            
            # ä¿æŒå†å²é•¿åº¦åœ¨åˆç†èŒƒå›´å†…ï¼ˆæœ€è¿‘20è½®å¯¹è¯ï¼‰
            if len(conversation_history) > 40:
                conversation_history = conversation_history[-40:]
            
            self.agent.conversation_sessions[session_id] = conversation_history
            
            return {
                "success": True,
                "output": output,
                "session_id": session_id
            }
                
        except Exception as e:
            logger.error(f"Master Agent æ‰§è¡Œå¤±è´¥: {e}")
            return {
                "success": False,
                "output": f"æ‰§è¡Œå¤±è´¥: {str(e)}",
                "session_id": session_id
            }
    
    async def _fallback_execute(self, initial_state: Dict[str, Any]) -> str:
        """Fallbackæ‰§è¡Œå™¨ï¼ˆå½“LangGraphä¸å¯ç”¨æ—¶ï¼‰"""
        logger.info("æ‰§è¡ŒFallbacké€»è¾‘")
        
        # è½¬æ¢ä¸ºAgentStateå¯¹è±¡
        state = AgentState.from_dict(initial_state)
        
        # æ‰§è¡Œå¼•å¯¼é˜¶æ®µ
        state = await self.agent.bootstrap_node(state)
        
        # å†³å®šä¸‹ä¸€æ­¥
        next_action = self.agent.action_forward(state)
        
        if next_action == "execute_tools":
            # æ‰§è¡Œå·¥å…·
            state = await self.agent.execute_tools_node(state)
            # æ€»ç»“
            state = await self.agent.summarize_node(state)
        elif next_action == "general_conversation":
            # é€šç”¨å¯¹è¯
            state = await self.agent.general_conversation_node(state)
        else:
            # ç›´æ¥æ€»ç»“
            state = await self.agent.summarize_node(state)
        
        # è·å–æœ€ç»ˆè¾“å‡º
        if isinstance(state.agent_outcome, AgentFinish):
            return state.agent_outcome.return_values.get("output", "æ‰§è¡Œå®Œæˆ")
        else:
            return "æ‰§è¡Œå®Œæˆï¼Œä½†æœªè·å–åˆ°è¾“å‡º"


# å·¥å‚å‡½æ•°
def create_master_agent() -> Tuple[MasterAgent, MasterAgentExecutor]:
    """åˆ›å»º Master Agent å’Œæ‰§è¡Œå™¨"""
    agent = MasterAgent()
    executor = MasterAgentExecutor(agent)
    return agent, executor
