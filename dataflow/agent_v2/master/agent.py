"""
DataFlow Master Agent
åŸºäº MyScaleKB-Agent æ¶æ„çš„ä¸»æ§æ™ºèƒ½ä½“ - ä½¿ç”¨çœŸæ­£çš„LangGraphå·¥ä½œæµ
"""
import logging
import asyncio
import time
import uuid
from typing import Dict, List, Any, Union, Optional, Tuple, TypedDict, Annotated
from pydantic import BaseModel
from enum import Enum
import operator

# LangGraphæ ¸å¿ƒç»„ä»¶
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolExecutor
from langchain_core.tools import StructuredTool
from langchain_core.agents import AgentFinish as LCAgentFinish, AgentAction as LCAgentAction

# from dataflow.agent_v2.base.core import SubAgent, GraphBuilder, BaseTool, node, edge, conditional_entry

# ä½¿ç”¨ myscalekb_agent_base åº“çš„ç»„ä»¶
from myscalekb_agent_base.sub_agent import SubAgent
from myscalekb_agent_base.graph_builder import GraphBuilder, node, edge, conditional_entry
from myscalekb_agent_base.schemas.agent_metadata import AgentMetadata

# ä¿ç•™è‡ªå·±çš„ç»„ä»¶
from dataflow.agent_v2.base.core import BaseTool

# å¯¼å…¥äº‹ä»¶ç³»ç»Ÿ
from ..events.core import EventSink, Event, EventType

from dataflow.agent_v2.llm_client import get_llm_client
from dataflow.agent_v2.subagents.apikey_agent import APIKeyTool
from dataflow.agent_v2.subagents.mock_tools import SleepTool, MockSearchTool, MockFormerTool, MockCodeGenTool
from dataflow.agent_v2.subagents.csvtools import CSVProfileTool, CSVDetectTimeColumnsTool, CSVVegaSpecTool, ASTStaticCheckTool, UnitTestStubTool, LocalIndexBuildTool, LocalIndexQueryTool

from concurrent.futures import ThreadPoolExecutor

def to_langchain_tool(tool: BaseTool) -> StructuredTool:
    ArgsSchema = tool.params()  # ä½ çš„å·¥å…·å·²ç»æä¾›äº† Pydantic å‚æ•°ç±»

    async def _arun(**kwargs):
        # äº¤ç»™åŸå·¥å…·æ‰§è¡Œï¼ˆç¡®ä¿æ˜¯ asyncï¼‰
        return await tool.execute(**kwargs)

    return StructuredTool.from_function(
        coroutine=_arun,                      # å¼‚æ­¥å‡½æ•°
        name=tool.name(),
        description=tool.description(),
        args_schema=ArgsSchema,               # å‚æ•°æ ¡éªŒ
        return_direct=False,                  # å¸¸è§„æƒ…å†µ Falseï¼›éœ€è¦æ—¶å¯ True
    )


def new_step_id() -> str:
    return f"step-{uuid.uuid4().hex[:8]}"


class PlannerOutput(BaseModel):
    decision: str              # "continue" | "finish"
    next_actions: list = []    # [ {"tool": "...", "tool_input": {...}} ... ]
    user_message: Optional[str] = None
    reasons: Optional[str] = None

logger = logging.getLogger(__name__)


# ä½¿ç”¨ myscalekb_agent_base å…¼å®¹çš„ AgentState ç»“æ„
class AgentState(TypedDict, total=False):
    """Master Agent çŠ¶æ€å®šä¹‰ - å…¼å®¹ myscalekb_agent_base ç»“æ„"""
    # myscalekb_agent_base æ ‡å‡†å­—æ®µ
    input: Any  # è¾“å…¥æ¶ˆæ¯ (å¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ– UserMessage)
    query: str  # è½¬æ¢åçš„æŸ¥è¯¢å­—ç¬¦ä¸²
    chat_history: List[Any]  # èŠå¤©å†å²
    agent_metadata: AgentMetadata  # ä»£ç†å…ƒæ•°æ®
    agent_outcome: Union[Any, None]  # ä»£ç†è¾“å‡º
    intermediate_steps: Annotated[List[Tuple[Any, Any]], operator.add]  # ä¸­é—´æ­¥éª¤
    trace_id: Union[str, None]  # è¿½è¸ªID
    
    # DataFlow æ‰©å±•å­—æ®µ 
    session_id: Optional[str]
    current_step: str
    form_data: Optional[Dict[str, Any]]
    xml_content: Optional[str]
    execution_result: Optional[str]
    conversation_history: List[Dict[str, str]]  # å¯¹è¯å†å²
    last_tool_results: Optional[Dict[str, Any]]  # æœ€è¿‘çš„å·¥å…·ç»“æœ
    
    # å¤šè½®ç¼–æ’æ”¯æŒ
    pending_actions: List[Any]  # å¾…æ‰§è¡Œçš„åŠ¨ä½œ
    tool_results: List[Dict[str, Any]]  # ç»“æ„åŒ–å·¥å…·ç»“æœ
    loop_guard: int  # å¾ªç¯è®¡æ•°å™¨
    max_steps: int  # æœ€å¤§æ­¥æ•°
    context_vars: Dict[str, Any]  # è·¨æ­¥å…±äº«æ•°æ®
    next_action: Optional[str]  # ä¸‹ä¸€ä¸ªåŠ¨ä½œå†³ç­–


class ActionType(Enum):
    """åŠ¨ä½œç±»å‹"""
    TOOL_EXECUTION = "tool_execution"
    SUB_AGENT_FORWARD = "sub_agent_forward"
    GENERAL_CONVERSATION = "general_conversation"
    END = "end"


class MasterAgent(SubAgent):
    """DataFlow Master Agent - åŸºäº MyScaleKB-Agent é£æ ¼çš„ LangGraph æ¶æ„"""
    
    def __init__(self, ctx=None, llm=None, memory=None, *args, **kwargs):
        # å¦‚æœæ²¡æœ‰ä¼ å…¥ llmï¼Œåˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„ llm å¯¹è±¡
        if llm is None:
            class MockLLM:
                def __init__(self):
                    self.model = get_llm_client()
            llm = MockLLM()
        
        # å¦‚æœæ²¡æœ‰ä¼ å…¥ ctxï¼Œåˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„ ctx å¯¹è±¡
        if ctx is None:
            class MockContext:
                def __init__(self):
                    self.embedding_model = None
                    self.myscale_client = None
                    self.variables = {"knowledge_scopes": []}
            ctx = MockContext()
        
        # å¦‚æœæ²¡æœ‰ä¼ å…¥ memoryï¼Œåˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„ memory å¯¹è±¡
        if memory is None:
            class MockMemory:
                pass
            memory = MockMemory()
        
        super().__init__(ctx, llm, memory, *args, **kwargs)
        
        self.forward_paths = {}
        self.sub_agents = {}
        self.conversation_sessions = {}  # ä¼šè¯ç®¡ç†
        self.tools = []
        
        # æ³¨å†Œå·¥å…·
        self._register_tools()
    
    @classmethod
    def name(cls) -> str:
        return "master_agent"
    
    @classmethod
    def description(cls) -> str:
        return "DataFlowä¸»æ§æ™ºèƒ½ä½“ï¼Œæ”¯æŒå¤šè½®ç¼–æ’å’Œå·¥å…·è°ƒç”¨ï¼Œå¯ä»¥å¤„ç†å¤æ‚çš„ç”¨æˆ·è¯·æ±‚"
    
    def _register_tools(self):
        """æ³¨å†Œå·¥å…·"""
        try:
            self.tools = [
                APIKeyTool(),
                # æ·»åŠ Mockå·¥å…·ç”¨äºæµ‹è¯•å¤šè½®ç¼–æ’
                SleepTool(),
                MockSearchTool(),
                MockFormerTool(),
                MockCodeGenTool(),
                CSVProfileTool(), 
                CSVDetectTimeColumnsTool(), 
                CSVVegaSpecTool(), 
                ASTStaticCheckTool(), 
                UnitTestStubTool(), 
                LocalIndexBuildTool(), 
                LocalIndexQueryTool()

            ]
            logger.info(f"å·²æ³¨å†Œ {len(self.tools)} ä¸ªå·¥å…·")
        except Exception as e:
            logger.error(f"å·¥å…·æ³¨å†Œå¤±è´¥: {e}")
            self.tools = []
        
        self.lc_tools = [to_langchain_tool(t) for t in self.tools]
        self.tool_executor = ToolExecutor(self.lc_tools)
    
    def build_app(self):
        """æ„å»ºä»£ç†å·¥ä½œæµ - ç±»ä¼¼ MyScaleKB-Agent çš„å®ç°"""
        workflow = self._build_graph(AgentState, compiled=False)
        
        # è®¾ç½®æ¡ä»¶å…¥å£ç‚¹
        workflow.set_conditional_entry_point(
            self.entry,
            {
                "bootstrap": "bootstrap",
            }
        )
        
        # æ·»åŠ æ¡ä»¶è¾¹
        workflow.add_conditional_edges(
            "bootstrap",
            self.action_forward,
            {
                "execute_tools": "execute_tools",
                "general_conversation": "general_conversation", 
                "end": "summarize"
            }
        )
        
        # æ‰§è¡Œå·¥å…·åè¿›å…¥è§„åˆ’å™¨è¿›è¡Œä¸‹ä¸€è½®å†³ç­–
        workflow.add_edge("execute_tools", "planner")
        workflow.add_edge("general_conversation", "planner")
        
        # è§„åˆ’å™¨å†³å®šç»§ç»­æ‰§è¡Œè¿˜æ˜¯ç»“æŸ
        workflow.add_conditional_edges(
            "planner",
            self.planner_router,
            {
                "continue": "execute_tools",  # ç»§ç»­æ‰§è¡Œæ›´å¤šå·¥å…·
                "finish": "summarize"        # å®Œæˆä»»åŠ¡
            }
        )
        
        workflow.add_edge("summarize", GraphBuilder.END)
        
        return workflow.compile()
    
    @staticmethod
    async def entry(data):
        """å…¥å£ç‚¹ - å†³å®šè·¯ç”±åˆ°å“ªä¸ªèŠ‚ç‚¹"""
        logger.info("ğŸšª è¿›å…¥Master Agentå…¥å£ç‚¹")
        # é»˜è®¤è¿›å…¥bootstrapèŠ‚ç‚¹è¿›è¡Œå¼•å¯¼
        return "bootstrap"
    
    @node
    async def bootstrap(self, data):
        """å¼•å¯¼èŠ‚ç‚¹ - æ¯æ¬¡åªè§„åˆ’ç¬¬ä¸€ä¸ªåŠ¨ä½œï¼Œåç»­é€šè¿‡plannerèŠ‚ç‚¹é€æ­¥è§„åˆ’"""
        user_input = data.get("input", "")
        logger.info(f"ğŸ”„ BootstrapèŠ‚ç‚¹: {user_input}")
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šBootstrapåªè´Ÿè´£å¯åŠ¨ç¬¬ä¸€ä¸ªåŠ¨ä½œï¼Œä¸è¿›è¡Œå¤æ‚çš„å¤šæ­¥éª¤è§„åˆ’
        # è·å–å¯ç”¨å·¥å…·åˆ—è¡¨
        available_tools = [tool.name() for tool in self.tools]
        logger.info(f"ğŸ”§ å¯ç”¨å·¥å…·åˆ—è¡¨: {available_tools}")
        
        # ğŸ”§ é‡è¦ï¼šBootstrapé˜¶æ®µåªé€‰æ‹©å¯åŠ¨åŠ¨ä½œï¼Œä¸è€ƒè™‘æ¬¡æ•°å’Œé—´éš”
        # ç®€åŒ–ç”¨æˆ·è¾“å…¥ï¼Œåªæå–ä¸»è¦ä»»åŠ¡ç±»å‹
        simplified_input = self._extract_main_task(user_input)
        logger.info(f"ğŸ“ ç®€åŒ–ä»»åŠ¡: {simplified_input}")
        
        # ä½¿ç”¨LLMåˆ†æç”¨æˆ·æ„å›¾ï¼Œä½†åªå…³æ³¨ç¬¬ä¸€ä¸ªåŠ¨ä½œ
        try:
            intent_analysis = self.llm.analyze_user_intent(simplified_input, available_tools)
            
            selected_tool = intent_analysis.get("selected_tool")
            confidence = intent_analysis.get("confidence", 0.0)
            parameters = intent_analysis.get("parameters", {})
            
            logger.info(f"ğŸ¯ æ„å›¾åˆ†æç»“æœ: å·¥å…·={selected_tool}, ç½®ä¿¡åº¦={confidence}")
            
            if selected_tool and confidence > 0.3:
                # ğŸ”§ é‡è¦ï¼šåªåˆ›å»ºä¸€ä¸ªåŠ¨ä½œï¼Œè®©plannerè´Ÿè´£åç»­è§„åˆ’
                logger.info(f"âœ… Bootstrapé€‰æ‹©å·¥å…·: {selected_tool} (å•æ¬¡æ‰§è¡Œ)")
                
                action = LCAgentAction(
                    tool=selected_tool,
                    tool_input=parameters,
                    log=f"Bootstrapå¯åŠ¨: {selected_tool}"
                )
                data["agent_outcome"] = [action]  # æ³¨æ„ï¼šåªæœ‰ä¸€ä¸ªåŠ¨ä½œ
                logger.info(f"ğŸš€ Bootstrapåˆ›å»ºå•ä¸ªAction: {action.tool}")
            else:
                # æ²¡æœ‰åˆé€‚çš„å·¥å…·ï¼Œæ ‡è®°ä¸ºéœ€è¦é€šç”¨å¯¹è¯
                logger.info(f"âŒ æ²¡æœ‰åˆé€‚å·¥å…·ï¼Œä½¿ç”¨é€šç”¨å¯¹è¯ (ç½®ä¿¡åº¦: {confidence})")

                action = LCAgentAction(
                    tool="general_conversation",
                    tool_input={"user_input": user_input},
                    log="é€šç”¨å¯¹è¯"
                )

                data["agent_outcome"] = [action]
                logger.info(f"ğŸ’¬ Bootstrapåˆ›å»ºå¯¹è¯Action")
                
        except Exception as e:
            logger.error(f"LLMæ„å›¾åˆ†æå¤±è´¥: {e}")
            
            raise Exception("LLMæœåŠ¡ä¸å¯ç”¨ï¼Œæ— æ³•æ‰§è¡Œä»»ä½•å·¥å…·æˆ–SubAgent")
        return data
    
    def _extract_main_task(self, user_input: str) -> str:
        """ç›´æ¥è¿”å›ç”¨æˆ·è¾“å…¥ï¼Œä¸åšä»»ä½•å…³é”®è¯åŒ¹é…å¤„ç†"""
        return user_input.strip()
    
    @node
    @edge(target_node="planner")
    async def execute_tools(self, data):
        """æ‰§è¡Œå·¥å…·èŠ‚ç‚¹ - ç¡®ä¿æ¯æ¬¡åªæ‰§è¡Œä¸€ä¸ªåŠ¨ä½œ"""
        agent_outcome = data.get("agent_outcome")
        
        logger.info(f"ğŸ› ï¸ è¿›å…¥execute_tools_nodeï¼Œagent_outcomeç±»å‹: {type(agent_outcome)}")
        logger.info(f"ğŸ› ï¸ agent_outcomeå†…å®¹: {agent_outcome}")

        # ğŸ”§ å…³é”®ä¿®å¤ï¼šç¡®ä¿æ¯æ¬¡åªå¤„ç†ä¸€ä¸ªåŠ¨ä½œ
        actions: List[LCAgentAction] = []
        if isinstance(agent_outcome, list):
            # ğŸ”§ é‡è¦ï¼šå³ä½¿agent_outcomeæ˜¯åˆ—è¡¨ï¼Œä¹Ÿåªå–ç¬¬ä¸€ä¸ªåŠ¨ä½œ
            if agent_outcome:
                actions = [agent_outcome[0]]  # åªå–ç¬¬ä¸€ä¸ª
                logger.info(f"ğŸ“‹ ä»åˆ—è¡¨ä¸­å–ç¬¬ä¸€ä¸ªåŠ¨ä½œ: {actions[0].tool}")
            else:
                logger.warning("âš ï¸ agent_outcomeæ˜¯ç©ºåˆ—è¡¨")
                return data
        elif hasattr(agent_outcome, "tool"):
            actions = [agent_outcome]
            logger.info(f"ğŸ“‹ å•ä¸ªåŠ¨ä½œ: {agent_outcome.tool}")
        else:
            logger.warning(f"âš ï¸ æ²¡æœ‰å¯æ‰§è¡Œçš„åŠ¨ä½œï¼Œagent_outcomeç±»å‹: {type(agent_outcome)}")
            return data

        if not data.get("tool_results"):
            data["tool_results"] = []

        # ğŸ”§ å…³é”®ï¼šåªæ‰§è¡Œè¿™ä¸€ä¸ªåŠ¨ä½œ
        if actions:
            action = actions[0]
            logger.info(f"ğŸ› ï¸ å¼€å§‹æ‰§è¡Œå•ä¸ªå·¥å…·: {action.tool}")
            
            try:
                result = await self.tool_executor.ainvoke(action)
                
                # è®°å½•åˆ°intermediate_steps
                if not data.get("intermediate_steps"):
                    data["intermediate_steps"] = []
                data["intermediate_steps"].append((action, result))
                
                # è®°å½•åˆ°tool_results
                data["tool_results"].append({
                    "tool": action.tool,
                    "ok": bool(result.get("success", True)) if isinstance(result, dict) else True,
                    "payload": result
                })
                
                logger.info(f"âœ… å·¥å…·æ‰§è¡Œå®Œæˆ: {action.tool}")
                
            except Exception as e:
                logger.error(f"âŒ å·¥å…·æ‰§è¡Œå¤±è´¥: {action.tool}, é”™è¯¯: {e}")
                err = {"success": False, "error": str(e)}
                if not data.get("intermediate_steps"):
                    data["intermediate_steps"] = []
                data["intermediate_steps"].append((action, err))
                data["tool_results"].append({"tool": action.tool, "ok": False, "payload": err})

        # ğŸ”§ å…³é”®ï¼šæ¸…ç©ºagent_outcomeï¼Œé¿å…é‡å¤æ‰§è¡Œ
        data["agent_outcome"] = []
        logger.info(f"ğŸ”„ å·¥å…·æ‰§è¡Œå®Œæˆï¼Œæ¸…ç©ºagent_outcomeï¼Œè¿›å…¥plannerèŠ‚ç‚¹")
        
        return data
    
    @node
    @edge(target_node="planner")
    async def general_conversation(self, data: AgentState) -> AgentState:
        """é€šç”¨å¯¹è¯èŠ‚ç‚¹ - å¤„ç†ä¸éœ€è¦å·¥å…·çš„å¯¹è¯"""
        user_input = data.get("input", "")
        conversation_history = data.get("conversation_history", [])
        
        logger.info(f"ğŸ’¬ é€šç”¨å¯¹è¯èŠ‚ç‚¹: {user_input}")
        
        # å¦‚æœLLMå¯ç”¨ï¼Œä½¿ç”¨æ™ºèƒ½å¯¹è¯
        if self.llm.api_available:
            try:
                # æ„å»ºé€šç”¨å¯¹è¯æç¤ºè¯
                system_prompt = """ä½ æ˜¯DataFlowæ™ºèƒ½åŠ©æ‰‹ï¼Œä¸€ä¸ªä¸“ä¸šã€å‹å¥½ã€æ™ºèƒ½çš„AIåŠ©æ‰‹ã€‚ä½ å¯ä»¥ï¼š

1. å›ç­”å„ç§é€šç”¨é—®é¢˜ï¼ˆç§‘å­¦ã€æŠ€æœ¯ã€ç”Ÿæ´»ã€å­¦ä¹ ç­‰ï¼‰
2. æä¾›ä¸“ä¸šçš„ç¼–ç¨‹ã€æ•°æ®åˆ†æå»ºè®®
3. ååŠ©è§£å†³é—®é¢˜å’Œæä¾›åˆ›æ„æƒ³æ³•
4. è®°ä½å¯¹è¯å†å²ï¼Œä¿æŒè¿è´¯å¯¹è¯
5. å½“ç”¨æˆ·éœ€è¦æ—¶ï¼Œå»ºè®®ä½¿ç”¨ä¸“ä¸šå·¥å…·ï¼ˆAPIå¯†é’¥ã€è¡¨å•ç”Ÿæˆã€æ•°æ®å¤„ç†ç­‰ï¼‰

ä½ çš„ç‰¹ç‚¹ï¼š
- çŸ¥è¯†ä¸°å¯Œï¼Œå–„äºåˆ†æå’Œè§£é‡Š
- å›ç­”å‡†ç¡®ã€æœ‰æ¡ç†
- è¯­è¨€è‡ªç„¶ã€å‹å¥½
- èƒ½å¤Ÿæ ¹æ®ä¸Šä¸‹æ–‡ç†è§£ç”¨æˆ·çœŸæ­£çš„éœ€æ±‚
- å¦‚æœç”¨æˆ·é—®é¢˜å¯èƒ½éœ€è¦ä¸“ä¸šå·¥å…·ï¼Œä¼šä¸»åŠ¨å»ºè®®

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œä¿æŒä¸“ä¸šä½†å‹å¥½çš„è¯­æ°”ã€‚"""

                # æ„å»ºå¯¹è¯å†å²
                history_text = self._build_history_text(conversation_history, k=8, clip=200)
                
                user_prompt = f"""ç”¨æˆ·é—®é¢˜: {user_input}

å¯¹è¯å†å²:
{history_text}

è¯·åŸºäºå¯¹è¯å†å²å’Œå½“å‰é—®é¢˜ï¼Œç»™å‡ºæœ€åˆé€‚çš„å›ç­”ã€‚å¦‚æœç”¨æˆ·è¯¢é—®ä¹‹å‰çš„å¯¹è¯å†…å®¹ï¼Œè¯·å‡†ç¡®å›å¿†ã€‚å¦‚æœé—®é¢˜å¯èƒ½éœ€è¦ä¸“ä¸šå·¥å…·ååŠ©ï¼ˆå¦‚APIå¯†é’¥è·å–ã€è¡¨å•ç”Ÿæˆã€æ•°æ®åˆ†æã€ä»£ç ç”Ÿæˆç­‰ï¼‰ï¼Œè¯·ä¸»åŠ¨å»ºè®®ã€‚"""

                # è°ƒç”¨LLM
                llm_service = self.llm._create_llm_service()
                responses = llm_service.generate_from_input(
                    user_inputs=[user_prompt],
                    system_prompt=system_prompt
                )
                
                if responses and responses[0]:
                    response = responses[0].strip()
                    
                    # å¦‚æœå›å¤å¤ªçŸ­ï¼Œå°è¯•æ‰©å±•
                    if len(response) < 30:
                        followup_prompt = f"""ç”¨æˆ·é—®é¢˜: {user_input}

è¯·æä¾›ä¸€ä¸ªæ›´è¯¦ç»†ã€æ›´æœ‰å¸®åŠ©çš„å›ç­”ã€‚å³ä½¿é—®é¢˜ç®€å•ï¼Œä¹Ÿè¦ç»™å‡ºå‹å¥½çš„å›å¤å’Œå¯èƒ½çš„æ‰©å±•å»ºè®®ã€‚å¦‚æœé—®é¢˜æ¶‰åŠæŠ€æœ¯ï¼Œå¯ä»¥æä¾›ä¸€äº›ç›¸å…³èƒŒæ™¯çŸ¥è¯†ã€‚"""
                        
                        followup_responses = llm_service.generate_from_input(
                            user_inputs=[followup_prompt],
                            system_prompt=system_prompt
                        )
                        
                        if followup_responses and followup_responses[0]:
                            response = followup_responses[0].strip()
                    

                    finish = LCAgentFinish(
                        return_values={"output": response},
                        log="é€šç”¨å¯¹è¯å®Œæˆ"
                    )

                    data["agent_outcome"] = finish
                    return data
                    
            except Exception as e:
                logger.error(f"é€šç”¨å¯¹è¯LLMè°ƒç”¨å¤±è´¥: {e}")
        
        # LLMä¸å¯ç”¨æ—¶çš„fallbacké€»è¾‘
        raise Exception("LLMæœåŠ¡ä¸å¯ç”¨ï¼Œæ— æ³•å¤„ç†å¯¹è¯")
    
    @node
    async def planner(self, data: AgentState) -> AgentState:
        """è§„åˆ’å™¨èŠ‚ç‚¹ - æ¯æ¬¡åªè§„åˆ’ä¸‹ä¸€ä¸ªå•ç‹¬åŠ¨ä½œ"""
        # âœ… æ­¥æ•°æŠ¤æ 
        if data.get('loop_guard') is None:
            data["loop_guard"] = 0
        data["loop_guard"] += 1
        
        max_steps = data.get('max_steps', 8)
        if data["loop_guard"] >= max_steps:
            # è§¦å‘æŠ¤æ ç›´æ¥ç»“æŸ
            logger.info(f"ğŸ›‘ è¾¾åˆ°æœ€å¤§æ­¥éª¤æ•° {max_steps}ï¼Œè‡ªåŠ¨ç»“æŸ")
            data["agent_outcome"] = []  # æ¸…ç©ºï¼Œè®©summarizeå¤„ç†
            data["next_action"] = "finish"
            return data
            
        logger.info(f"ğŸ¯ è¿›å…¥è§„åˆ’å™¨èŠ‚ç‚¹ - æ­¥éª¤ {data['loop_guard']}/{max_steps}")
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ¯æ¬¡åªè§„åˆ’ä¸‹ä¸€ä¸ªå•ç‹¬åŠ¨ä½œ
        try:
            analysis = self._analyze_user_needs(data.get("input", ""), data.get("tool_results", []))
            logger.info(f"ğŸ“‹ éœ€æ±‚åˆ†æ: {analysis}")
            
            if analysis["should_continue"] and analysis["next_action"]:
                # ğŸ”§ é‡è¦ï¼šåªåˆ›å»ºä¸€ä¸ªåŠ¨ä½œ
                next_action = analysis["next_action"]
                
                single_action = LCAgentAction(
                    tool=next_action.get("tool", ""),
                    tool_input=next_action.get("tool_input", {}),
                    log=f"Plannerè§„åˆ’: {next_action.get('tool','')}"
                )
                
                data["agent_outcome"] = [single_action]  # æ³¨æ„ï¼šåªæœ‰ä¸€ä¸ªåŠ¨ä½œ
                data["next_action"] = "continue"
                
                logger.info(f"ğŸ“‹ Plannerè§„åˆ’ä¸‹ä¸€ä¸ªåŠ¨ä½œ: {next_action.get('tool', '')} (å•æ¬¡)")
                logger.info(f"ğŸ“‹ åŸå› : {'; '.join(analysis['reasons']) if isinstance(analysis['reasons'], list) else analysis['reasons']}")
            else:
                # ğŸ”§ ä¿®å¤ï¼šæµè½¬åˆ°summarizeèŠ‚ç‚¹è¿›è¡Œæ™ºèƒ½æ€»ç»“
                data["agent_outcome"] = []  # æ¸…ç©ºï¼Œè®©summarize_nodeå¤„ç†
                data["next_action"] = "finish"
                logger.info(f"ğŸ Plannerå†³å®šç»“æŸï¼Œæµè½¬åˆ°summarizeèŠ‚ç‚¹")
                logger.info(f"ğŸ“‹ ç»“æŸåŸå› : {'; '.join(analysis['reasons']) if isinstance(analysis['reasons'], list) else analysis['reasons']}")
                
            return data
            
        except Exception as e:
            logger.error(f"è§„åˆ’å™¨é”™è¯¯: {e}")
            # ğŸ”§ ä¿®å¤ï¼šå¼‚å¸¸æ—¶ä¹Ÿæµè½¬åˆ°summarizeèŠ‚ç‚¹
            data["agent_outcome"] = []
            data["next_action"] = "finish"
            data["error_message"] = f"è§„åˆ’é”™è¯¯: {str(e)}"
            return data

    def planner_router(self, data: AgentState) -> str:
        """è§„åˆ’å™¨è·¯ç”±å™¨ - ä¿®å¤ç‰ˆæœ¬ï¼Œå…œåº•è¿”å›finish"""
        next_action = data.get("next_action")
        result = "continue" if next_action == "continue" else "finish"
        logger.info(f"ï¿½ è·¯ç”±å†³ç­–: {next_action} -> {result}")
        return result
    
    @node
    @edge(target_node=GraphBuilder.END)
    async def summarize(self, data: AgentState) -> AgentState:
        """æ€»ç»“èŠ‚ç‚¹ - æ™ºèƒ½æ€»ç»“æ‰€æœ‰å·¥å…·æ‰§è¡Œç»“æœï¼Œè€Œä¸æ˜¯ç®€å•çš„æ­¥éª¤è®¡æ•°"""
        logger.info(f"ğŸ“ æ€»ç»“èŠ‚ç‚¹å¼€å§‹ï¼Œintermediate_stepsæ•°é‡: {len(data.get('intermediate_steps', []))}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯æ¶ˆæ¯
        if data.get('error_message'):

            finish = LCAgentFinish(
                return_values={"output": data.get('error_message')},
                log="é”™è¯¯æ€»ç»“"
            )

            data["agent_outcome"] = finish
            return data
        
        # å¦‚æœå·²ç»æ˜¯æœ€ç»ˆç»“æœï¼Œç›´æ¥è¿”å›
        agent_outcome = data.get('agent_outcome')
        if hasattr(agent_outcome, 'return_values'):
            # å·²ç»æ˜¯æœ€ç»ˆç»“æœï¼Œç›´æ¥è¿”å›
            logger.info("ğŸ“ æ£€æµ‹åˆ°å·²æœ‰return_valuesï¼Œç›´æ¥è¿”å›")
            return data
        
        # ğŸ”§ æ ¸å¿ƒä¿®å¤ï¼šå¯¹æ‰€æœ‰å·¥å…·æ‰§è¡Œç»“æœè¿›è¡ŒLLMæ™ºèƒ½æ€»ç»“
        intermediate_steps = data.get('intermediate_steps', [])
        if intermediate_steps:
            logger.info(f"ğŸ¤– å¼€å§‹LLMæ™ºèƒ½æ€»ç»“ï¼Œå…±{len(intermediate_steps)}ä¸ªæ‰§è¡Œæ­¥éª¤")
            final_output = await self._generate_conversation_response(data)
            logger.info(f"âœ… LLMæ™ºèƒ½æ€»ç»“å®Œæˆ: {final_output[:100]}...")
        else:
            # å¦‚æœæ²¡æœ‰å·¥å…·æ‰§è¡Œï¼Œç›´æ¥ä½¿ç”¨é€šç”¨å¯¹è¯å›å¤
            logger.info("ğŸ’¬ æ²¡æœ‰å·¥å…·æ‰§è¡Œï¼Œä½¿ç”¨é€šç”¨å¯¹è¯å›å¤")
            final_output = await self._get_direct_conversation_response(data)
        

        finish = LCAgentFinish(
            return_values={"output": final_output},
            log="æ™ºèƒ½æ€»ç»“å®Œæˆ"
        )

        data["agent_outcome"] = finish
        
        return data
    
    async def action_forward(self, data: AgentState) -> str:
        """å†³å®šä¸‹ä¸€æ­¥åŠ¨ä½œ - ä¿®å¤ç‰ˆæœ¬ï¼Œæ­£ç¡®å¤„ç†å·¥å…·æ‰§è¡Œåçš„è·¯ç”±"""
        agent_outcome = data.get('agent_outcome')
        logger.info(f"ğŸ”€ Action Forwardå¼€å§‹ï¼Œagent_outcomeç±»å‹: {type(agent_outcome)}")
        logger.info(f"ğŸ”€ Agent outcomeå†…å®¹: {agent_outcome}")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç»“æŸçŠ¶æ€
        if hasattr(agent_outcome, 'return_values'):
            logger.info("ğŸ“ æ£€æµ‹åˆ°return_valuesï¼Œç»“æŸæµç¨‹")
            return "end"

        # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§æ­¥æ•°æˆ–æœ‰next_actionæ ‡å¿—
        if data.get('next_action') == "finish":
            logger.info("ğŸ æ£€æµ‹åˆ°finishæ ‡å¿—ï¼Œè¿›å…¥æ€»ç»“é˜¶æ®µ")
            return "summarize"
        
        # ğŸ”§ å¦‚æœagent_outcomeä¸ºç©ºåˆ—è¡¨ï¼Œæ ¹æ®ä¸Šä¸‹æ–‡åˆ¤æ–­
        if isinstance(agent_outcome, list) and len(agent_outcome) == 0:
            # æ£€æŸ¥æ˜¯å¦æœ‰loop_guardï¼ˆè¡¨ç¤ºåœ¨plannerä¸­è¾¾åˆ°æœ€å¤§æ­¥æ•°ï¼‰
            if data.get('loop_guard', 0) >= data.get('max_steps', 8):
                logger.info("ğŸ›‘ è¾¾åˆ°æœ€å¤§æ­¥æ•°ï¼Œè¿›å…¥æ€»ç»“é˜¶æ®µ")
                return "summarize"
            else:
                logger.info("ğŸ”„ å·¥å…·æ‰§è¡Œå®Œæˆï¼Œå›åˆ°plannerç»§ç»­å†³ç­–")
                return "planner"
        
        # è·å–agent_action - ç›´æ¥ä½¿ç”¨agent_outcomeæˆ–ä»åˆ—è¡¨ä¸­å–ç¬¬ä¸€ä¸ª
        if isinstance(agent_outcome, list):
            agent_action = agent_outcome[0] if agent_outcome else None
            logger.info(f"ğŸ¬ ä»åˆ—è¡¨è·å–agent_action: {agent_action}")
        else:
            agent_action = agent_outcome
            logger.info(f"ğŸ¬ ç›´æ¥è·å–agent_action: {agent_action}")
        
        if agent_action:
            # æ£€æŸ¥LangGraphæ¨¡å¼ä¸‹çš„å·¥å…·
            if hasattr(agent_action, 'tool'):
                tool_name = agent_action.tool
                logger.info(f"ğŸ”§ LangGraphæ¨¡å¼ - å·¥å…·å: {tool_name}")
                # é™¤äº†general_conversationå¤–ï¼Œæ‰€æœ‰å·¥å…·éƒ½è·¯ç”±åˆ°execute_tools
                if tool_name == "general_conversation":
                    logger.info("ğŸ’¬ è·¯ç”±åˆ°: general_conversation")
                    return "general_conversation"
                else:
                    logger.info(f"ğŸ› ï¸ è·¯ç”±åˆ°: execute_tools (å·¥å…·: {tool_name})")
                    return "execute_tools"
            
        logger.info("âš ï¸ æ— åŒ¹é…æ¡ä»¶ï¼Œé»˜è®¤è·¯ç”±åˆ°general_conversation")
        return "general_conversation"

    def _build_history_text(self, conversation_history: List[Dict[str, str]], k: int = 8, clip: int = 200) -> str:
        """æŠŠæœ€è¿‘ k æ¡å†å²æ‹¼æˆç»Ÿä¸€æ–‡æœ¬ï¼›é•¿æ¶ˆæ¯è£å‰ªåˆ° clip å­—ç¬¦ã€‚"""
        if not conversation_history:
            return ""

        recent = conversation_history[-k:]
        lines = []
        for msg in recent:
            role = "ç”¨æˆ·" if msg.get("role") == "user" else "åŠ©æ‰‹"
            content = msg.get("content", "")
            if len(content) > clip:
                content = content[:clip] + "..."
            lines.append(f"{role}: {content}")
        return "\n".join(lines)
    
    
    async def _generate_conversation_response(self, data: AgentState) -> str:
        """åŸºäºå·¥å…·æ‰§è¡Œç»“æœå’Œå¯¹è¯å†å²ç”Ÿæˆæ™ºèƒ½å“åº” - è®©å¤§æ¨¡å‹å¯¹æ‰€æœ‰å·¥å…·ç»“æœè¿›è¡Œæ™ºèƒ½æ€»ç»“"""
        user_input = data.get("input", "")
        conversation_history = data.get("conversation_history", [])
        
        # æ„å»ºè¯¦ç»†çš„å·¥å…·æ‰§è¡Œç»“æœ - é€šç”¨åŒ–å¤„ç†ï¼Œä¸ç¡¬ç¼–ç ç‰¹å®šå­—æ®µ
        detailed_tool_results = []
        tool_output_summary = {}  # æŒ‰å·¥å…·ç±»å‹æ±‡æ€»è¾“å‡º
        
        for i, (action, result) in enumerate(data.get("intermediate_steps", [])):
            tool_name = action.tool
            step_num = i + 1
            
            # è§£æå·¥å…·ç»“æœå¹¶æ”¶é›†è¯¦ç»†ä¿¡æ¯
            if isinstance(result, dict):
                detailed_tool_results.append({
                    "step": step_num,
                    "tool": tool_name,
                    "result": result
                })
                
                # é€šç”¨åŒ–åœ°æ”¶é›†æ¯ä¸ªå·¥å…·çš„è¾“å‡º
                if tool_name not in tool_output_summary:
                    tool_output_summary[tool_name] = []
                
                # åŠ¨æ€æå–ç»“æœä¸­çš„é‡è¦ä¿¡æ¯
                important_fields = []
                for key, value in result.items():
                    # ä¸è·³è¿‡ä»»ä½•å­—æ®µï¼Œè®©LLMçœ‹åˆ°å®Œæ•´çš„å·¥å…·è¿”å›æ•°æ®
                    if isinstance(value, (str, int, float, bool)) and len(str(value)) < 200:
                        important_fields.append(f"{key}: {value}")
                
                # åˆ¤æ–­æ‰§è¡ŒçŠ¶æ€ - æ›´æ™ºèƒ½çš„çŠ¶æ€åˆ¤æ–­
                is_success = (
                    result.get("success") is True or 
                    result.get("access_granted") is True or
                    result.get("ok") is True or
                    result.get("status") == "completed"
                )
                
                tool_output_summary[tool_name].append({
                    "step": step_num,
                    "status": "æˆåŠŸ" if is_success else "å¤±è´¥",
                    "details": important_fields
                })
                    
            else:
                # å¦‚æœä¸æ˜¯å­—å…¸ï¼Œä¹Ÿè¦è®°å½•
                detailed_tool_results.append({
                    "step": step_num,
                    "tool": tool_name,
                    "result": {"raw_output": str(result)}
                })
                
                if tool_name not in tool_output_summary:
                    tool_output_summary[tool_name] = []
                tool_output_summary[tool_name].append({
                    "step": step_num,
                    "status": "å®Œæˆ",
                    "details": [f"è¾“å‡º: {str(result)[:50]}"]
                })
        
        # å¦‚æœLLMä¸å¯ç”¨ï¼Œä½¿ç”¨å¢å¼ºçš„æ ¼å¼åŒ–è¾“å‡º
        if not self.llm.api_available:
            Exception("LLMæœåŠ¡ä¸å¯ç”¨ï¼Œæ— æ³•æ‰§è¡Œä»»ä½•å·¥å…·æˆ–SubAgent")
        
        try:
            # æ„å»ºæ›´æ™ºèƒ½çš„æç¤ºè¯ï¼Œè¦æ±‚å¤§æ¨¡å‹è¿›è¡Œæ·±åº¦æ€»ç»“
            system_prompt = """ä½ æ˜¯DataFlowæ™ºèƒ½åŠ©æ‰‹ã€‚è¯·åŸºäºå·¥å…·æ‰§è¡Œç»“æœè¿›è¡Œæ€»ç»“ã€‚

è¦æ±‚ï¼š
1. å‡†ç¡®æ˜¾ç¤ºæ‰€æœ‰å·¥å…·è¿”å›çš„æ•°æ®ï¼ŒåŒ…æ‹¬å…·ä½“çš„APIå¯†é’¥å€¼
2. ä¸è¦ç¼–é€ ä¿¡æ¯ï¼Œæ‰€æœ‰å†…å®¹éƒ½åŸºäºæ‰§è¡ŒæŠ¥å‘Š
3. ç”¨è‡ªç„¶çš„è¯­è¨€æ€»ç»“æ‰§è¡Œç»“æœ"""

            # æ„å»ºå¯¹è¯å†å²æ–‡æœ¬
            history_text = self._build_history_text(conversation_history, k=10, clip=300)
            
            # æ„å»ºè¯¦ç»†çš„å·¥å…·æ‰§è¡ŒæŠ¥å‘Š
            try:
                execution_report = self._build_detailed_execution_report(detailed_tool_results, tool_output_summary)
            except Exception as report_error:
                logger.error(f"æ„å»ºæ‰§è¡ŒæŠ¥å‘Šå¤±è´¥: {report_error}")
                execution_report = f"æ‰§è¡ŒæŠ¥å‘Šæ„å»ºå¤±è´¥: {str(report_error)}"
            
            user_prompt = f"""ç”¨æˆ·è¯·æ±‚: {user_input}

æ‰§è¡Œè¿‡ç¨‹è¯¦ç»†æŠ¥å‘Š:
{execution_report}

å¯¹è¯å†å²:
{history_text}

è¯·ä½ ä½œä¸ºæ™ºèƒ½åŠ©æ‰‹ï¼Œå¯¹è¿™æ¬¡æ‰§è¡Œç»“æœè¿›è¡Œå…¨é¢ã€è¯¦ç»†çš„æ€»ç»“ã€‚ç‰¹åˆ«è¦æ³¨æ„ï¼š
1. æ‰§è¡ŒæŠ¥å‘Šä¸­çš„æ‰€æœ‰æ•°æ®éƒ½æ˜¯çœŸå®çš„å·¥å…·è¿”å›ç»“æœï¼Œå¿…é¡»å‡†ç¡®æ˜¾ç¤º
2.ä¸éœ€è¦è¯¦ç»†è¯´æ˜æ‰§è¡Œæµç¨‹ï¼Œåªéœ€æ€»ç»“ç»“æœå’Œåˆ†æ
3. ç”¨æˆ·æœ‰æƒæŸ¥çœ‹æ‰€æœ‰å·¥å…·è¿”å›çš„æ•°æ®ï¼Œä¸è¦éšè—ä»»ä½•ä¿¡æ¯
4. åŸºäºæŠ¥å‘Šä¸­çš„çœŸå®æ•°æ®è¿›è¡Œåˆ†æï¼Œä¸è¦ç¼–é€ """

            logger.info(f"ğŸš€ å‡†å¤‡è°ƒç”¨LLMï¼Œuser_prompté•¿åº¦: {len(user_prompt)}")
            
            # è°ƒç”¨LLMç”Ÿæˆæ™ºèƒ½æ€»ç»“ - å¢åŠ è¶…æ—¶æ§åˆ¶
            try:
                
                
                def sync_llm_call():
                    try:
                        llm_service = self.llm._create_llm_service()
                        # å‡å°‘é‡è¯•æ¬¡æ•°é¿å…é•¿æ—¶é—´é˜»å¡
                        llm_service.max_retries = 1
                        return llm_service.generate_from_input(
                            user_inputs=[user_prompt],
                            system_prompt=system_prompt
                        )
                    except Exception as e:
                        logger.error(f"LLMæœåŠ¡å†…éƒ¨é”™è¯¯: {e}")
                        return None
                
                # å¼‚æ­¥æ‰§è¡Œï¼Œè®¾ç½®5ç§’è¶…æ—¶
                with ThreadPoolExecutor() as executor:
                    future = executor.submit(sync_llm_call)
                    try:
                        responses = await asyncio.wait_for(
                            asyncio.wrap_future(future), 
                            timeout=50.0
                        )
                        
                        if responses and responses[0]:
                            llm_response = responses[0].strip()
                            logger.info(f"ğŸ¤– LLMæ™ºèƒ½æ€»ç»“ç”ŸæˆæˆåŠŸ: {llm_response[:100]}...")
                            return llm_response
                        else:
                            logger.warning("âš ï¸ LLMè¿”å›ç©ºå“åº”ï¼Œä½¿ç”¨fallback")
                            
                    except asyncio.TimeoutError:
                        logger.warning("âš ï¸ LLMè°ƒç”¨è¶…æ—¶ï¼ˆ5ç§’ï¼‰ï¼Œä½¿ç”¨fallback")
                    except Exception as e:
                        logger.error(f"LLMå¼‚æ­¥è°ƒç”¨é”™è¯¯: {e}")
                        
            except Exception as e:
                logger.error(f"LLMæ™ºèƒ½æ€»ç»“è°ƒç”¨å¤±è´¥: {e}")
                import traceback
                logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            
        except Exception as e:
            logger.error(f"LLMæ™ºèƒ½æ€»ç»“ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            # ç›´æ¥æŠ›å‡ºå¼‚å¸¸ï¼Œä¸ä½¿ç”¨fallback
            raise e
      
    def _build_detailed_execution_report(self, detailed_tool_results: List[Dict], tool_output_summary: Dict) -> str:
        """æ„å»ºè¯¦ç»†çš„æ‰§è¡ŒæŠ¥å‘Šä¾›LLMåˆ†æ - ç²¾ç®€ç‰ˆæœ¬ï¼Œåªæ˜¾ç¤ºå…³é”®çš„æ‰§è¡Œç»“æœ"""
        report_sections = []
        
        # æ‰§è¡Œæ¦‚å†µ
        total_steps = len(detailed_tool_results)
        report_sections.append(f"æ‰§è¡Œæ¦‚å†µ: æ€»å…±{total_steps}ä¸ªæ­¥éª¤")
        
        # å®Œæ•´æ‰§è¡Œæ—¶åºå’Œç»“æœ - æ˜¾ç¤ºå®Œæ•´çš„å·¥å…·è¿”å›æ•°æ®
        report_sections.append(f"\næ‰§è¡Œç»“æœè¯¦æƒ…:")
        for tool_result in detailed_tool_results:
            step = tool_result["step"]
            tool = tool_result["tool"]
            result = tool_result["result"]
            
            # æ˜¾ç¤ºå®Œæ•´çš„å·¥å…·è¿”å›æ•°æ®
            if isinstance(result, dict):
                # æ›´æ™ºèƒ½çš„çŠ¶æ€åˆ¤æ–­
                is_success = (
                    result.get("success") is True or 
                    result.get("access_granted") is True or
                    result.get("ok") is True or
                    result.get("status") == "completed"
                )
                status = "æˆåŠŸ" if is_success else "å®Œæˆ"
                
                # æ˜¾ç¤ºå®Œæ•´çš„å…³é”®å­—æ®µï¼Œç‰¹åˆ«æ˜¯apikey
                key_info = []
                priority_fields = ["apikey", "result", "message"]  # ä¼˜å…ˆæ˜¾ç¤ºçš„å­—æ®µ
                
                # å…ˆæ·»åŠ ä¼˜å…ˆå­—æ®µ
                for field in priority_fields:
                    if field in result:
                        value = result[field]
                        if isinstance(value, (str, int, float, bool)):
                            key_info.append(f"{field}: {value}")
                
                # å†æ·»åŠ å…¶ä»–å­—æ®µ
                for key, value in result.items():
                    if key not in priority_fields and isinstance(value, (str, int, float, bool)):
                        if len(str(value)) < 100:  # åªæ˜¾ç¤ºåˆç†é•¿åº¦çš„å­—æ®µ
                            key_info.append(f"{key}: {value}")
                
                if key_info:
                    # æ˜¾ç¤ºæ‰€æœ‰é‡è¦å­—æ®µï¼Œä¸æˆªæ–­
                    info_text = "\n    ".join(key_info)
                    report_sections.append(f"  æ­¥éª¤{step}: [{tool}] {status}")
                    report_sections.append(f"    {info_text}")
                else:
                    report_sections.append(f"  æ­¥éª¤{step}: [{tool}] {status}")
            else:
                report_sections.append(f"  æ­¥éª¤{step}: [{tool}] å®Œæˆ â†’ {str(result)[:100]}")
        
        return "\n".join(report_sections)
    
    async def _get_direct_conversation_response(self, data: AgentState) -> str:
        """å½“æ²¡æœ‰å·¥å…·æ‰§è¡Œæ—¶ï¼Œè·å–ç›´æ¥å¯¹è¯å›å¤"""
        user_input = data.get("input", "")
        conversation_history = data.get("conversation_history", [])
        
        # ç›´æ¥ä½¿ç”¨LLMï¼Œä¸åšfallback
        if self.llm.api_available:
            try:
                system_prompt = "ä½ æ˜¯DataFlowæ™ºèƒ½åŠ©æ‰‹ï¼Œè¯·ç›´æ¥ã€è‡ªç„¶åœ°å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"
                
                # æ„å»ºå¯¹è¯å†å²
                history_text = self._build_history_text(conversation_history, k=8, clip=200)
                
                user_prompt = f"""ç”¨æˆ·é—®é¢˜: {user_input}

å¯¹è¯å†å²:
{history_text}

è¯·åŸºäºå¯¹è¯å†å²è‡ªç„¶åœ°å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"""
                
                llm_service = self.llm._create_llm_service()
                responses = llm_service.generate_from_input(
                    user_inputs=[user_prompt],
                    system_prompt=system_prompt
                )
                
                if responses and responses[0]:
                    return responses[0].strip()
                    
            except Exception as e:
                logger.error(f"LLMè°ƒç”¨å¤±è´¥: {e}")
                raise e
        
        # LLMä¸å¯ç”¨æ—¶æŠ›å‡ºå¼‚å¸¸
        raise Exception("LLMæœåŠ¡ä¸å¯ç”¨ï¼Œæ— æ³•å¤„ç†å¯¹è¯")

    async def _planner(self, state: AgentState) -> PlannerOutput:
        """æ™ºèƒ½è§„åˆ’å™¨ï¼šåŸºäºç”¨æˆ·éœ€æ±‚å’Œå·²æ‰§è¡Œçš„å·¥å…·ç»“æœï¼Œå†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨"""
        user_input = state.get("input", "")
        tool_results = state.get("tool_results", [])
        
        # åˆ†æç”¨æˆ·åŸå§‹éœ€æ±‚ä¸­çš„å…³é”®ä¿¡æ¯
        needs_analysis = self._analyze_user_needs(user_input, tool_results)
        
        logger.info(f"ğŸ¯ è§„åˆ’å™¨åˆ†æ: {needs_analysis}")
        
        # åŸºäºåˆ†æç»“æœå†³å®šä¸‹ä¸€æ­¥
        if needs_analysis["should_continue"]:
            next_action = needs_analysis["next_action"]
            logger.info(f"âœ… è§„åˆ’å™¨å†³å®šç»§ç»­: {next_action}")
            return PlannerOutput(
                decision="continue",
                next_actions=[next_action],
                reasons="; ".join(needs_analysis["reasons"]) if isinstance(needs_analysis["reasons"], list) else needs_analysis["reasons"]
            )
        else:
            logger.info(f"ğŸ è§„åˆ’å™¨å†³å®šç»“æŸ: {needs_analysis['reasons']}")
            # ç”Ÿæˆç®€å•çš„æ€»ç»“æ¶ˆæ¯ï¼Œé¿å…å¤æ‚çš„LLMè°ƒç”¨ - é€šç”¨åŒ–å¤„ç†
            summary_msg = "ä»»åŠ¡å·²å®Œæˆ"
            tool_results = state.get("tool_results", [])
            if tool_results:
                latest_result = tool_results[-1]
                tool_name = latest_result.get("tool", "æœªçŸ¥å·¥å…·")
                result_data = latest_result.get("result", {})
                
                # é€šç”¨åŒ–åœ°æå–ç»“æœä¿¡æ¯
                if isinstance(result_data, dict):
                    important_info = []
                    for key, value in result_data.items():
                        if key in ["success", "ok", "status"]:
                            continue
                        elif isinstance(value, (str, int, float)) and len(str(value)) < 50:
                            important_info.append(f"{key}: {value}")
                    
                    if important_info:
                        info_text = ", ".join(important_info[:1])  # åªæ˜¾ç¤ºç¬¬1ä¸ªé‡è¦å­—æ®µ
                        summary_msg = f"å·²å®Œæˆ{tool_name}æ‰§è¡Œï¼Œç»“æœ: {info_text}"
                    else:
                        summary_msg = f"å·²å®Œæˆ{tool_name}æ‰§è¡Œ"
                else:
                    summary_msg = f"å·²å®Œæˆ{tool_name}æ‰§è¡Œ"
            
            return PlannerOutput(
                decision="finish",
                user_message=summary_msg,
                reasons="; ".join(needs_analysis["reasons"]) if isinstance(needs_analysis["reasons"], list) else needs_analysis["reasons"]
            )

    def _analyze_user_needs(self, user_input: str, tool_results: List[Dict]) -> Dict[str, Any]:
        """è®©LLMæ™ºèƒ½åˆ†æç”¨æˆ·éœ€æ±‚å’Œå½“å‰æ‰§è¡ŒçŠ¶æ€ï¼Œå†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨ - å®Œå…¨åŸºäºLLMå†³ç­–"""
        
        # ğŸ”§ æ ¸å¿ƒä¿®å¤ï¼šè®©LLMæ¥ç†è§£å’Œå†³ç­–
        if not self.llm.api_available:
            # LLMä¸å¯ç”¨æ—¶çš„ç®€å•fallback
            return {
                "should_continue": False,
                "next_action": None,
                "reasons": ["LLMä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œæ™ºèƒ½å†³ç­–"],
                "analysis": {}
            }
        
        try:
            # æ„å»ºå¯ç”¨å·¥å…·çš„è¯¦ç»†æè¿°ï¼ŒåŒ…å«å‚æ•°ä¿¡æ¯
            available_tools = []
            for tool in self.tools:
                tool_info = {
                    "name": tool.name(),
                    "description": tool.description(),
                    "parameters": self._get_tool_parameters(tool)
                }
                available_tools.append(tool_info)
            
            # æ„å»ºæ‰§è¡Œå†å² - é€šç”¨åŒ–å¤„ç†ï¼Œä¸ç¡¬ç¼–ç ç‰¹å®šå­—æ®µ
            execution_history = []
            for i, result in enumerate(tool_results, 1):
                tool_name = result.get("tool", "unknown")
                success = result.get("ok", False)
                payload = result.get("payload", {})
                
                step_info = f"æ­¥éª¤{i}: æ‰§è¡Œäº†{tool_name}"
                if success:
                    if isinstance(payload, dict):
                        # åˆ¤æ–­å…·ä½“æ‰§è¡ŒçŠ¶æ€
                        is_tool_success = (
                            payload.get("success") is True or 
                            payload.get("access_granted") is True or
                            payload.get("ok") is True or
                            payload.get("status") == "completed"
                        )
                        
                        if is_tool_success:
                            step_info += " - æˆåŠŸ"
                            
                            # é€šç”¨åŒ–åœ°æå–é‡è¦ä¿¡æ¯ï¼Œæ˜¾ç¤ºæ‰€æœ‰å…³é”®å­—æ®µ
                            important_info = []
                            for key, value in payload.items():
                                if isinstance(value, (str, int, float, bool)) and len(str(value)) < 50:
                                    important_info.append(f"{key}: {value}")
                            
                            if important_info:
                                info_text = ", ".join(important_info[:3])  # æ˜¾ç¤ºå‰3ä¸ªé‡è¦å­—æ®µ
                                step_info += f" ({info_text})"
                        else:
                            step_info += " - å¤±è´¥"
                    else:
                        step_info += " - å®Œæˆ"
                else:
                    step_info += " - å¤±è´¥"
                
                execution_history.append(step_info)
            
            # æ„å»ºLLMå†³ç­–æç¤ºè¯ - ä¸¥æ ¼JSONæ ¼å¼
            system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½å†³ç­–åŠ©æ‰‹ï¼Œéœ€è¦æ ¹æ®ç”¨æˆ·éœ€æ±‚å’Œå½“å‰æ‰§è¡Œæƒ…å†µï¼Œå†³å®šä¸‹ä¸€æ­¥åº”è¯¥æ‰§è¡Œä»€ä¹ˆå·¥å…·ã€‚

å†³ç­–åŸåˆ™ï¼š
1. ä»”ç»†ç†è§£ç”¨æˆ·çš„å®Œæ•´éœ€æ±‚ï¼ŒåŒ…æ‹¬è¦æ‰§è¡Œå¤šå°‘æ¬¡ã€æ˜¯å¦éœ€è¦é—´éš”ç­‰
2. åˆ†æå½“å‰çš„æ‰§è¡Œå†å²ï¼Œäº†è§£å·²ç»å®Œæˆäº†ä»€ä¹ˆ
3. åŸºäºå·¥å…·çš„åŠŸèƒ½æè¿°ï¼Œé€‰æ‹©æœ€åˆé€‚çš„ä¸‹ä¸€æ­¥åŠ¨ä½œ
4. æ¯æ¬¡åªå†³ç­–ä¸€ä¸ªåŠ¨ä½œï¼Œä¸è¦ä¸€æ¬¡æ€§è§„åˆ’å¤šä¸ªæ­¥éª¤
5. å¦‚æœä»»åŠ¡å·²å®Œæˆï¼Œåº”è¯¥é€‰æ‹©ç»“æŸ

**é‡è¦ï¼šä½ å¿…é¡»åªè¾“å‡ºJSONæ ¼å¼ï¼Œä¸è¦æœ‰ä»»ä½•é¢å¤–çš„è§£é‡Šæ–‡å­—ï¼**

è¿”å›æ ¼å¼ï¼ˆå¿…é¡»æ˜¯çº¯JSONï¼Œæ— ä»»ä½•å…¶ä»–å†…å®¹ï¼‰ï¼š
{
    "decision": "continue" æˆ– "finish",
    "tool": "å·¥å…·åç§°",
    "tool_input": {"å‚æ•°å": "å‚æ•°å€¼"},
    "finish_message": "ä»»åŠ¡å®Œæˆè¯´æ˜ï¼ˆä»…å½“decisionä¸ºfinishæ—¶ï¼‰",
    "reason": "ç®€çŸ­çš„å†³ç­–åŸå› "
}"""

            user_prompt = f"""ç”¨æˆ·åŸå§‹éœ€æ±‚: {user_input}

å¯ç”¨å·¥å…·åˆ—è¡¨:
{chr(10).join([f"- {tool['name']}: {tool['description']} | å‚æ•°: {tool['parameters']}" for tool in available_tools])}

å½“å‰æ‰§è¡Œå†å²:
{chr(10).join(execution_history) if execution_history else "è¿˜æ²¡æœ‰æ‰§è¡Œä»»ä½•å·¥å…·"}

è¯·åˆ†æå½“å‰æƒ…å†µï¼Œå†³å®šä¸‹ä¸€æ­¥åº”è¯¥ï¼š
1. ç»§ç»­æ‰§è¡ŒæŸä¸ªå·¥å…·ï¼ˆå¦‚æœä»»åŠ¡æœªå®Œæˆï¼‰
2. è¿˜æ˜¯ç»“æŸä»»åŠ¡ï¼ˆå¦‚æœå·²ç»æ»¡è¶³ç”¨æˆ·éœ€æ±‚ï¼‰

**é‡è¦ï¼šåªè¾“å‡ºJSONæ ¼å¼ï¼Œä¸è¦ä»»ä½•è§£é‡Šæ–‡å­—ï¼**"""

            logger.info(f"ğŸ¤– è°ƒç”¨LLMè¿›è¡Œæ™ºèƒ½å†³ç­–...")
            
            # è°ƒç”¨LLMè¿›è¡Œå†³ç­– - ä½¿ç”¨åŸºç¡€è°ƒç”¨æ–¹å¼
            llm_service = self.llm._create_llm_service()
            responses = llm_service.generate_from_input(
                user_inputs=[user_prompt],
                system_prompt=system_prompt
            )
            
            if responses and responses[0]:
                content = responses[0].strip()
                logger.info(f"ğŸ¤– LLMå†³ç­–å“åº”: {content}")
                
                # å°è¯•æ¸…ç†JSONæ ¼å¼ - ç§»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
                if content.startswith("```json"):
                    content = content[7:]
                if content.endswith("```"):
                    content = content[:-3]
                content = content.strip()
                
                # è§£æLLMå“åº”
                try:
                    import json
                    decision = json.loads(content)
                    
                    decision_type = decision.get("decision", "finish")
                    tool_name = decision.get("tool")
                    tool_input = decision.get("tool_input", {})
                    finish_message = decision.get("finish_message", "")
                    reasoning = decision.get("reason", "")
                    
                    # åˆ¤æ–­æ˜¯å¦ç»§ç»­
                    should_continue = (decision_type == "continue")
                    
                    # æ„å»ºnext_action
                    next_action = None
                    if should_continue and tool_name and tool_input:
                        next_action = {
                            "tool": tool_name,
                            "tool_input": tool_input
                        }
                    elif should_continue and tool_name:
                        # å¦‚æœåªæœ‰å·¥å…·åæ²¡æœ‰å‚æ•°ï¼Œä½¿ç”¨ç”¨æˆ·è¾“å…¥ä½œä¸ºfallback
                        next_action = {
                            "tool": tool_name,
                            "tool_input": {"user_message": user_input}
                        }
                    
                    result = {
                        "should_continue": should_continue,
                        "next_action": next_action,
                        "reasons": [reasoning or finish_message],
                        "analysis": {
                            "decision_type": decision_type,
                            "llm_decision": decision,
                            "execution_count": len(tool_results),
                            "json_parsed": True
                        }
                    }
                    
                    logger.info(f"ğŸ¯ LLMå†³ç­–ç»“æœ: decision={decision_type}, tool={tool_name}")
                    logger.info(f"ğŸ¯ å·¥å…·å‚æ•°: {tool_input}")
                    logger.info(f"ğŸ¯ å†³ç­–åŸå› : {reasoning}")
                    
                    return result
                    
                except json.JSONDecodeError as e:
                    logger.error(f"LLMå“åº”JSONè§£æå¤±è´¥: {e}")
                    logger.error(f"åŸå§‹å“åº”: {content}")
                    
                    return 
            
        except Exception as e:
            logger.error(f"LLMæ™ºèƒ½å†³ç­–å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
        
        # å‡ºé”™æ—¶çš„fallback
        return {
            "should_continue": False,
            "next_action": None,
            "reasons": ["æ™ºèƒ½å†³ç­–å¤±è´¥ï¼Œç»“æŸä»»åŠ¡"],
            "analysis": {}
        }
    
    def _get_tool_parameters(self, tool) -> str:
        """è·å–å·¥å…·çš„å‚æ•°ä¿¡æ¯ - åŠ¨æ€è§£æå·¥å…·å‚æ•°ï¼Œä¸ä½¿ç”¨ç¡¬ç¼–ç """
        try:
            # æ–¹æ³•1ï¼šå°è¯•è°ƒç”¨å·¥å…·çš„å‚æ•°æ–¹æ³•
            if hasattr(tool, 'params'):
                params_class = tool.params()
                if hasattr(params_class, '__annotations__'):
                    # ä½¿ç”¨__annotations__è·å–ç±»å‹æ³¨è§£
                    annotations = params_class.__annotations__
                    param_info = []
                    for field_name, field_type in annotations.items():
                        type_name = getattr(field_type, '__name__', str(field_type))
                        param_info.append(f"{field_name}: {type_name}")
                    return "{" + ", ".join(param_info) + "}"
                
                elif hasattr(params_class, '__dict__'):
                    # å°è¯•ä»ç±»å­—å…¸è·å–ä¿¡æ¯
                    class_dict = params_class.__dict__
                    param_info = []
                    for key, value in class_dict.items():
                        if not key.startswith('_'):
                            param_info.append(f"{key}: auto_detected")
                    if param_info:
                        return "{" + ", ".join(param_info) + "}"
            
            # æ–¹æ³•2ï¼šå°è¯•æ£€æŸ¥å·¥å…·çš„args_schema
            if hasattr(tool, 'args_schema') and tool.args_schema:
                schema = tool.args_schema
                if hasattr(schema, '__annotations__'):
                    annotations = schema.__annotations__
                    param_info = []
                    for field_name, field_type in annotations.items():
                        type_name = getattr(field_type, '__name__', str(field_type))
                        param_info.append(f"{field_name}: {type_name}")
                    return "{" + ", ".join(param_info) + "}"
            
            # æ–¹æ³•3ï¼šå°è¯•inspectå·¥å…·çš„runæ–¹æ³•
            if hasattr(tool, 'run'):
                import inspect
                sig = inspect.signature(tool.run)
                param_info = []
                for param_name, param in sig.parameters.items():
                    if param_name not in ['self', 'kwargs', 'args']:
                        type_hint = param.annotation if param.annotation != inspect.Parameter.empty else "auto"
                        type_name = getattr(type_hint, '__name__', str(type_hint))
                        param_info.append(f"{param_name}: {type_name}")
                if param_info:
                    return "{" + ", ".join(param_info) + "}"
            
            # æ–¹æ³•4ï¼šæœ€åfallback - åŸºäºå·¥å…·æè¿°æ¨æ–­
            description = tool.description().lower()
            if "path" in description or "file" in description:
                return '{"path": "string", "additional_params": "auto"}'
            elif "query" in description or "search" in description:
                return '{"query": "string", "additional_params": "auto"}'
            elif "seconds" in description or "time" in description:
                return '{"seconds": "number", "additional_params": "auto"}'
            else:
                return '{"user_message": "string(é€šç”¨å‚æ•°)"}'
                
        except Exception as e:
            logger.debug(f"åŠ¨æ€è§£æå·¥å…·{tool.name()}å‚æ•°å¤±è´¥: {e}")
            # æœ€ç»ˆfallback
            return '{"user_message": "string(é€šç”¨å‚æ•°)"}'


